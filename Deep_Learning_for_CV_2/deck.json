{
    "__type__": "Deck",
    "children": [],
    "crowdanki_uuid": "a0b89bb9-7d23-11ec-b057-182649c72b2b",
    "deck_config_uuid": "a0b89bba-7d23-11ec-beae-182649c72b2b",
    "deck_configurations": [
        {
            "__type__": "DeckConfig",
            "autoplay": true,
            "crowdanki_uuid": "a0b89bba-7d23-11ec-beae-182649c72b2b",
            "dyn": false,
            "interdayLearningMix": 0,
            "lapse": {
                "delays": [
                    10.0
                ],
                "leechAction": 1,
                "leechFails": 8,
                "minInt": 1,
                "mult": 0.0
            },
            "maxTaken": 60,
            "name": "Default",
            "new": {
                "bury": false,
                "delays": [
                    1.0,
                    10.0
                ],
                "initialFactor": 2500,
                "ints": [
                    1,
                    4,
                    0
                ],
                "order": 1,
                "perDay": 20
            },
            "newGatherPriority": 0,
            "newMix": 0,
            "newPerDayMinimum": 0,
            "newSortOrder": 0,
            "replayq": true,
            "rev": {
                "bury": false,
                "ease4": 1.3,
                "hardFactor": 1.2,
                "ivlFct": 1.0,
                "maxIvl": 36500,
                "perDay": 200
            },
            "reviewOrder": 0,
            "timer": 0
        }
    ],
    "desc": "",
    "dyn": 0,
    "extendNew": 0,
    "extendRev": 0,
    "media_files": [
        "addnnorm.png",
        "assymetric-nonlocal.png",
        "bayesian.png",
        "channelatt.png",
        "deformableatt.png",
        "deformabledetr.png",
        "detrobjectqueries.png",
        "detropt.png",
        "detroverview.png",
        "detrpred.png",
        "detrtrans.png",
        "distill.png",
        "distofconcepts.png",
        "dualatt.png",
        "ece.png",
        "enccas.png",
        "erf.png",
        "gradcam.png",
        "gradientbased.png",
        "gradients-interpretability.png",
        "mla.png",
        "non-local.png",
        "panoptic.png",
        "paste-009634588cccaf44a062b5e2814d638b50bef7c1.jpg",
        "paste-0227840a90cb69e7f6038e2e9f578a89d34ef987.jpg",
        "paste-02733f858062cad2ab0bbcae007f70634ee29222.jpg",
        "paste-02bdf548805b6bdc42de8ed04ecd8c2c55770f77.jpg",
        "paste-032b62452e420e7d85d820aaaa11007827962ba9.jpg",
        "paste-0428f1496b9f64cac60fbfd389cf5b4faa27b5ce.jpg",
        "paste-04e9b90197a3b6243f99bcf83969598d7f31cd6d.jpg",
        "paste-055808a11993c8446283e2e6fa9a727dab5e2cae.jpg",
        "paste-06fc603005d37146b5d13ee8d9f100bae6a1bca6.jpg",
        "paste-0743e7782acb48cb25386fa0f2d67de70388db86.jpg",
        "paste-07cff660823667950fb0c63c0323a16c5271cca1.jpg",
        "paste-0899d807362afe108ba5aaeb6d9afbd9d6533bbd.jpg",
        "paste-09a88dda1b66233143cc626063c9dfd0ae3b31d3.jpg",
        "paste-09f6eae51e3b2f973175f56ffcf174d8cdc27ab3.jpg",
        "paste-0b517da9f364eb17461b38452ee6a051c504aa59.jpg",
        "paste-0b7a33b46b99ca6180067ff563f04c745fd6bb3d.jpg",
        "paste-0c1424398aee9a1c0c4bf18e97b6fe02c5451491.jpg",
        "paste-0d8638095088f2c6756edc0f510b6b35aa2dffa6.jpg",
        "paste-0ed58c407428832dc8c33892c4b93c250b27ffd0.jpg",
        "paste-10014cc44ae5e7dc665464b40773131329d2b8e5.jpg",
        "paste-1097f7a9db9ae3f4d080902bf1025f06495316b6.jpg",
        "paste-11c71e73fb8b752c8488285f3274b3c7e020d0ad.jpg",
        "paste-123293f031c62041583261f79397ec4a7370bf07.jpg",
        "paste-13430ce4d8b567121fba6f330f9e56054a4a1832.jpg",
        "paste-16d0a29059b641389aaad4f9a1405a8a0832029f.jpg",
        "paste-17ac8d03cd57635e01c884457df2a5d5cd437855.jpg",
        "paste-17baf89eea2b506120d6cc37bd8810f0e1542bbb.jpg",
        "paste-187daafccf372e03a9ea275ee6e532ef2a41f590.jpg",
        "paste-1b782acfdfa08e8b7caf6c9b32c94c779a84bfaa.jpg",
        "paste-1c0305f73387e5bfbff1afad99f05619bd9e93a3.jpg",
        "paste-1ca79e3dcf178bb864b80e3070cbe4f98471e1e5.jpg",
        "paste-1cfb931a806d4941364ac8f6b45aad5075c1e9ec.jpg",
        "paste-1e6c2ee2b695b6eb675a4cfae6cd7f1b8ac286a1.jpg",
        "paste-1e798cc7d60541829ba3b4bf645a7c69054b9ce1.jpg",
        "paste-20c2d02008c9a6ab3d962349928e36c3bf6fd2dc.jpg",
        "paste-226bae653b4c5af40bf45db4df58d76553f6d88c.jpg",
        "paste-24d08b85d34548d9357836a90d51019c89a511a3.jpg",
        "paste-262ce599bd5b32c79e8d377bfe2ab1da4321810f.jpg",
        "paste-2656c36139971804ef03ee0cfcc7cadf368f9997.jpg",
        "paste-26655b33f2eac1a97be619c255bea78ba0f686ed.jpg",
        "paste-28d8a3941bed64a21dd3447d7c52af83a326f679.jpg",
        "paste-2c6080c9ded359b654f25ddc2785c984fbdf0d70.jpg",
        "paste-2c685eaa5d23739613bd955f34d3b2b39a5b3df0.jpg",
        "paste-2c9f76f2d79264c0632bd02c6a2aeb347265e221.jpg",
        "paste-2dc464f688cd4c9e3860418e2f38b5e6663e4b50.jpg",
        "paste-2e44669237e1b53f47acc75d321dd8dfce87e5a3.jpg",
        "paste-305356f004b14461f24d96709e4aa4da5821832c.jpg",
        "paste-31d7c09f358360b7147ac144674abe17da318bcc.jpg",
        "paste-3240a86a46d54ee1452333768f16e2971744a230.jpg",
        "paste-3299bb615c2fcc62f38cb875c57fc530d61ea8f4.jpg",
        "paste-354f80d340c009b648b38b980752f5dd953f84ab.jpg",
        "paste-35f8c6ff4117515bfcca84d95649432d5919f49c.jpg",
        "paste-37759bafedb54078fe502cd0f532b4356b1e232b.jpg",
        "paste-386bd1c4d3623ed8a866e9b11d5773a7f5e59966.jpg",
        "paste-3be48d03cab58848d21541265b2221fd2d0c6d6f.jpg",
        "paste-3c6049444fabd9bc0d9b97cfa07b259a45422f32.jpg",
        "paste-3cba2285a0d9852b5f4f96b9a2d51217fa55a29e.jpg",
        "paste-3ce0006ac7d538fba2f25b57687c5496d2703342.jpg",
        "paste-3f50de25449fc22a33661edaf5281b2b7a397f54.jpg",
        "paste-410dc42d9d19c13b14581f703ac9b1bedb7fa203.jpg",
        "paste-43153e25abba4adca963b8fe6b45379c86a41ee2.jpg",
        "paste-45869e3be7d16c01d15740efdc36303a6ef151db.jpg",
        "paste-482f73353bc216525e83dcb1f129d67e546ac6e1.jpg",
        "paste-4b77301b82c08ba58e65d31ca330449ca16bbc83.jpg",
        "paste-4d8b8b70426958020c45d77b101989e2c47b89c2.jpg",
        "paste-4f49e383b77539f8b3ef56d0282b901d5b6d2514.jpg",
        "paste-51faa6aa8f3a29082b875683d2fd03e5ddc7356b.jpg",
        "paste-526795975da2eb01ce2d4e67a9dce0a8e4ff6403.jpg",
        "paste-53685d6d29359eb1fbfcb6fc65b6be3d714ab19f.jpg",
        "paste-5431e41618b55070c6960840bd99bf6e87896b04.jpg",
        "paste-5530b2182de1f9d815796383e61ffd642687eed4.jpg",
        "paste-5530fc973b7776f0374a6fc53026383b06d20e3a.jpg",
        "paste-557fa472dbcedf0d20d9250ec658a7d9c2c813b8.jpg",
        "paste-57bb1211adb25d28f65fdaa71067faa83d5d546b.jpg",
        "paste-5c64c72b14d0a52825a7f7cccd84d9e18c4008a4.jpg",
        "paste-5d801e8a6c7acba97dea7afc070bc5c8a664c117.jpg",
        "paste-5f0c755cbcf73d3dac8051bcc55724873ff9e96f.jpg",
        "paste-5f2763c6ffe8802e154076eeed5322424ff0861e.jpg",
        "paste-602296c3def84de27dd774984b5e463811e56952.jpg",
        "paste-620c45e5d9ec098e852496f6b1da4cad6754cf2b.jpg",
        "paste-62915ea3fe48543e013be2f5f3e93054f998683e.jpg",
        "paste-630a4330cfe62e2dcc6ab8fedc2f12b629f4d995.jpg",
        "paste-635a33aa070cf98fcaaf97b6d410628b65635b7f.jpg",
        "paste-64adda44c5eed1044c51f24ae3cbe6565515b0e3.jpg",
        "paste-67c4c5e28772f5596a2dd73134459003f66c7872.jpg",
        "paste-67e06902a69d07acf64038dbb58aac7e5dda049e.jpg",
        "paste-6a7692ec8ec97076a36f9eaaf6dcbe77de3f3998.jpg",
        "paste-6a9ff2fcf7279fc313697be13af78688586f6ea0.jpg",
        "paste-6cfeeefa6d0fc16fc2b7767773a468e2d0513457.jpg",
        "paste-6d815c27c9ca3c2266a23b6bd2e0d09e65398f9e.jpg",
        "paste-6ddd4206804af88eaf0a5d88b3ec5137785b15ef.jpg",
        "paste-6df15ee063d021ac661863513c6575b34765913b.jpg",
        "paste-6f374904ba1bd76b03462eb0211e22dad38fab5f.jpg",
        "paste-6f72e024a6386afbfabcbfbd6aa9d1c4007bde1b.jpg",
        "paste-715038db4ed468b22ba4a8bdb0fc0cbd7985c841.jpg",
        "paste-72f7c1b9217c4be3a9eeadc3238f41ae3e2a6629.jpg",
        "paste-746ff01510f75e6d1324db02d1f767e58b7759ae.jpg",
        "paste-76e99d220acfe215bc647dedd788fdbfe81cd19f.jpg",
        "paste-770661740410ceb972841af16d008930fab701c8.jpg",
        "paste-77199e91ce106a0e14c5834f8228329e264244ac.jpg",
        "paste-77bfdd016fc6b9a8e5b2ce2fce4dcef2dd192359.jpg",
        "paste-78a75427da936036c7d91999dbede25825d5ff3a.jpg",
        "paste-7a716a334e4151d55f1c9dda0d40c1fab962d1bb.jpg",
        "paste-7a9e0a452701836d59db815749fc74d4f7d76e37.jpg",
        "paste-7aa0eb2b481dd0ee7f4b0390474657ad792f59bf.jpg",
        "paste-7e6efca05feb10ecdd6a9adac2267ed56d1f5dad.jpg",
        "paste-7edc11a8786b67c8e08ca5f2b8318b32cd1fccfd.jpg",
        "paste-7ee7d849d3ab6d9c289970f4f43b24dc1e0a3495.jpg",
        "paste-7ef8bbe2146f8e96683f8f3ba0b3a252a5543765.jpg",
        "paste-80eb2fc6d014c340bfb9be96e461085c1617f539.jpg",
        "paste-82a1d99f2afa4ab8c197386133ec13cee911bb21.jpg",
        "paste-8389d8705f08af5eeb0b18c2d3525e11e1e910fb.jpg",
        "paste-83ec79f662e6f8863adb8658d34d4cbdec27a2e0.jpg",
        "paste-8426fba602999ee54c1003512201b9865330080c.jpg",
        "paste-84b4614c81c69bbf31ac52cf5ca1889e0b7577c7.jpg",
        "paste-8522d32fb03e12d71ccc378b1c1751b8f4e8d8dd.jpg",
        "paste-85c4fc5f63c9172690b3bdbb075e06b920b64d8e.jpg",
        "paste-87557fc48b83b6f9c964036afb122e99df455fea.jpg",
        "paste-89058b5eeff01b221cf9c44e108176713811baf5.jpg",
        "paste-8a3162453d26c55dbb1a1ef4caec05285d08319a.jpg",
        "paste-8acaba786d5cdddcb9e9ee4ed2a123f6081cc78d.jpg",
        "paste-8b0a31587577362d44611f92300f3c04df9f4364.jpg",
        "paste-8b2064791f7602a151a60eec60594703069e91c7.jpg",
        "paste-8b3006394a255cc0a7f15aaa5ba1e8e33e80d993.jpg",
        "paste-8b8731f3b04f6d35ac50bd29bd1f720cdecf236c.jpg",
        "paste-8d99fa0608fcae8162a4714848d5fc17259ca277.jpg",
        "paste-8f06883b5a3063ca1a1a840a19d9140254d2074b.jpg",
        "paste-8f3d6023662f006a078768533431b13d06404a4b.jpg",
        "paste-9019a144a987ffd7dab474bce66d7c70237867ea.jpg",
        "paste-9054bb976290a44e6fb533e7789ff00418d7eff8.jpg",
        "paste-91bfc8cb36dc8d4ad58ba110fb0187e13138bdab.jpg",
        "paste-92505791355b5a9c37b2fc13f6dd4d0ce61bfa09.jpg",
        "paste-9337b4b4ef2ef21d05ce20eaf9f728672f082869.jpg",
        "paste-9526b7a4c0b9b11797dd655b1ba565090e78fa5b.jpg",
        "paste-954677df3fd9afda610f4b08fc66641f224366a3.jpg",
        "paste-979791a8b9a8b5564bb0e6b95532ed100e41d6b3.jpg",
        "paste-9813930bd5adc2385c079b35dd6b9abfb584954f.jpg",
        "paste-98480122c2ee4d91fdc2a57e4e4e7ff5036df82e.jpg",
        "paste-98c1d8ae16c01b8446ebe10e5cac97219ca5e867.jpg",
        "paste-98e83f69744f849082e7914438261e9507d383cd.jpg",
        "paste-98fc340766848e064234ea8748d33d65628f1537.jpg",
        "paste-9904ea36a5b2da19627b2dfd2f8078cce251a1c6.jpg",
        "paste-9a74699cd3ce3c6fab9632bbc358240ed378e93a.jpg",
        "paste-9b1f701b517808b1ea2d090ab6cb0e16ada506b5.jpg",
        "paste-9d61e3a44cf9bcd79b05dfff151f6bfeff9671fa.jpg",
        "paste-9f2ecf6b91420d4004a528c53136eab995d307e4.jpg",
        "paste-9f3de22e16497eac34ee3b6cc7d3f53c98af72ad.jpg",
        "paste-a1a4671e6e5463108cf356621ab8611d1857ff28.jpg",
        "paste-a1ba950d36137c763de685e5f260c9cc7a1a69b4.jpg",
        "paste-a241e65638208ee89283e3a3be5273ae80f7fcba.jpg",
        "paste-a26606c7ecd88062e73f77d55869cf5e64b04954.jpg",
        "paste-a334fe72b91e9d783fcdc1682531bf69a38d71a8.jpg",
        "paste-a3dd219b307fd09e128041c31b6485e86f2a17e1.jpg",
        "paste-a3f1119ad86e2a543c86be09c65106691abf1939.jpg",
        "paste-a48b150879944286a3d072f4a36df8d593b1d155.jpg",
        "paste-a497fee30b3b5e117c1ef0271c0ad89aae3f91a2.jpg",
        "paste-a51992b91e0982975114da85208f3bbb9d5599c6.jpg",
        "paste-a520ea6c22c847734e00ca73e518fafb311acc64.jpg",
        "paste-a8efd147ccfefc8fdb2d4d9dfb0e076a63adc7d8.jpg",
        "paste-a9fa56d463a4d36dca38c3c5634d099a8fc813d0.jpg",
        "paste-aa90eb1fbd746390fc66eaf05fa98a891f95472e.jpg",
        "paste-aabfef2e5afac78682425e4ed0201dbc3590d55b.jpg",
        "paste-ac5f895650d6622dd83703b61c4a8853c8fecea7.jpg",
        "paste-ae3302e31cfdd09b61334d72ab047f0dea547841.jpg",
        "paste-af4955100c456877c9c291607e408227cd25a7d5.jpg",
        "paste-b2a7e4e864546cf9fb105231642e84ab46624f69.jpg",
        "paste-b2ae73a550d14b86d8fa47be8c3c02ef07c51efc.jpg",
        "paste-b4487dbc1eee9b2f91625a29eb74d87489f989c0.jpg",
        "paste-b5470d87c051406c3f342ce9809738d597774612.jpg",
        "paste-b5dd8629c74380fc1d2ff86059549c3a38e574f6.jpg",
        "paste-b7a8790124f2a2ad920c591b184339ea6aea9637.jpg",
        "paste-b92c8c83e0629539254251d2da62f11529507f5e.jpg",
        "paste-b9566cba1f9de1ee88e9ae1774fc064f38e6d9d6.jpg",
        "paste-b9ef0c4d03ad173145ec072062a37da50a2b7c5c.jpg",
        "paste-bb09f7cf3217d1843a92818a7fcc6cb5332bac22.jpg",
        "paste-bb739c684fb5b20104b3db1520668279bc0504f5.jpg",
        "paste-bc518497dd664c5c4ec1e85091f40dfbe6f1771d.jpg",
        "paste-bc765240107bfbb97d567dc7b4928698ad936626.jpg",
        "paste-bc8c5e9494b35e1b35bc3ca70d1e7c96968afc0b.jpg",
        "paste-bcdbe115d5fde9d2d7bc77071680f8b024484e81.jpg",
        "paste-bcf49851beb10d0da0d5f76da2e9ac3152d14a53.jpg",
        "paste-bfab39bde04cf38fe2aa112e6ad0bab1b70cadf4.jpg",
        "paste-bfd0ea4cbde38946359e2289e7e60f716d4457e6.jpg",
        "paste-c15498ef28acd6b0df42da94c384ff8f57ad583b.jpg",
        "paste-c20fef3441d4999151437bf26074db68df2c8c74.jpg",
        "paste-c2623a8d325fcd3e1daf69bc7f6898ceaa830a1b.jpg",
        "paste-c3352a7538b6b6dd1cab77a364769e27afe9c634.jpg",
        "paste-c420c9adfdde8768070020961c0fd9c67089b60a.jpg",
        "paste-c43bdb1b79e0bffde6cce66ca2afe5394aafb08f.jpg",
        "paste-c5bc8e29237dc4043a22233deac42b1753083ccb.jpg",
        "paste-c7b19ac120aa61f064c4025b98264b9f92f9c8ee.jpg",
        "paste-c914660266baacc38d6d191c4f9fbbc571c09f7b.jpg",
        "paste-cacd223ef1c514ef1b7b1c016f604036be127ed4.jpg",
        "paste-cbc2749090a368760bffa394b91f90f670253622.jpg",
        "paste-cd7a6d049143b20c98515f99629a6ee5788d57f4.jpg",
        "paste-cfa3e79b235a66f25cf89926fb8b99399dd72549.jpg",
        "paste-d01a81b70dd54b2610e03afe02f9e13931bc06e9.jpg",
        "paste-d08ce305789c960eff3b2184e35a137213c3c9d2.jpg",
        "paste-d0d53232e866a6a83f1bc12fc08ce786c554f519.jpg",
        "paste-d188d5a3d78d4604c09bc97bcb90d002b1fac379.jpg",
        "paste-d38c2ec87dc54f15f85671f9dfe4efe568c7c858.jpg",
        "paste-d99d54eb82d29a9655ca0bdeaf7559809e9bce4b.jpg",
        "paste-da37213a494a2e1a6c5bcd42c89f779e69bdf9de.jpg",
        "paste-da86cc7b3b210282e76f620ed79aebc677ab8428.jpg",
        "paste-dcc9cb54296caf0ffaf147777b54f90dd1b0bfe8.jpg",
        "paste-dd1889c23e4725354216d2d24f4c26c7c65b1d6e.jpg",
        "paste-e0d2ddd0bef5f69a9595dfbe351e2bc42a26eaea.jpg",
        "paste-e0e6495cc4354a7b8cf462f9e093a61b0a3e8b76.jpg",
        "paste-e142435093cb26d454eefb7e01485dc4128fec2b.jpg",
        "paste-e18b118a25b2874fbc31a28c774a9a343c31ab18.jpg",
        "paste-e1e84942023ee7850fb9546fb97c0ed83f4eb63a.jpg",
        "paste-e3d5316e976c70817ae8ee40ff4923932f46eada.jpg",
        "paste-e5b9154ba5b5795c8377ec54dccc497e01b61205.jpg",
        "paste-e67cefa81955b57eb93642f7a5cd5118665426e2.jpg",
        "paste-e74c11735dbae0770c4044c7f43496255f3d8a76.jpg",
        "paste-e801016f8dd6c6ee1b1680502f22ed303bfde8a5.jpg",
        "paste-e806b1ee80547c79c882a16979134553217facef.jpg",
        "paste-e8544894618a6cfe951ea6601a9ecfe8eb0509f5.jpg",
        "paste-e8a9ef21f999b619a39ca67965f152d92f8df1d8.jpg",
        "paste-e92a44df68c856075e0afbf8802462356f518dd2.jpg",
        "paste-eb387293d7ead88a49ba336272543f23fbd66fdf.jpg",
        "paste-ebb6343490bdf52e226640862d4d8bf80555d33e.jpg",
        "paste-eca264ba365e4229620babb7e71804f473a72866.jpg",
        "paste-ecf3fa4d8718e55f9e2c67d604e224045c150d72.jpg",
        "paste-eea6ae36c35883371506d888dc627c60e0da1712.jpg",
        "paste-ef61d61a52b570eb8e6ee42e4eb0644e81631f6b.jpg",
        "paste-ef6f8f10f634403d8238fd3bfa3fe68cccc82b37.jpg",
        "paste-ef773936b86c86533b0dc95592698ddbde70f18a.jpg",
        "paste-f54f507df528ec4359d11c7e49632f0b6691442c.jpg",
        "paste-f665f348f741ef923753f0564e3d627b1f297b7c.jpg",
        "paste-f914499c70c2ad1b1704e9a9369e97cbc2e6946b.jpg",
        "paste-f94bf7453a4dd955981ca92c5a7aa37d83d35e8e.jpg",
        "paste-fbc0fb2dc06869c79057ef7117e0fd31c2e20fb5.jpg",
        "paste-fc5f7b4444633219fa2cdff16ea593bfa1776146.jpg",
        "paste-fc68ef92c6c95986f7c9a92ed7ae44fe0fc2e05b.jpg",
        "paste-fe0a14148fdb389efbbdc22df0caebdaec657a17.jpg",
        "paste-feb7dba0835bb3e0089bf94fa7dab63c796dca8d.jpg",
        "patchemb.png",
        "posatt.png",
        "pup.png",
        "sSE.png",
        "savsmhsa.png",
        "seformerdecoder.png",
        "segc.png",
        "segformer.png",
        "segformerenc.png",
        "selfatt.png",
        "setr.png",
        "simplifiednl.png",
        "squeezeandexc.png",
        "transformer.png",
        "uncertaintytypes.png"
    ],
    "name": "Deep Learning for CV 2",
    "note_models": [
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "css": ".card {\n  font-family: arial;\n  font-size: 20px;\n  text-align: center;\n  color: black;\n  background-color: white;\n}\n",
            "flds": [
                {
                    "font": "Arial",
                    "name": "Front",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "font": "Arial",
                    "name": "Back",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Basic",
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tmpls": [
                {
                    "afmt": "{{FrontSide}}\n\n<hr id=answer>\n\n{{Back}}",
                    "bafmt": "",
                    "bfont": "",
                    "bqfmt": "",
                    "bsize": 0,
                    "did": null,
                    "name": "Card 1",
                    "ord": 0,
                    "qfmt": "{{Front}}"
                }
            ],
            "type": 0
        },
        {
            "__type__": "NoteModel",
            "crowdanki_uuid": "8a4770e1-8446-11ec-9773-182649c72b2b",
            "css": ".card {\n  font-family: arial;\n  font-size: 20px;\n  text-align: center;\n  color: black;\n  background-color: white;\n}\n\n.cloze {\n font-weight: bold;\n color: blue;\n}\n.nightMode .cloze {\n color: lightblue;\n}\n",
            "flds": [
                {
                    "font": "Arial",
                    "name": "Text",
                    "ord": 0,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                },
                {
                    "font": "Arial",
                    "name": "Back Extra",
                    "ord": 1,
                    "rtl": false,
                    "size": 20,
                    "sticky": false
                }
            ],
            "latexPost": "\\end{document}",
            "latexPre": "\\documentclass[12pt]{article}\n\\special{papersize=3in,5in}\n\\usepackage[utf8]{inputenc}\n\\usepackage{amssymb,amsmath}\n\\pagestyle{empty}\n\\setlength{\\parindent}{0in}\n\\begin{document}\n",
            "latexsvg": false,
            "name": "Cloze",
            "req": [
                [
                    0,
                    "any",
                    [
                        0
                    ]
                ]
            ],
            "sortf": 0,
            "tmpls": [
                {
                    "afmt": "{{cloze:Text}}<br>\n{{Back Extra}}",
                    "bafmt": "",
                    "bfont": "",
                    "bqfmt": "",
                    "bsize": 0,
                    "did": null,
                    "name": "Cloze",
                    "ord": 0,
                    "qfmt": "{{cloze:Text}}"
                }
            ],
            "type": 1
        }
    ],
    "notes": [
        {
            "__type__": "Note",
            "fields": [
                "Goals of Interpretability",
                "<b>Understanding</b><br><ul style=\"\"><li style=\"\">Input signals triggering the decisions of a model<br></li><li style=\"\">Parts of a model responsible for different decisions</li></ul><br><b>Identifying weaknesses</b><br><ul><li style=\"\">Spurious correlations</li></ul><br><b>Building trust and transparency</b><br><ul><li>Map decision process to human intuition</li><li>Ability to justify important decisions based on AI systems e.g., in medicine, law, autonomous driving</li></ul>"
            ],
            "guid": "D8A9xZF18Y",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "<b>{{c1::Goals of Interpretability}}</b><br><br><b>{{c2::</b><b>Understanding</b><b>}}</b><br><ul><li><b>{{c3::</b>Input signals triggering the decisions of a model<b>}}</b><br></li><li><b>{{c3::</b>Parts of a model responsible for different decisions<b>}}</b><br></li></ul><br><b>{{c2::</b><b>Identifying weaknesses}}</b><br><ul><li><b>{{c3::</b>Spurious correlations<b>}}</b><br></li></ul><br><b>{{c2::</b><b>Building trust and transparency}}</b><br><ul><li><b>{{c3::</b>Map decision process to human intuition<b>}}</b><br></li><li><b>{{c3::</b>Ability to justify important decisions based on AI systems\ne.g., in medicine, law, autonomous driving<b>}}</b></li></ul>",
                ""
            ],
            "guid": "ooLwE5Ta^S",
            "note_model_uuid": "8a4770e1-8446-11ec-9773-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Types of Interpretability in CV",
                "<ul><li>Pixel attributions</li><li>Attributions on deeper layers</li><li>Understanding and manipulating single neurons</li></ul>"
            ],
            "guid": "F.RvK/zvHB",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Perturbation: Idea",
                "<ul><li>Occlude parts of the image with a sliding window<br></li><li>Compute the prediction scores of the target label#<br></li><li>Create a heatmap based on the score drops<br></li></ul>"
            ],
            "guid": "dH21CUpX=l",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Perturbation: More Elaborate Methods",
                "<ul><li>Smoothen the masks or use blurring<br></li><li>Occlude simultaneously with multiple masks<br></li><li>Learn to dilate the masks<br></li></ul>"
            ],
            "guid": "x6BM~4`e1E",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Perturbation: Advantages and Limitations",
                "<b>Advantages</b><br><ul style=\"\"><li style=\"\">Very simple to implement<br></li><li style=\"\">Model agnostic<br></li></ul><b>Limitations</b><br><ul style=\"\"><li style=\"\">Too slow<br></li><li style=\"\">Sensitive to the size of the box &amp; stride<br></li><li style=\"\">Cannot capture fine details<br></li></ul>"
            ],
            "guid": "qw|s-9M``b",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Intepretability, Gradients: Idea",
                "<ul><li>Assume f is the function learned by a model,&nbsp;\\(x_{ij}\\) the variable\nrepresenting the input pixel in the (i, j)-th position and \\(f_c(x)\\) the prediction for class c given image \\(x\\)</li><li>The partial derivatives of the image class with respect to\neach pixel can be used as pixel attributions<br></li></ul>"
            ],
            "guid": "zFXhv;p{IJ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability, Gradients: Idea, sketch",
                "<img src=\"gradients-interpretability.png\">"
            ],
            "guid": "/U%Bl<pAQ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability, Gradients: Variations",
                "Problem: vanishing gradients, noise<br>Approaches to migitate this:<br><ul><li>Gradient times input<br></li><li>Integrated gradients<br></li><li>SmoothGrad<br></li></ul><img src=\"gradientbased.png\"><br>"
            ],
            "guid": "z`XDd>Y<&#",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability: Gradient times input",
                "<ul><li>Element-wise product of the input and the gradient \\[R^c_{ij}(x) = x_{ij}\\, \\cdot \\, \\frac{\\partial f_c}{\\partial x_{ij}}(x) \\]</li><li>Mitigates \"gradient saturation\", reduces visual diffusion</li></ul>"
            ],
            "guid": "P[q(e!8UpR",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability: Integrated gradients",
                "<ul><li>Leverage a \"baseline image\" \\(x'\\) (often random noise or black image)</li><li>t - controls the interpolation between the baseline \\(x'\\) and target image \\(x\\)</li><li>Sums gradients over gradual\nmodification of the input from a\nbaseline value to the original value of&nbsp;\\[R^c_{ij}(x) = (x_{ij} - x'_{ij})\\int_0^1 \\frac{\\partial f_c(x' - t(x- x'))}{x_{ij}}dt\\]</li><li>Mitigates \"gradient saturation\", reduces visual diffusion</li></ul>"
            ],
            "guid": "Bq)UT<6w2`",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability: SmoothGrad",
                "<ul><li>Use a \"base\" attribution method&nbsp;\\(R^c_{ij}\\)</li><li>Average over explanations of noisy copies of an input&nbsp;\\[R_{ij}^c(x)_{smooth} = E[R_{ij}^c(x+g)], \\; where \\; g \\sim \\mathcal{N}(0, \\sigma^2I_n) \\]<br></li><li>Against noise and visual diffusion</li><li>Can be combined with different \"base\" attribution methods</li></ul>"
            ],
            "guid": "kFWG{1S-6K",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Intepretability, Gradients: Advantages and Limitations",
                "<b>Advantages</b><br><ul style=\"\"><li style=\"\">Direct to compute for any network<br></li><li style=\"\">Fast<br></li><li style=\"\">Close to the structure of the model<br></li></ul><b>Limitations</b><br><ul style=\"\"><li style=\"\">Noisy attributions<br></li><li style=\"\">Focus more on details than concepts<br></li></ul>"
            ],
            "guid": "vf995xJu3/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability, Grad-CAM: Idea",
                "<ul><li>Instead of computing attributions on the pixel level where they are noisy, compute\nthem on higher layer feature maps<br></li><li>The larger number of feature maps available close to the output of networks, allow\nto define attributions based on the contribution of the features</li></ul>"
            ],
            "guid": "f+A<XUAkJF",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability, Grad-CAM: Idea - Sketch",
                "<img src=\"gradcam.png\">"
            ],
            "guid": "QNJ|Wk#f.,",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Interpretability, Grad-CAM: Algorithm",
                "<ol><li>Standard forward pass -&gt; predicted class</li><li>Estimate the gradient of the output \\(y_c\\) (before softmax), with\nrespect to feature map activations&nbsp;\\(A_k\\) of a convolutional layer<br></li><li>Obtain the feature importance&nbsp;\\(a_k^c\\) from each individual feature\nmap&nbsp;\\(k\\) by averaging the gradients over all its \\(H \\times W\\) locations<br></li><li>In each location&nbsp;\\((i, j)\\)&nbsp;we linearly combine the feature values by\nweighting their feature importance, followed by ReLU</li></ol><ul><li>\\(a_k^c = \\frac{1}{H\\cdot W} \\sum_{i=1}^H \\sum_{j=1}^W \\frac{\\partial f^c}{\\partial A_{ij}^k}(x)&nbsp;\\)<br></li><li>\\(R_{ij}^c(x) = ReLU\\left( \\sum_{k=1}^K a_k^c A_{ij}^k \\right)\\)<br></li></ul>"
            ],
            "guid": "nH~t_Y6.{G",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "LRP: Layer-wise Relevance Propagation: Idea",
                "<ul><li>Backpropagate the relevance \\(R\\) which preserves the\ntotal prediction and indicates the contribution of each neuron<br></li><li>Conservation property: what has been received by a\nneuron must be redistributed to the lower layer in equal\namount (total relevance is preserved along layers):&nbsp;\\[\\sum_j R_j = \\sum_k R_k\\]</li><li>The following rule should hold:&nbsp;\\[R_j = \\sum_k \\frac{z_{jk}}{\\sum_j z_{jk}} R_k\\]</li></ul>"
            ],
            "guid": "dEALqq^-c6",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "LRP: Propagation rules",
                "<ul><li>LRP-0: Basic rule which is equivalent to gradient x input&nbsp;\\[R_j = \\sum_k \\frac{a_jw_{jk}}{\\sum_{0,j}a_jw_{jk}}R_k\\]<br></li><li>LRP - \\(\\varepsilon\\)&nbsp;Reduces the gradient noise by suppressing\ncontradicting of insignificant information&nbsp;\\[R_j = \\sum_k \\frac{a_jw_{jk}}{\\varepsilon + \\sum_{0,j}a_jw_{jk}}R_k\\]</li><li>LRP-\\(\\gamma\\): Favours positive contributions to the output&nbsp;\\[R_j = \\sum_k \\frac{a_j(w_{jk} + \\gamma w_{jk}^+)}{\\sum_{0,j}a_j(w_{jk} + \\gamma w_{jk}^+)}R_k\\]</li></ul>"
            ],
            "guid": "lk)wPx1UKB",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "LRP: Composite LRP",
                "<b>Advantages/disadvantages of different rules</b><br><ul style=\"\"><li style=\"\">LRP-0: noisy</li><li style=\"\">LRP-\\(\\varepsilon\\): reduces the signal of the propagation</li><li style=\"\">LRP-\\(\\gamma\\): might keep irrelevant information</li><li style=\"\">Networks are noisier on the first layers and\nmore specific on the last layers, thus\ncombining the rules improves results<br></li></ul>"
            ],
            "guid": "wIC3)qGtfB",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "LRP: Advantages and Limitations",
                "<b>Advantages</b><br><ul style=\"\"><li style=\"\">High detail attributions without the noise of gradients<br></li><li style=\"\">Fast to compute<br></li><li style=\"\">Encompasses other methods<br></li></ul><b>Limitations</b><br><ul style=\"\"><li style=\"\">Propagation rules need to be defined for each type of layer<br></li><li style=\"\">Too many hyper-parameters to configure<br></li></ul>"
            ],
            "guid": "r6z=GTdr%f",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Unit Dissection: Unit activation threshold",
                "<ul><li>Examine single units of a classifier, map them to the semantic concepts<br></li><li>Given a unit (neuron) \\(u\\), let be \\(a_u (x, (i, j))\\) the output of the unit at location \\((i, j)\\)\nwhen the image \\(x\\) is passed through the network<br></li><li>For each unit: based on the top 1% percentile of the activation values, select\nthe neuron-specific activation threshold \\(t_u\\)<br></li></ul>"
            ],
            "guid": "C^=4k1!.K",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Unit Dissection: Match outputs to concepts",
                "For new images<br><ul><li>For each unit compute which regions “trigger” it (output \\(&gt; t_u\\) ). The result is a unit-specific segmentation mask<br></li><li>Use an external segmentation network, to get ground-truth of concepts<br></li><li>Get IOU per concept for all images and assign the class with the highest score to unit<br></li></ul>"
            ],
            "guid": "num5KP>N<:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Unit Dissection: Distributions of concepts",
                "<img src=\"distofconcepts.png\">"
            ],
            "guid": "upaPxY/hNe",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Unit Dissection: Dissect",
                "<ul><li>Blocking the output of the top units associated with the concepts with the highest\nimpact on a class make it non-predictable<br></li><li>Removing the most important units of a layer, render the network output random<br></li><li>Removing the least important units of a layer has minimal impact<br></li></ul>"
            ],
            "guid": "MpHf!|zT2o",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Unit Dissection: GAN Dissect",
                "<ul><li>Applying the same process on GANs: suppress/exaggerate unit-specific outputs<br></li><li>Strong indication that both classification and generation models develop objectspecific units<br></li></ul>"
            ],
            "guid": "pfn(#bp6Qy",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Limitations of Interpretability Methods",
                "<ul><li>Many attribution methods inherit\nbiases from the input image<br></li><li>Example below: corrupt different\nlayers from top to bottom<br></li><li>Result: still get reasonable output<br></li><li>&nbsp;-&gt;&nbsp;If you want to analyze your\nnetwork: use a combination of\ndifferent methods, do not take the\nresult 100% for granted</li></ul>"
            ],
            "guid": "Az+zjF;L}6",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Limitations of Attribution Methods",
                "<ul><li>Prone to biases inherited from the input image<br></li><li>Some methods are less effective when applied to deeper or\nmore 'exotic' networks<br></li><li>Still no widely accepted evaluation methods<br></li><li>Though single units have been analyzed, still no thorough\nanalysis of groups of units<br></li></ul>"
            ],
            "guid": "DcJLXV1~-I",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Types of predictive uncertainty",
                "<img src=\"uncertaintytypes.png\">"
            ],
            "guid": "vnF?])qqbs",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Aleatoric uncertainty",
                "<ul><li>Data uncertainty</li><li>Noise inherent in the observations</li><li>In theory, can be explained away by unlimited sensing</li><li>Irreducable, but can be learned</li><li>Types: Homoscedastic (same for all inputs, e.g imprecise sensor), Heteroscedastic (uncertainty can vary with input)</li></ul>"
            ],
            "guid": "xcBb0|eF$,",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Epistemic uncertainty",
                "<ul><li>Model uncertainty, reducible</li><li>Measures what your model doesn't know</li><li>In theory, can be explained away by unlimited data</li></ul>"
            ],
            "guid": "KR4DJCVA|k",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Measuring Uncertainty",
                "<ul><li>Expected Calibration Error</li><li>Proper scoring rules (Negative log likelihood, brier score)</li><li>Out-of-distribution detection</li></ul>"
            ],
            "guid": "M>}(D?]POa",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Expected Calibration Error: Idea",
                "<ul><li>Model probability estimates should\nideally, indeed reflect the probability of\na correct outcome -&gt; Should reflect model accuracy</li><li>Example: If the accuracy of a model\nis 70%, the average confidence\n(pribability estimates) should be 0.7<br></li><li><img src=\"ece.png\"></li></ul>"
            ],
            "guid": "L7R8sH@ZzM",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Expected Calibration Error (ECE): Formula",
                "<ul><li>Perfect model&nbsp;\\[\\mathbb{P}(a_{pred} = a_{true} | conf(a_{pred}) = p) = p, \\quad \\forall p \\in [0,1]\\]</li><li>Expected Calibration Error (ECE)&nbsp;\\[ECE=\\sum_{i=1}^K \\frac{N_{bin_i}}{N_{total}} | acc(bin_i) - conf(bin_i) |\\]</li></ul>"
            ],
            "guid": "bX^HZzl3yk",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Proper scoring rules",
                "Negative Log-Likelihood<br><ul><li>Also known as cross-entropy</li><li>Can overemphasize tail probalities</li></ul>Brier Score<br><ul><li>Mean squared error of the outcome<br></li><li>Can be numerically unstable to optimize<br></li><li>Insensitive towards rare classes in the test set<br></li><li>Binary Brier Score&nbsp;\\[BS=\\frac{1}{N}\\sum^N_{t=1}(f_t-o_t)^2\\]</li><li>Multi-Class Brier Score&nbsp;\\[BS = \\frac{1}{N}\\sum_{t=1}^N \\sum_{i=1}^R (f_{ti} - o_{ti})^2\\]</li></ul><br>"
            ],
            "guid": "D0z]OHvr^$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Out-of-distribution Detection",
                "<b>Dataset shift</b><br><ul><li>Covariate shift:&nbsp;distribution of features\n\\(p(x)\\) changes, \\(p(y|x)\\) is fixed -&gt; Data appearance changes</li><li>Open set recognition: new classes may appear at test-time</li></ul>"
            ],
            "guid": "k_B<{^wxl3",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Open Set Recognition",
                "<ul><li>Test time: known and unknown classes</li><li>View it as a binary problem (known vs. unknown)</li><li>Area under the ROC curve</li><li>Multi-Class categorization: (N known + 1 unknown classes)</li></ul>"
            ],
            "guid": "Hu(E/@s#%",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Methods for quantifying uncertainty",
                "<ul><li>Calibration-based methods</li><li>Bayesian Neural Networks</li><li>Ensemble-based methods</li></ul>"
            ],
            "guid": "wp^R*Mj[7}",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why not use Softmax score directly?",
                "<ul><li>Reliability of confidence values often not considered<br></li><li>Excellent Top-1 recognition results, but biased towards\nhigh confidence predictions<br></li><li>neural networks are miscalibrated (see\ngraphics)<br></li><li>99% confidence predictions on\nimages not recognizable to human eye<br></li></ul>"
            ],
            "guid": "Ol4{U*VV%E",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Calibration-based methods",
                "<ul><li>Post-hoc methods for calibration of confidence values</li><li>histogram binning</li><li>Isotonic Regression</li><li>Bayesian Binning into Quantiles</li><li>Temperature Scaling</li></ul>"
            ],
            "guid": "k|%&L[|f>n",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weak spot of calibration-based methods",
                "<ul><li>post-hoc calibration: changes in data distribution</li></ul>"
            ],
            "guid": "l+<.NL#OOF",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Post-hoc methods for calibration of confidence values",
                "<ul><li>A separate held-out validations set used for calibration</li><li>Train the core model</li><li>Use the held-out validation set to improve the confidences</li><li>Do not explicitly disentangle aleatoric and epistemic uncertainties</li></ul>"
            ],
            "guid": "t@JM;K1hUx",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Histogram binning",
                "<ul><li>Divide the original predictions into M bins</li><li>Either divide the probability space into M equal intervals&nbsp;\\(B_1 \\dots B_M\\), or</li><li>Assure equal amount of samples into each bin</li><li>Estimate a calibrated score&nbsp;\\(\\theta<br>_m\\) for each bin&nbsp;\\(m\\)</li><li>At test time, if the prediction&nbsp;\\(\\theta_m\\) falls into bin&nbsp;\\(B_m\\), then the calibrated prediction is&nbsp;\\(\\theta_m\\)&nbsp;</li><li>The&nbsp; predictions&nbsp;\\(\\theta_m\\) are chosen to minimize the bin-wise squared loss:&nbsp;\\( min_{\\theta_1, \\dots , \\theta_M} \\sum_{m=1}^M \\sum_{i=1}^n \\underbrace{1(a_m \\leq \\hat{p}_i \\lt a_{m+1})}_{indicator \\; function}(\\theta_m - y_i)^2\\)</li></ul>"
            ],
            "guid": "oN^J9#T+.M",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Isotonic Regression",
                "<ul><li>Learns a piecewise constant function&nbsp;\\(f\\) to transform uncalibrated outputs&nbsp;\\(\\hat{p}_i: \\hat{q}_i = f(\\hat{p}_i)\\)</li><li>Strict generalization of histogram binning: bin predictions and bin boundaries are jointly optimized</li></ul>"
            ],
            "guid": "t^/Hqq&gG:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Bayesian Binning into Quantiles",
                "<ul><li>An ensemble of multiple Histogram Binning models,\nweighted averaging over many binning schemes as the\nresulting probability estimate</li></ul>"
            ],
            "guid": "v1P-VwiOFH",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Temperature Scaling",
                "<ul><li>Temperature parameter&nbsp;\\(\\tau\\)</li><li>Same value for all inputs</li><li>Optimized on held-out data</li><li>All output neurons (logits) are divided by the same value</li><li>Simple, but highly effective</li><li>Probably the most popular method today</li></ul>"
            ],
            "guid": "C%J++[GmAc",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Temperature Scaling Formula",
                "<ul><li>Standard Softmax-normalized logits&nbsp;\\[conf(a_{pred}) = max_{a \\in A} \\frac{exp(y_a)}{\\sum_{\\hat{a} \\in A} exp(y_{\\hat{a}})}\\]</li><li>Temperature scaling \\[conf(a_{pred}) = max_{a \\in A} \\frac{exp(\\frac{y_a}{\\tau})}{\\sum_{\\hat{a} \\in A} exp(\\frac{y_{\\hat{a}}}{\\tau})}\\]</li></ul>"
            ],
            "guid": "PmY2+~1?Xm",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Bayesian Neural Networks Overview",
                "<ul><li>Weights with a distribution on each weight</li><li>Output: a distribution instead of pseudo-probability point estimates</li></ul>"
            ],
            "guid": "HC!txFb/=c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Bayesian Neural Networks Formula",
                "<img src=\"bayesian.png\">"
            ],
            "guid": "L%5]Ej1~m4",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dropout Idea",
                "<ul><li>Regularization technique: very simple and effective<br></li><li>Training: in each iteration, ignore (zero out) a random fraction, \\(p\\), of nodes (and corresponding\nactivations)<br></li><li>Inference (standard NN): dropout deactivated<br></li></ul>"
            ],
            "guid": "ze%UsaW+Qt",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dropout for BNN approximation",
                "<b>Conventional Neural Network</b><br><ul style=\"\"><li style=\"\">Inference is deterministic, dropout inactive at test-time</li></ul><b>BNN Approximation via Monta-Carlo Dropout</b><br><ul style=\"\"><li style=\"\">Probabilistic inference, dropout active at test-time<br></li><li style=\"\">Compute T stochastic forward passes with active dropout<br></li><li style=\"\">Calculate the output statistics: Mean is used for prediction, Variance is out measure of epistemic uncertainty<br></li></ul>"
            ],
            "guid": "nkeV7L*Jq2",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BNN Approximation with Monte-Carlo Dropout, Formula",
                "<ul><li>Predictive mean&nbsp;\\[E(a_i|x) \\approx \\frac{1}{T} \\sum_1^T softmax(WD_T \\cdot x_{emb}+b)\\]</li><li>Predictive Unvertainty&nbsp;\\[U(a_i|x) \\approx \\frac{1}{T-1} \\sum_1^T [softmax(WD_T \\cdot x_{emb}+b) - E(a_i|x)]\\]</li></ul>"
            ],
            "guid": "hP.<-4c5ky",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Ensemble based uncertainty estimation",
                "<ul><li>Use multiple hypothesis to obtain a better one</li><li>Ensemble variance is the measure of uncertainty</li><li>MC-Dropout can be viewed as shared weights ensemble</li><li>Very simple and suprisingly effective under dataset shift</li><li>Often competition winners</li></ul>"
            ],
            "guid": "qXucNP5fcy",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Problems of ensemble-based uncertainty estimation",
                "<ul><li>Computational cost</li><li>Increases linearly with number of ensembles</li></ul>"
            ],
            "guid": "LQJNL)rMF2",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BatchEnsemble",
                "For each layer i:<br><ul><li>Weight matrix&nbsp;\\(W_i\\) shared</li><li>For each ensemble&nbsp;\\(m\\): multiply&nbsp;\\(W_i\\) by the outer product of two vectors:&nbsp;\\(r_m\\) and&nbsp;\\(s_m\\):&nbsp;\\[\\bar{W}_i&nbsp; = W \\circ F_i,\\; where\\; F_i = s_i r_i ^T\\]</li><li>Vectors&nbsp;\\(r_m\\) and&nbsp;\\(s_m\\) are different for each ensemble member.&nbsp;\\(W_i\\) is shared.</li><li>Additional batch optimizations</li></ul>"
            ],
            "guid": "L0JPO4<Mx=",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BatchEnsemble Advantages",
                "<ul><li>Much better in terms of time and memory</li><li>Only slightly worse ECE performance</li></ul>"
            ],
            "guid": "p&-Ma=]b~a",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why are native neural networks overconfident?",
                "<ul><li>Overfitting to the 0/1 labels<br></li><li>Nature of the Softmax function:&nbsp;Passing a point estimate of a function through a\nSoftmax results in extrapolations with unjustified\nhigh confidence for points far from the training data<br></li></ul>"
            ],
            "guid": "Q6U{*jzl7t",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why non-local Neural Networks?",
                "<ul><li>Capturing long-range dependencies is of central importance in DNNs<br></li><li>Convolutional and recurrent operations both process local neighborhood, either in space or time<br></li><li>Long-range dependencies can only be captured when these operations are applied repeatedly,\npropagating signals progressively</li></ul>"
            ],
            "guid": "F=y75:<Iwy",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Non-local Neural Networks: Idea",
                "<ul><li>A non-local operation computes the response at a position as a weighted sum of the features at\nall positions in the input feature maps<br></li><li>The set of positions can be in space, time, or spacetime<br></li><li>Can be applied for image, sequence, and video problems<br></li></ul>"
            ],
            "guid": "D^L<Ol~$B5",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Non-local Neural Networks: Basic Formula",
                "\\[y_i = \\frac{1}{\\mathcal{C}(x)}\\sum_{\\forall j} f(x_i, x_j) g(x_j)\\]<br>where&nbsp;<br><ul><li>\\(i\\): index of an outpout position (in space, time or spacetime)<br></li><li>\\(j\\): index that enumerates all positions<br></li><li>\\(x\\): input signal<br></li><li>\\(y\\): output signal, same size as&nbsp;\\(x\\)<br></li><li>\\(f\\): pairwise function that computes a scalar<br></li><li>\\(g\\): computes a representation of the input signal at position&nbsp;\\(j\\)<br></li><li>\\(\\mathcal{C}(x)\\): response is normalized by this factor<br></li></ul>"
            ],
            "guid": "qh|f_C[xq)",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Non-local Neural Networks Function choices",
                "<ul><li>\\(g\\) is in the form of a linear embedding:&nbsp;\\(W_g\\) is a weight matrix to be learned:&nbsp;\\(g(x_i) = W_gx_j\\). Can be implemented as&nbsp;\\(1\\times 1\\) convolution in space or&nbsp;\\(1\\times 1 \\times 1\\) convolution in spacetime.</li></ul>Choices for the pairwise function&nbsp;\\(f\\):<br><ul><li>Gaussian function&nbsp;\\(f(x_i, x_j) = e^{x_i^T x_j}\\)</li><li>Embedded Gaussian function with two embeddings&nbsp;\\(\\Theta\\) and&nbsp;\\(\\Phi\\)&nbsp;\\(f(x_i, x_j) = e^{\\theta(x_i)^T\\phi(x_j)}\\).</li><li>Dot product similarity:&nbsp;\\(f(x_i, x_j) = \\theta(x_i)^T\\phi(x_j)\\)</li><li>Concatenation:&nbsp;\\(W_f\\): a weight vector projects the concatenated vector to a scalar:&nbsp;\\(f(x_i, x_j) = ReLU(w_f^T[\\theta(x_i), \\phi(x_j)])\\).</li></ul>"
            ],
            "guid": "f3z(6F=u1Y",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Non-local Neural Networks: Non-local block",
                "<ul><li>Figure shows an implemenation using the embedded Gaussian version:</li></ul><img src=\"non-local.png\"><br>"
            ],
            "guid": "@Yg!fb]o~",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Non-local Neural Networks: findings",
                "<ul><li>Non-local behavior is important and insensitive to the instantiations</li><li>More non-local blocks generally lead to better results (multiple non-local blocks can perform multi-hop communication, messages can be delivered back in spacetime which is hard to do with local models)</li></ul>"
            ],
            "guid": "wsq7e9+I!k",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Asymmetric Non-local Neural Networks",
                "<ul><li>For vision tasks, the spatial size (\\(N=H\\times W\\)) is large, making the computation complexity and\nmemory requirement very high (Quadratic complexity with the size of the input feature (\\(N \\times N\\)))</li><li>Asymmetric non-local neural networks uses sampling methods to reduce the computation, e.g.,\nvia pooling or pyramid pooling (\\(N\\gg S)\\)<br></li></ul><img src=\"assymetric-nonlocal.png\"><br>"
            ],
            "guid": "f|CCYAul,G",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Squeeze-and-Excitation Networks: Idea",
                "<ul><li>Proposing a mechanis, that allows the network to perform feature recalibration</li><li>Use global information to selectively emphasize informative features and supress less useful ones</li><li>Introducing an architectural unit, termed as \"Squeeze-and-Excitation\" (SE) block</li></ul>"
            ],
            "guid": "HaW{s~yVPt",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Squeeze-and-Excitation Block",
                "<ul><li>Squeeze operation, aggregates the feature maps across spatial dimensions to produce a channel descriptor</li><li>Excitation operation, in which sample-specific activations, learned for each channel by a self-gating mechanism based on channel dependence, govern the excitation of each channel</li><li>The input feature maps are then reweighted to generate the output</li></ul><img src=\"squeezeandexc.png\"><br>"
            ],
            "guid": "oj8CG4ghWO",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Squeeze Operation",
                "Squeeze: Global Information Embedding<br>\\[z_c = F_{sq}(u_c) = \\frac{1}{H \\times W} \\sum_{i=1}^H \\sum_{j=1}^W u_c(i, j)\\]<br><ul><li>To squeeze global spatial information into a channel descriptor, by using global average\npooling to generate channel-wise statistics<br></li><li>More sophisticated aggregation strategies could be employed as well<br></li></ul>"
            ],
            "guid": "lLR|*JEBH/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Excitation Operation",
                "Excitation: Adaptive Recalibration<br>\\[s = F_{ex}(z,W) = \\sigma(g(z, W)) = \\sigma(W_2\\delta (W_1z))\\]<br><ul><li>\\(\\sigma\\): sigmoid activation,&nbsp;\\(\\delta\\): ReLU activation<br></li><li>Two FC layers</li><li>A dimensionality reduction layer with parameters&nbsp;\\(w_1 \\in \\mathbb{R}^{\\frac{C}{r} \\times C}\\)</li><li>A dimensionality increasing layer with parameters&nbsp;\\(w_2 \\in \\mathbb{R}^{C \\times \\frac{C}{r}}\\)</li></ul>"
            ],
            "guid": "f|gW:BS_V?",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Squeeze-and-Excitation Block Output",
                "<ul><li>Output&nbsp;\\(\\tilde{X} = [\\tilde{x}_1, \\tilde{x}_2, \\dots, \\tilde{x}_c]\\)</li><li>By scaling U with activations&nbsp;\\(\\tilde{x}_c = F_{scale}(u_c, s_c) = s_c \\cdot u_c\\)</li><li>Channel-wise multiplication between the feature map \\(u_c\\)(\\(H\\times W\\)) and the scalar&nbsp;\\(s_c\\)<br></li><li>The activations act as channel weights adapted to the input-specific descriptor<br></li></ul>"
            ],
            "guid": "r&L,:y-u(s",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Squeeze and Excitation Networks: Model Complexity",
                "\\[\\frac{2}{r} \\sum_{s=1}^S N_s \\cdot C_s^2\\]<br><br>Additional parameters:<br><ul><li>\\(r\\) reduction ratio<br></li><li>\\(S\\) numer of stages, each stage is a collection of blocks operating on feature maps<br></li><li>\\(C_s\\) the dimension of output channels<br></li><li>\\(N_s\\) number of the repeated blocks for stage s<br></li></ul>"
            ],
            "guid": "m-jKMVSVEk",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Channel Squeeze and Spatial Excitation",
                "<ul><li>Original SE block (cSE): Spatial Squeeze and Channel Excitation (Encodes channel-wise dependencies, ignores less important channels and emphasizes the important ones)</li><li>sSE: Channel Squeeze and Spatial Excitation: Provides more importance to relevant spatial locations and ignores irrelevant ones. Important for dense prediction tasks like semantic segmentation</li></ul><img src=\"sSE.png\"><br>"
            ],
            "guid": "J2jyJdb-`J",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Non-local meets Squeeze-Excitation",
                "<ul><li>The simplified NL block shares similar structure with SE block</li><li>Global Context (GC) block unifies the simplified NL block and SE block</li><li>a context modeling module which aggregates the features of all positions together to form a global context\nfeature<br></li><li>a feature transform module to capture the channel-wise interdependencies<br></li><li>a fusion module to merge the global context feature into features of all positions<br></li></ul><img src=\"segc.png\"><br>"
            ],
            "guid": "E::?9^SCP$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Simplified non-local block",
                "<ul><li>Using a query-independent attention map for all query positions<br></li><li>Then we add the same aggregated features using this attention map to the features of all query positions for\nforming the output<br></li><li>Significantly smaller computation cost than the original nonlocal block, but with almost no decrease in accuracy<br></li></ul><img src=\"simplifiednl.png\"><br>"
            ],
            "guid": "nGRKlH4~^8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why Transformers?",
                "<ul><li>Most sequence transduction models are based on the encoder-decoder architecture<br></li><li>Problem of RNN: Linear dependence leads to poor parallel calculation, since current calculations\nrely on the previous results.<br></li><li>Problem of CNN: Convolution is difficult to capture long-range dependencies, which is important\nfor long sequences<br></li><li>Transformer replaces recurrence and convolutions with only attention mechanisms, for both\nefficiency and long-range dependencies.<br></li></ul>"
            ],
            "guid": "FksOwP.N2W",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transformer: Basic Architecture",
                "<ul><li>Encoder-Decoder architecture:</li></ul><b>Encoder:</b><br><ul style=\"\"><li style=\"\">&nbsp;A stack of (\\(N=6\\)) identical layers/blocks</li><li style=\"\">each layer/block has two sub-layers:</li><li style=\"\">Multi-Head Self-Attention (MHSA) layer and Feed Forward (FF) network/layer</li></ul><b>Decoder:</b><br><ul style=\"\"><li style=\"\">Similar to Encoder's structure</li><li style=\"\">To prevent from attending to subsequent positions, insert an additional third layer: Masked MHSA layer</li></ul><img src=\"transformer.png\"><br>"
            ],
            "guid": "P3/qywYz}n",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-Attention Mechanism",
                "<ul><li>Inputs: <b>Q</b>uery, <b>K</b>ey and <b>V</b>alue are calculated by multiplying the input embedding by respective matrix:&nbsp;\\(Q = XW^Q, K=XW^K, V=XW^V\\)</li><li>Dot-Product: \\(MatMul(Q, K)\\) get weights</li><li>Scaled: weights are scaled to stabilize the gradient. (optional) Sequence Mask: Masked out the future attention (as -inf), then future information as unknown to Decoder</li><li>Softmax: softmax is applied to get probabilities</li><li>Output: \\(Z=\\)weighted&nbsp;\\(V\\)</li><li>\\(Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\\)<br></li><li>where&nbsp;\\(d_k\\) is the key dimension</li></ul><img src=\"selfatt.png\"><br>"
            ],
            "guid": "bZnp.ETL2R",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Multi-Head Self-Attention",
                "<b>Self-Attention Mechanism:</b><br><ul style=\"\"><li style=\"\">Only single attention function, which means single pattern of attention is learned for each token</li></ul><b>Multi-Head Self-Attention</b><br><ul style=\"\"><li style=\"\">Employ (\\(h = 8\\)) parallel attention heads to make model learn\nh different attention patterns for each token.<br></li><li style=\"\">Concatenate all heads and multiply\nwith the output weight matrix.<br></li><li style=\"\">\\(MultiHead(Q,K,V) = Concat(head_1, \\dots, head_n)W^O\\)<br></li><li style=\"\">where&nbsp;\\(head_i\\) =&nbsp;\\(Attention(QW_i^Q, KW_i^K, VW_i^V)\\)</li><li style=\"\">Number of self-attention head, \\(h=8\\).<br></li><li style=\"\">The matrix \\(W^O\\) is learnable and used to project weighted output.<br></li></ul>"
            ],
            "guid": "x%U$1X(PwE",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-Attention + MHSA Sketch",
                "<img src=\"savsmhsa.png\">"
            ],
            "guid": "bOD8?x>0Nc",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transformer: Feed-Forward Network(FFN)",
                "<ul><li>Stack two fully connected feed-forward layers after MHSA<br></li><li>The same FFN is applied to each position separately and identically, so\ncalled position-wise FFN<br></li><li>\\(FFN(x) = max(0, xW_1 +b_1)W_2+b_2\\)<br></li></ul>"
            ],
            "guid": "r-#3<G@635",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transformer:&nbsp;Residual and Layer Normalization (Add &amp; Norm)",
                "A residual connection is added around each of the two sub-layers,\nfollowed by layer normalization<br><img src=\"addnnorm.png\"><br>"
            ],
            "guid": "H};$=X;~Z@",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transformer: Position information of sequence",
                "<ul><li>RNN is sequential, CNN has local information.<br></li><li>Transformer has no recurrence and no convolution, both MHSA and FF layer\ncompute each item of sequence independently.<br></li></ul>"
            ],
            "guid": "bMGtI->2C&",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transformer: Postional Encoding (PE)",
                "<ul><li>PE is added at the beginning of the encoder and decoder, to represent the order\nof the input sequence.<br></li><li>The sine and cosine functions of different frequencies are used to calculate the\nrelative position:<br></li><li>\\(PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})\\)<br></li><li>\\(PE_{(pos,2i+1)} = cos(pos/10000^{2i/d_{model}})\\)<br></li><li>where&nbsp;\\(pos \\in [0, len(X)), \\; i \\in [0, d/2), \\; d\\) is embedding dimension</li></ul>"
            ],
            "guid": "y!JP`kTk}+",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Main Workflow of ViT (Vision Transformer)",
                "<ul><li>Image Patch and Position Embedding</li><li>Encoding and classification</li></ul>"
            ],
            "guid": "yK;)f>:-m=",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViT: Image Patch and Position Embedding",
                "<ul><li>The input image is splited as fixed-size patches (\\(P \\times P \\times C\\)) and flattened indicated by&nbsp;\\(x_p\\)<br></li><li>Embedding for each patch is realized through a \\(P^2 \\cdot C \\rightarrow D\\) linear projection indicated by&nbsp;\\(E\\)<br></li><li>Position embedding \\(E_pos\\), achieved by the location of the patch in image, is added to the patch\nembedding</li></ul><img src=\"patchemb.png\"><br>"
            ],
            "guid": "Br[Q+j6QUy",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViT: Encoding and classification",
                "<ul><li>The resulting sequence&nbsp;\\(z_0\\) is then fed into a standard Transformer encoder indicated by MSA<br></li><li>An extra learnable “classification token” is added to the sequence in order to perform classification\nwith MLP<br></li><li>The MLP contains two layers with a GELU to add non-linearity<br></li><li>LN is layer normalization<br></li></ul><img src=\"enccas.png\"><br>"
            ],
            "guid": "r)I6X_Yd&3",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViT: Inductive Bias",
                "<ul><li>Vision Transformer has less image-specific inductive bias than CNNs</li><li>In CNNs, locality, two-dimensional neighborhood structure, and translation equivariance are\nbaked into each layer throughout the whole model.<br></li><li>In ViT, only MLP layers are local and translationally equivariant, while the self-attention layers are\nglobal<br></li><li>In ViT, the two-dimensional neighborhood structure is used very sparingly<br></li></ul>"
            ],
            "guid": "r7JvPE/_-w",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViT: Hybrid Architecture",
                "<ul><li>As an alternative to raw image patches, the input sequence can be formed from feature maps of a\nCNN<br></li><li>As a special case, the patches can have spatial size 1x1, which means that the input sequence is\nobtained by simply flattening the spatial dimensions of the feature map and projecting to the\nTransformer dimension<br></li></ul>"
            ],
            "guid": "lS-76goXGb",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Main Highlights of DeiT: Data efficient image Transformer",
                "<ul><li>It is a competitive convolution-free transformer by training on Imagenet only. Its reference vision\ntransformer achieves state-of-the-art on ImageNet without external data.<br></li><li>A teacher-student strategy specific to transformers is introduced, which relies on a distillation token\nensuring that the student learns from the teacher through attention.<br></li></ul>"
            ],
            "guid": "z<h.9ldPna",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeiT: Main Vision Transformer Structure Overview",
                "<ul><li>Multi-head Self-Attention Layers (MHSA)</li><li>Transformer Block for Images</li><li>The classification token: trainable vector, appended to the patch tokens before the first layer which goes through the transformer layers and is then projected with a linear layer to predict the class</li><li>Fixing the positional encoding across resolutions</li></ul>"
            ],
            "guid": "s)-m2Oz;p2",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeiT: Transformer Block for Images",
                "<ul><li>Built based on ViT</li><li>A FFN is added on top of the MHSA layer. The FFN is composed of two linear layers separated by GeLU activation.</li><li>Both MSA and FFN are operating as residual operators thanks to skip-connections, and with a layer normalization</li><li>The fixed-size input RGB image is decomposed into a batch of&nbsp;\\(N\\) patches of a fixed size of&nbsp;\\(16 \\times 16\\) pixels (\\(N = 14 \\times 14\\))</li><li>Each patch is projected with a linear layer that conserves its overall dimension&nbsp;\\(3 \\times 16 \\times 16 = 768\\)</li><li>The transformer block is invariant to the order of the patch embeddings, and thus does not\nconsider their relative position<br></li></ul>"
            ],
            "guid": "B#-On-3-Fn",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeiT: Fixing the positional encoding across resolutions",
                "<ul><li>It is desirable to use a lower training resolution and fine-tune the network at the larger resolution\naccording to Touvron et al.<br></li><li>This speeds up the full training<br></li><li>The accuracy is improved under prevailing data augmentation schemes.<br></li><li>Interpolate the positional encoding when changing the resolution (works with the fine-tuning stage)<br></li></ul>"
            ],
            "guid": "wI<.BR`w_U",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeiT: Soft distillation",
                "<ul><li>Soft distillation minimizes the Kullback-Leibler divergence between the softmax of the teacher and\nthe softmax of the student model.<br></li><li>Let&nbsp;\\(Z_t\\) be the logits of the teacher model,&nbsp;\\(Z_s\\) the logits of the student model.&nbsp;\\(\\tau\\) denotes the temperature for the distillation,&nbsp;\\(\\lambda\\) is the coefficient balancing the Kullback-Leibler divergence loss (KL) and the cross-entropy is&nbsp;\\(\\mathcal{L}_{CE}\\) on ground truth labels&nbsp;\\(y\\), and&nbsp;\\(\\psi\\) is the softmax function:&nbsp;\\[\\mathcal{L}_{global} = (1 - \\lambda)\\mathcal{L}_{CE}(\\psi(Z_s), y) + \\lambda \\tau^2 KL(\\psi(Z_s/\\tau), \\psi(Z_t/\\tau))\\]</li></ul>"
            ],
            "guid": "N#Z(&ox=Td",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeiT: Hard-label distillation",
                "<ul><li>The hard decision of the teacher is took as a true label</li><li>\\(y_t = argmax_cZ_t(c)\\) denotes hard decision of teacher<br></li><li>The objective function becomes&nbsp;\\[\\mathcal{L}^{hardDistill}_{global} = \\frac{1}{2}\\mathcal{L}_{CE}(\\psi(Z_s), y) + \\frac{1}{2}\\mathcal{L}_{CE}(\\psi(Z_s), y_t)\\]</li></ul>"
            ],
            "guid": "kZH`aK+34.",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeiT: Distillation Token",
                "<ul><li>A new distillation token interacts with\nthe class and patch tokens through the\nself-attention layers</li><li>This distillation token is employed in a\nsimilar fashion as the class token, except\nthat on output of the network its objective\nis to reproduce the (hard) label predicted\nby the teacher, instead of true label<br></li><li>Both the class and distillation tokens input\nto the transformers are learned by backpropagation<br></li></ul><img src=\"distill.png\"><br>"
            ],
            "guid": "f<Gl.8&#ZR",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Examples of applications with attention and transformer models",
                "<ul><li>ACNet: Attention based network to exploit complementary features for RGBD semantic\nsegmentation<br></li><li>Capturing omni-range context for omnidirectional segmentation<br></li><li>Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired\npeople navigate in the real world.<br></li></ul>"
            ],
            "guid": "n}nHW`J/#5",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DETR: Object Detection with Transformers: Overview",
                "<ul><li>Solve object detection via set prediction. The model predicts an unordered set of all the\nobjects present.<br></li><li>End-to-End pipeline, without Non-Maximum Suppression (NMS) procedure, without anchor\ngeneration.<br></li><li>CNN backbone (ResNet50) to learn image features, then input to an encoder-decoder\ntransformer<br></li><li>Fixed number of learnable object queries as additional input of decoder and attends to the\nencoder output.</li></ul><img src=\"detroverview.png\"><br>"
            ],
            "guid": "n-uD>%?}-8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Modified transformer of DETR",
                "<ul><li>Only <b>K, Q</b>&nbsp;have positional encoding, since <b>V</b>&nbsp;is irrelated to attention map calculation</li><li>Remove the masked MHSA in decoder,\ndue to the parallel processing.<br></li><li>Object queries for set prediction.<br></li></ul><img src=\"detrtrans.png\"><br>"
            ],
            "guid": "xlpM.s#lV/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DETR Object queries",
                "<ul><li>The&nbsp;\\(N=100\\) object queries are transformed into an output embedding by the decoder</li><li>They are then independently decoded into box coordinates and class labels by specific FFN, resulting in \\(N\\) final predictions</li><li>\\(N\\) objects are decoded in parallel at each decoder layer, which is different to the original autoregressive transformer model (no sequence mask needed)<br></li></ul><img src=\"detrobjectqueries.png\"><br>"
            ],
            "guid": "&3mC6?nEL",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DETR prediciton heads FFN",
                "<ul><li>Different to the general FFN inside encoder and decoder</li><li>Box prediction FFN build by a 3-layer perceptron with ReLU activation function and hidden dimension is used to predict center coordinates, height and width of the bounding box</li><li>Class prediction FFN build with linear projection layer is used to predict class labels</li></ul><img src=\"detrpred.png\"><br>"
            ],
            "guid": "b9`nIHE<6f",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DETR Optimal bipartite matching",
                "<ul><li>Training issue: predictions are unordered to the ground truth</li><li>Given fixed-size set of \\(N\\) predictions (unordered), find optimal match between predicted\nobjects (class, position, size) and the ground truth.<br></li><li>If the predicted class have larger probability to label class, or the box gap between the two is\nsmaller, the match cost is smaller, and vice versa. Find the class with smallest match cost.</li></ul><img src=\"detropt.png\"><br>"
            ],
            "guid": "x:%($N>nZ4",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Shortcomings of DETR",
                "<ul><li>Slow convergence. Because the attention weights assigned to each pixel are almost\nuniform, long training process is necessary for the attention weights to be learned to focus\non sparse meaningful locations.<br></li><li>Lower performance at detecting small objects. Because the attention weights computation\nis quadratic to pixel numbers, it is hard to process high-resolution feature maps.<br></li></ul>"
            ],
            "guid": "2+un*#$Z5",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Deformable DETR: Idea",
                "<ul><li>Slow convergence → Deformable Attention Module. It only attends to a small set of key\nsampling points (tokens) around a reference point, regardless of the size of the feature\nmaps.<br></li><li>Poor detection of small objects → Multiscale Deformable Attention Module. It looks over\nmultiple sampling points from multi-scale inputs, instead of only one sampling point.</li></ul><img src=\"deformabledetr.png\"><br>"
            ],
            "guid": "vqw/bR>C>K",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Deformable DETR: Deformable Attention Module (DAM)",
                "<ul><li>MHSA as:&nbsp;\\(MHSA(z_q, x) = \\sum_{m=1}^M W_m[\\sum_{k \\in \\Omega} A_{mqk} \\cdot W'_m x_k&nbsp;]\\)</li><li>DAM as:&nbsp;\\(DAM(z_q, p_q, x) = \\sum_{m=1}^M W_m[\\sum_{k = 1}^K A_{mqk} \\cdot W'_m x(p_q + \\Delta p_{mqk})&nbsp;]\\)</li><li>The sampling offset&nbsp;\\(\\Delta p_{mqk}\\), according to the reference point&nbsp;\\(p_q\\), is obtained via linear projection over the query feature&nbsp;\\(z_q\\)</li></ul><img src=\"deformableatt.png\"><br>"
            ],
            "guid": "uK<.MZ.%fT",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Deformable DETR: Characteristics",
                "<ul><li>Cross attention decoder: There are self-attention and cross-attention modules in the decoder</li><li>Bounding Box refinement: Each decoder layer refined the bounding boxes based on the predictions from the previous layer</li><li>Two stage detector:&nbsp;The generated regional proposals are fed into decoder as object queries.</li></ul>"
            ],
            "guid": "PR#SERVDmj",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Semantic Segmenation Transformers",
                "<ul><li>Dual Attention Network</li><li>SETR (Segmentation Transformer)</li><li>SegFormer</li><li>MaskFormer (Semantic + Panoptic Segmentation)</li></ul>"
            ],
            "guid": "zlLXqJv9N@",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dual Attention Network: Structure",
                "<ul><li>Dual Attention Network (DANet) with self-attention mechanism to enhance the\ndiscriminant ability of feature representations<br></li><li>Backbone: a dilated residual network<br></li><li>A <b>position attention module</b> to learn spatial interdependencies<br></li><li><b>A channel attention module</b> to model channel interdependencies</li></ul><img src=\"dualatt.png\"><br>"
            ],
            "guid": "j,#.tK=FuD",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dual Attention Network: Position attention module",
                "<ul><li>Local feature \\(A\\), convolutional layers to generate two feature maps \\(B, C\\)</li><li>Reshape \\(B, C\\) from&nbsp;\\(\\{B, C\\} \\in \\mathbb{R}^{C \\times H \\times W}\\) to&nbsp;\\(\\mathbb{R}^{C \\times N}\\) where&nbsp;\\(N=H \\times W\\)</li><li>Matrix multiplication between the transpose of \\(C\\) and \\(B\\), apply a softmax:&nbsp;\\(s_{ji} = \\frac{exp(B_i \\cdot C_j)}{\\sum_{i=1}^N exp(B_i \\cdot C_j)}\\)</li><li>Obtain the spatial attention map&nbsp;\\(S \\in \\mathbb{R}^{N \\times N}\\)</li></ul><img src=\"posatt.png\"><br>"
            ],
            "guid": "uB`HLd^7tA",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dual Attention Network: Position attention module: Output E",
                "<ul><li>Multiply the reshaped tensor by a scale parameter&nbsp;\\(\\alpha\\)<br></li><li>Element-wise sum operation with \\(A\\) to obtain output&nbsp;\\(E \\in \\mathbb{R}^{C \\times H \\times W}\\)<br></li><li>\\(\\alpha\\) is initialized as 0 and gradually learns to assign more weight&nbsp;\\[E_j =&nbsp; \\alpha \\sum_{i=1}^N (s_{ji}D_i) + A_j\\]</li><li>\\(E\\) at each position is a weighted sum of the features across all positions and original features<br></li><li>Obtains a global contextual view</li><li>Selectively aggregates contexts according to the spatial attention map<br></li></ul>"
            ],
            "guid": "q%Oir$p~QK",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dual Attention Network: Channel attention module",
                "<ul><li>Directly calculate the channel attention map&nbsp;\\(X \\in \\mathbb{R}^{C \\times C}\\) from original&nbsp;\\(A\\)</li><li>Reshape&nbsp;\\(A \\in \\mathbb{R}^{C \\times H \\times W}\\) to&nbsp;\\(\\mathbb{R}^{C \\times N}\\), matrix multiplication between \\(A\\) and transpose of \\(A\\). Apply a softmax to get channel attention map&nbsp;\\(X \\in \\mathbb{R}^{C \\times C}\\)</li><li>\\(x_{ji}\\) measures the ith channel impact on the jth channel&nbsp;\\[x_{ji} = \\frac{exp(A_i \\cdot A_j)}{\\sum_{i=1}^C exp(A_i \\cdot A_j)}\\]<br></li></ul><img src=\"channelatt.png\"><br>"
            ],
            "guid": "J=[LQcn_O<",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Dual Attention Network: Chanel attention module: Output E",
                "<ul><li>A matrix multiplication between the transpose of X and A, reshape the their\nresult to&nbsp;\\(\\mathbb{R}^{C \\times H \\times W}\\)<br></li><li>Multiply it by a scale parameter&nbsp;\\(\\beta\\) (gradually learns a weight from 0)</li><li>Element-wise sum with&nbsp;\\(A\\), output&nbsp;\\(E \\in \\mathbb{R}^{C \\times H \\times W}\\)&nbsp;\\[E_j = \\beta \\sum_{i=1}^C (x_{ji}A_i) + A_j\\]</li><li>Final feature of each channel is a weighted sum of the features of all channels\nand original features<br></li><li>Models the long-range semantic dependencies between feature maps<br></li><li>Helps to boost feature discriminability<br></li></ul>"
            ],
            "guid": "P*wL8hOlq:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Segmentation Transformer: Motivation",
                "<ul><li>Due to the locality nature of convolution operation, FCN-based architectures having limited\nreceptive fields for context modeling<br></li><li>Dependency learning is lacking in FCN-based semantic segmentation architectures, leading\nto sub-optimal representation learning<br></li></ul>"
            ],
            "guid": "wZjQW!dq}J",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Overview",
                "<ul><li>Split an image into fixed-size patches, linearly embed each of them, add\nposition embeddings<br></li><li>Feed the resulting sequence of vectors to a standard Transformer encoder (e.g.,\nViT or DeiT)<br></li><li>Three different decoder designs (Naive upsampling, progressive upsampling(PUP), Multi-Level feature Aggregation(MLA))</li></ul><img src=\"setr.png\"><br>"
            ],
            "guid": "Rew.=@1w!T",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Image to sequence",
                "<ul><li>Divide the input image&nbsp;\\(x \\in \\mathbb{R}^{H \\times W \\times 3}\\) into a grid of&nbsp;\\(\\frac{H}{16} \\times \\frac{W}{16}\\) patches</li><li>Map each vectorized patch \\(p\\) into a latent \\(C\\)-dimensional embedding space\nusing a linear projection function&nbsp;\\(f: p \\rightarrow e \\in \\mathbb{R}^C\\)<br></li><li>Obtain a 1D sequence of patch embeddings for an image&nbsp;\\(x\\)</li><li>To keep spatial information: learn a specific embedding&nbsp;\\(p_i\\) for every location \\(i\\)\nwhich is added to \\(e_i\\)\nto form the final sequence input&nbsp;\\[E = \\{ e_1 + p_1, e_2 + p_2, \\dots , e_L + p_L \\}\\]<br></li></ul>"
            ],
            "guid": "P9kUsKy9zZ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Encoder",
                "<ul><li>ViT or DeiT</li><li>Accepts a 1D sequence of feature embeddings&nbsp;\\(Z \\in \\mathbb{R}^{L \\times C}\\)<br></li><li>\\(L\\): the length of sequence (set to \\(\\frac{H}{16} \\times \\frac{W}{16} = \\frac{HW}{256}\\))<br></li><li>\\(C\\): the hidden channel size<br></li><li>Consists of \\(L_e\\)\nlayers of multi-head self-attention (MHSA) and Multilayer\nPerceptron (MLP) blocks<br></li><li>The output of each layer&nbsp;\\(Z^l\\)&nbsp;\\[Z^l = MHSA(Z^{l-1}) + MLP(MHSA(Z^{l-1})) \\in&nbsp; \\mathbb{R}^{L \\times C}\\]</li><li>Features of transformer layers&nbsp;\\(\\{ Z^1, Z^2, \\dots , Z^{L_e} \\}\\)</li></ul>"
            ],
            "guid": "w]C~7v[win",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Decoder designs",
                "<ul><li>Goal of decoder: generate the segmentation results in the original 2D image space (\\(H \\times W\\))</li><li>To reshape the encoder's features Z from a 2D shape of&nbsp;\\(\\frac{HW}{256} \\times C\\) to a standard 3D feature map&nbsp;\\(\\frac{H}{16} \\times \\frac{W}{16} \\times C\\)</li><li>3 Decoders possible: Naive Upsampling, Progressive upsampling (PUP), Multi-level feature aggregation (MLA)</li></ul>"
            ],
            "guid": "e^4KQ@TS|J",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Naive upsampling",
                "<ul><li>first projects the transformer featur&nbsp;\\(Z^{L_e}\\) to the dimension\nof category number (e.g., 19 for Cityscapes)<br></li><li>A simple 2-layer network with architecture:&nbsp;1×1 conv + sync batch norm (w/ ReLU) + 1×1 conv<br></li><li>Simply bilinearly upsample the output to the full image resolution<br></li><li>Followed by a classification layer with pixel-wise cross-entropy loss<br></li></ul>"
            ],
            "guid": "gD[Qf6,TQL",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Progressive Upsamling (PUP)",
                "<ul><li>Alternates conv layers and upsampling operations<br></li><li>Restricts upsampling to 2&nbsp;\\(\\times\\)<br></li><li>A total of 4 operations for reaching the full resolution<br></li></ul><img src=\"pup.png\"><br>"
            ],
            "guid": "hF:YA`(t3t",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SETR: Multi-Level feature Aggregation (MLA)",
                "<ul><li>Take as input the feature representations&nbsp;\\( \\{ Z^m\\} \\; (m \\in \\{ \\frac{L_e}{M}, 2\\frac{L_e}{M}, \\dots , M\\frac{L_e}{M} \\})\\) from&nbsp;\\(M\\) layers uniformly distributed across the layers with step&nbsp;\\(\\frac{L_e}{M}\\)</li><li>\\(M\\)&nbsp;streams are deployed, with each focusing on specific layer<br></li><li>In each stream, first reshape the encoder’s feature&nbsp;\\(Z^l\\) from a 2D shape of&nbsp;\\(\\frac{HW}{256} \\times C\\) to a 3D feature map&nbsp;\\(\\frac{H}{16} \\times \\frac{W}{16} \\times C\\)<br></li><li>A 3-layer (kernel size 1×1, 3×3, and 3×3) network is applied with the feature\nchannels halved at the first and third layers respectively<br></li><li>Element-wise addition after the first layer to enhance interactions. An additional\n3×3 conv after the element-wise additioned feature<br></li><li>After the third layer, obtain the fused feature via channel-wise concatenation,\nwhich is bilinearly upsampled 4× to the full resolution<br></li></ul><img src=\"mla.png\"><br>"
            ],
            "guid": "Dw*_1x+~JM",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Main Contributions",
                "<ul><li>A novel positional-encoding-free and hierarchical Transformer encoder<br></li><li>A lightweight All-MLP decoder design that yields a powerful representation\nwithout complex and computationally demanding modules<br></li><li>Setting new a state-of-the-art in terms of efficiency, accuracy, and robustness<br></li></ul>"
            ],
            "guid": "Fq(&E6Z2dn",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Overview",
                "<ul><li>A hierarchical Transformer encoder to generate high-resolution coarse features\nand low-resolution fine features<br></li><li>A lightweight All-MLP decoder to fuse these multi-level features<br></li><li>Input image HxWx3, divide it into patches of 4x4 (Contrary to ViT that uses\n16x16 patches, using a smaller number of patches favors dense prediction)<br></li><li>Hierarchical Transformer encoder obtains multi-scale features at\n{1/4,1/8,1/16,1/32} of the original image resolution<br></li><li>All-MLP decoder predicts the segmentation mask at H/4 x W/4 x N_cls<br></li></ul><img src=\"segformer.png\"><br>"
            ],
            "guid": "t>2=23_mRQ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Hierarchical Transformer Encoder",
                "<ul><li>Hierarchical Feature Representation</li><li>Overlapped Patch Merging</li><li>Efficient Self-Attention</li><li>Mix-FFN</li></ul><img src=\"segformerenc.png\"><br>"
            ],
            "guid": "o@.j#i9v}u",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Hierarchical Transformer Encoder:&nbsp;Hierarchical Feature Representation",
                "To generate multi-level features,\nusually boost the performance of semantic segmentation"
            ],
            "guid": "KXN6`^FDZc",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Hierarchical Transformer Encoder:&nbsp;Overlapped Patch Merging",
                "Shrink features F1 (H/4 x W/4 x C1) to F2 (H/8 x\nW/8 x C2), then iterate for any other feature map in the hierarchy, with stride\nbetween adjacent patches, to preserve the local continuity"
            ],
            "guid": "Aad/M!:Vu7",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Hierarchical Transformer Encoder:&nbsp;Efficient Self-Attention",
                "<ul><li>N=HxW, the computational complexity of self-attention\nis \\(\\mathcal{O}(N^2\n)\\), prohibitive for large image resolutions<br></li><li>Used a sequence reduction process with a reduction ratio \\(R\\)&nbsp;to reduce the length\nof the sequence, \\(K\\) is the sequence to be reduced with a \\(C\\)-dimensional tensor<br></li><li>Reduced the complexity to \\(\\mathcal{O}(\\frac{N^2}{R})\\)<br></li></ul>"
            ],
            "guid": "H2cX~=|gF;",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Hierarchical Transformer Encoder: Mix-FFN",
                "<ul><li>ViT uses positional encoding (PE) to introduce the location\ninformation. The resolution of PE is fixed. When the test resolution is different\nfrom the training one, the positional code needs to be interpolated and this\noften leads to dropped accuracy<br></li><li>Argue positional encoding is not necessary for semantic segmentation<br></li><li>Introduce Mix-FFN, considers the effect of zero padding to leak location\ninformation, directly using a 3x3 Conv in the feed-forward network (FFN)<br></li></ul>\\[x_{out} = MLP(GELU(Conv_{3 \\times 3}(MLP(x_{in})))) + x_{in}\\]<br>"
            ],
            "guid": "nhGOC=${$m",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Lightweight All-MLP Decoder",
                "<ol><li>multi-level features go through an MLP to unify channel dimension<br></li><li>features are up-sampled and concatenated together<br></li><li>an MLP layer is adopted to fuse the concatenated features<br></li><li>another MLP layer takes the fused feature to predict the segmentation mask<br></li></ol><img src=\"seformerdecoder.png\"><br>"
            ],
            "guid": "p=tO,C,SE^",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SegFormer: Effective Receptive Field (ERF) Analysis",
                "<ol><li>ERF of DeepLabv3+ is relatively small even at Stage-4, the deepest stage<br></li><li>SegFormer’s encoder naturally produces local attentions which resemble\nconvolutions at lower stages, while able to output highly non-local\nattentions that effectively capture contexts at Stage-4<br></li><li>The ERF of the MLP head differs from Stage-4 with a significant stronger\nlocal attentions besides the non-local attention<br></li></ol><img src=\"erf.png\"><br>"
            ],
            "guid": "A;:F3&cx3c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Panoptic Segmentation",
                "<ul><li>Unifies semantic segmentation and instance\nsegmentation, encompasses both stuff and thing classes<br></li><li>For each pixel, predicts both class label and instance ID<br></li></ul><img src=\"panoptic.png\"><br>"
            ],
            "guid": "9^zbA16s9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Per-Pixel classification vs. Mask Classification",
                "<ul><li>Key observation: mask classification is sufficiently general to solve both solve\nboth semantic- and instance-level segmentation tasks<br></li><li>Left: Semantic segmentation with per-pixel classification applies the same\nclassification loss to each location<br></li><li>Right: Mask classification predicts a set of binary masks and assigns a single\nclass to each mask<br></li></ul><img src=\"paste-16d0a29059b641389aaad4f9a1405a8a0832029f.jpg\"><br>"
            ],
            "guid": "lRSW.RS<2/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Semantic/Panoptic Segmentation: MaskFormer: Overview",
                "<ul><li>Computes \\(N\\) probability-mask pairs&nbsp;\\(z = \\{ (p_i, m_i) \\}^N_{i=1}\\)<br></li><li>Pixel-level module</li><li>Transformer module</li><li>Segmentation module</li></ul><img src=\"paste-82a1d99f2afa4ab8c197386133ec13cee911bb21.jpg\"><br>"
            ],
            "guid": "v;n,,hC[fl",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MaskFormer: Pixel-level module",
                "<ul><li>A backbone generates a low-resolution image feature\nmap&nbsp;\\(\\mathcal{F} \\in \\mathbb{R}^{C_{\\mathcal{F}} \\times \\frac{H}{S} \\times \\frac{W}{S}}\\), and a pixel decoder gradually upsamples the features to generate per-pixel embeddings&nbsp;\\(\\varepsilon_{pixel} \\in \\mathbb{R}^{C_{\\varepsilon} \\times H \\times W}\\)<br></li><li>Any per-pixel classification-based segmentation model fits the module</li></ul><img src=\"paste-04e9b90197a3b6243f99bcf83969598d7f31cd6d.jpg\"><br>"
            ],
            "guid": "CKYxbbUs23",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MaskFormer: Transformer module",
                "<ul><li>uses the standard Transformer decoder to compute from\nimage features&nbsp;\\(\\mathcal{F}\\) and \\(N\\) learnable positional embeddings (i.e., queries)<br></li><li>Its output:&nbsp;\\(N\\) per-segment embeddings&nbsp;\\(\\mathcal{Q} \\in&nbsp; \\mathbb{R}^{C_{\\mathcal{Q}} \\times N}\\) of dimensions&nbsp;\\(C_{\\mathcal{Q}}\\):&nbsp;Encode global information about each segment MaskFormer predicts. Similar to DETR, the decoder yields all predictions in parallel</li></ul><img src=\"paste-6d815c27c9ca3c2266a23b6bd2e0d09e65398f9e.jpg\"><br>"
            ],
            "guid": "dxYp?j4cFP",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MaskFormer: Segmentation Module",
                "<ul><li>a linear classifier, followed by a softmax activation on\ntop of the per-segment embeddings&nbsp;\\(\\mathcal{Q}\\), to yield class probability&nbsp;\\(\\{ p_i \\in \\Delta^{K+1} \\}_{i=1}^N\\) for each segment (\\(K\\) categories)<br></li><li>The classifier predicts an additional \"no object\" category</li><li>A MLP with 2 hidden layers converts the per-segment embeddings \\(\\mathcal{Q}\\) to&nbsp;\\(N\\) mask embeddings&nbsp;\\(\\varepsilon \\in \\mathbb{R}^{C_{\\varepsilon} \\times N}\\) of dimension&nbsp;\\(C_{\\varepsilon}\\)</li><li>Obtain each binary mask prediction&nbsp;\\(m_i \\in [0,1]^{H \\times W}\\) via a dot product between the\nith mask embedding and per-pixel embeddings&nbsp;\\(\\varepsilon_{pixel}\\)<br></li><li>The dot product is followed by a sigmoid activation<br></li><li>Classification loss: cross entropy classification loss; Binary mask loss for each\npredicted segment (same as DETR, a linear combination of focal loss and a\ndice loss)<br></li></ul><img src=\"paste-2c9f76f2d79264c0632bd02c6a2aeb347265e221.jpg\"><br>"
            ],
            "guid": "L&kTE(bHd@",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MaskFormer: Inference",
                "<ul><li>Converts mask classification outputs&nbsp;\\(\\{ (p_i, m_i) \\}^N_{i=1}\\) to either panoptic or semantic\nsegmentation outputs<br></li><li>General inference: partitions an image into segments by assigning each pixel\n[h, w] to one of the N predicted probability-mask pairs via:&nbsp;\\(argmax_{i:c_i \\ne \\varnothing} p_i(c_i) \\cdot m_i [h, w]\\).&nbsp;\\(c_i\\) is the most likely class label&nbsp;\\(c_i = argmax_{c \\in \\{ 1, \\dots , K, \\varnothing \\}}p_i(c)\\)&nbsp;for each probability-mask pair i.<br></li><li>Semantic inference: marginalization over probability-mask\npairs&nbsp;\\( argmax_{c \\in \\{ 1, \\dots , K \\}} \\sum_{i=1}^N p_i(c) \\cdot m_i[h,w]\\),&nbsp;yields better results than the hard assignment\nof each pixel to a probability-mask pair \\(i\\) used in the general inference strategy</li></ul><img src=\"paste-7a9e0a452701836d59db815749fc74d4f7d76e37.jpg\"><br>"
            ],
            "guid": "HW5Ym=3cC+",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MaskFormer: Baselines",
                "<ul><li>PerPixelBaseline: uses the pixel-level module and directly outputs per-pixel\nclass scores<br></li><li>PerPixelBaseline+: adds the transformer module and mask embedding MLP to\nthe PerPixelBaseline<br></li><li>PerPixelBaseline+ and MaskFormer differ only in the formulation:\nper-pixel vs. mask classification<br></li></ul><img src=\"paste-62915ea3fe48543e013be2f5f3e93054f998683e.jpg\"><br>"
            ],
            "guid": "P.j+~PkiEx",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "General Transfomers for Visual Recognition",
                "<ul><li>SWIN: Hierarchical Vision Transformer using Shifted Windows</li><li>PVT: Pyramid Vision Transformer</li></ul>"
            ],
            "guid": "m,6E$$~%kH",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Motivation",
                "<ul><li>Performance evaluated in variety of tasks (Image classification, object\ndetection, and semantic segmentation).<br></li><li>Representation is computed through Shifted WINdow</li><li>Hierarchical structure is leveraged.<br></li></ul><img src=\"paste-3be48d03cab58848d21541265b2221fd2d0c6d6f.jpg\"><br>"
            ],
            "guid": "s/cX{wL03A",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer:&nbsp;Shifted Window Approach for Self-attention Computation",
                "<ul><li>In layer l (left), a regular window partitioning scheme is adopted, and self-attention is\ncomputed within each window.<br></li><li>In the next layer l + 1 (right), the window partitioning is shifted, resulting in new windows.<br></li><li>The self-attention computation in the new windows crosses the boundaries of the previous\nwindows in layer l, providing connections among them.<br></li></ul><img src=\"paste-98480122c2ee4d91fdc2a57e4e4e7ff5036df82e.jpg\"><br>"
            ],
            "guid": "qZGx[}Yp-O",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Benefits of Hierarchical Structure",
                "<ul><li>Previous ViT based approaches produce feature maps of a single low resolution and have\nquadratic computation complexity to input image size due to computation of self-attention\nglobally<br></li><li>Swin Transformer builds hierarchical feature maps by merging image patches in deeper\nlayers and has linear computation complexity to input image size due to computation of selfattention only within each local window.<br></li><li>Swin Transformer can thus serve as a general-purpose backbone for both image\nclassification and dense recognition tasks.<br></li></ul>"
            ],
            "guid": "hI$%GHPv7`",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Benefits for Shifted Window",
                "<ul><li>The shifted windows bridge the windows of the preceding layer, providing connections\namong them that significantly enhance modeling power.<br></li><li>Shifted window approach has much lower latency than the sliding window approach.<br></li></ul>"
            ],
            "guid": "p-ugz?V7{v",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Architecture Overview",
                "<ul><li>It first splits an input RGB image into non-overlapping patches by a patch splitting module,\nlike ViT. Each patch is treated as a “token” and its feature is set as a concatenation of the\nraw pixel RGB values.<br></li><li>Several Transformer blocks with modified self-attention computation (Swin Transformer\nblocks) are applied on these patch tokens.<br></li><li>To produce a hierarchical representation, the number of tokens is reduced by patch merging\nlayers (neighbour patches concatenation) as the network gets deeper.<br></li></ul><img src=\"paste-226bae653b4c5af40bf45db4df58d76553f6d88c.jpg\"><br>"
            ],
            "guid": "mW/e5NW@mk",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Swin Transformer Blocks",
                "<ul><li>Two successive Swin Transformer Blocks, W-MSA and\nSW-MSA, which are multi-head self attention modules\nwith regular and shifted windowing configurations,\nrespectively.<br></li><li>Swin Transformer is built by replacing the standard multihead self attention (MSA) module in a Transformer block\nby a module based on shifted windows<br></li><li>A Swin Transformer block consists of a shifted window\nbased MSA module, followed by a 2-layer MLP with\nGELU nonlinearity in between.<br></li><li>A LayerNorm (LN) layer is applied before each MSA\nmodule and each MLP, and a residual connection is\napplied after each module<br></li></ul><img src=\"paste-e8a9ef21f999b619a39ca67965f152d92f8df1d8.jpg\"><br>"
            ],
            "guid": "M#seuM1Z%o",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Shifted Window based Self-Attention",
                "<ul><li>Self-attention in non-overlapped windows for efficient modeling<br></li><li>Supposing each window contains M ×M patches, the computational complexity of a\nglobal MSA module and a window based one on an image of h ×w patches are,&nbsp;\\[\\Omega(MSA) = 4hwC^2 + 2(hw)^2C\\] \\[\\Omega(W-MSA) = 4hwC^2 + 2M^2hwC\\]</li><li>where the former is quadratic to patch number \\(hw\\), and the latter is linear when \\(M\\) is\nfixed (set to 7 by default).<br></li><li>Global self-attention computation is generally unaffordable for a large \\(hw\\), while the\nwindow based self-attention is scalable.</li></ul>"
            ],
            "guid": "Mvv&>;PKg8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Swin Transformer: Shifted window partitioning in succesive blocks",
                "<ul><li>The window-based self-attention module lacks connections across windows, which limits its\nmodeling power.<br></li><li>To introduce cross-window connections while maintaining the efficient computation of nonoverlapping windows<br></li><li>It alternates between two partitioning configurations in consecutive SwinTransformer blocks.</li><li>\\(\\hat{z}_l\\) and&nbsp;\\(z_l\\) denote the output features of the (S)WMSA module and the MLP module for block&nbsp;\\(l\\), respectively<br></li></ul><img src=\"paste-d99d54eb82d29a9655ca0bdeaf7559809e9bce4b.jpg\"><br><ul><li>The shifted window partitioning approach introduces connections between neighboring nonoverlapping windows in the previous layer and is found to be effective in image\nclassification, object detection, and semantic segmentation.<br></li></ul>"
            ],
            "guid": "MnS|c<+zz)",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pyramid Vision Transformer (PVT): Motivation",
                "<b>Issues to use ViT for dense prediction</b><br><ul><li>ViT yields low resolution outputs and incurs high computational and memory costs;<br></li><li>Only single scale feature map (16- or 32-stride) is output.<br></li></ul><b>Recall: pyramid features in CNN</b><br><ul><li>The resolution of the feature map gradually decreases as the network deepens.<br></li><li>The channel number of the feature map gradually increases as the network deepens.<br></li></ul>"
            ],
            "guid": "gl250LZG^G",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "PVT: Comparison of different features",
                "<ul><li>CNN use a pyramid structure for dense prediction tasks (e.g. object detection,\nsegmentation).<br></li><li>ViT is a “columnar” structure specifically designed for image classification.<br></li><li>PVT combine the pyramid feature and transformer, as a versatile backbone for different\nvision tasks.<br></li></ul><img src=\"paste-da86cc7b3b210282e76f620ed79aebc677ab8428.jpg\"><br>"
            ],
            "guid": "Oj=a=0]oiJ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "PVT: Pyramid features in PVT",
                "<ul><li>A progressive shrinking pyramid to reduce the sequence length of Transformer as the network\ndeepens, significantly reducing the computational cost<br></li><li>There are four stages, each of which has a patch embedding layer and a L-layer transformer\nencoder<br></li></ul><img src=\"paste-8b3006394a255cc0a7f15aaa5ba1e8e33e80d993.jpg\"><br>"
            ],
            "guid": "d8@Wig_7N_",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "PVT: Spatial-reduction attention (SRA)",
                "<ul><li>To process high-resolution (e.g., 4-stride) feature maps is computationally costly, then use a SRA\nlayer to replace the traditional MHSA layer in the encoder.<br></li><li>Reshape the input sequence to \\(R\\) times smaller size,<br></li><li>After the attention, use a linear projection to recover the dimension of the sequence to \\(C\\)<br></li></ul><img src=\"paste-bb739c684fb5b20104b3db1520668279bc0504f5.jpg\"><br>"
            ],
            "guid": "sOV2l&=*lS",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "PVT: Detail of each stage",
                "<ul><li>At the beginning of stage (by patch embedding layer), downsampling feature map into smaller scale&nbsp;\\(\\left(&nbsp; \\frac{H_{i-1}}{P_i}, \\frac{W_{i-1}}{P_i} \\right) \\)&nbsp;and linearly project to larger number of channel&nbsp;\\(C_i = P_i^2 C_{i-1}\\)<br></li><li>Reshape spatial features into sequential ones, then add the positional encoding<br></li><li>After&nbsp;\\(L_i \\times \\) transformer encoder, reshape sequential features into spatial ones for next stage.<br></li></ul><img src=\"paste-1e6c2ee2b695b6eb675a4cfae6cd7f1b8ac286a1.jpg\"><br>"
            ],
            "guid": "hl!h1AKE)D",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViViT: A Video Vision Transformer: Idea",
                "<ul><li>ViViT is proposed to achieve a pure-transformer architecture for video\nclassification<br></li><li>Extracts spatiotemporal tokens from the input video, which are then encoded by a series of\ntransformer layers<br></li><li>Several, efficient variants of ViViT which factorise the spatial- and temporal dimensions of\nthe input will be introduced:&nbsp;In order to handle the long sequences of tokens encountered in video<br></li><li>Achieves state-of-the-art results on multiple video classification benchmarks including\nKinetics 400 and 600, Epic Kitchens, Something-Something v2 and Moments in Time,\noutperforming prior methods based on deep 3D convolutional networks<br></li></ul>"
            ],
            "guid": "pY_nVgz4V(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViViT: Embedding of the Video Clips",
                "<ul><li>Uniform frame sampling (Left): simply sample 𝑛𝑡\nframes, and embed each 2D frame\nindependently following ViT.<br></li><li>Tubelet embedding (Right): We extract and linearly embed nonoverlapping tubelets that\nspan the spatio-temporal input volume<br></li></ul><img src=\"paste-a9fa56d463a4d36dca38c3c5634d099a8fc813d0.jpg\"><br><br>"
            ],
            "guid": "pk(01~uH|7",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViVit: Architecture Overview",
                "<img src=\"paste-a8efd147ccfefc8fdb2d4d9dfb0e076a63adc7d8.jpg\">"
            ],
            "guid": "n)6/Kyz`@A",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViViT: Spatio-temporal Attention Model",
                "<ul><li>This model simply forwards all spatio-temporal tokens extracted from the video, 𝑧0,\nthrough the transformer encoder.<br></li><li>Each transformer layer models all pair-wise interactions between all spatio-temporal\ntokens<br></li><li>Each transformer layer thus models long-range interactions across the video from the\nfirst layer.<br></li><li>This is different to CNN architectures, where the receptive field grows linearly with the\nnumber of layers<br></li></ul>"
            ],
            "guid": "H!`F@,bc*F",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViViT: Factorised encoder model",
                "<ul><li>Two transformer encoder in series</li><li>first models interactions between tokens extracted from the same temporal\nindex to produce a latent representation per time-index<br></li><li>second transformer models interactions between time steps. It thus\ncorresponds to a “late fusion” of spatial- and temporal information.<br></li></ul><img src=\"paste-6df15ee063d021ac661863513c6575b34765913b.jpg\"><br>"
            ],
            "guid": "cj!2Ce4=yA",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViViT: Factorised self-attention model",
                "<ul><li>Within each transformer block, the multi-headed self-attention operation is factorised\ninto two operations (indicated by striped boxes) that first only compute self-attention\nspatially, and then temporally.<br></li></ul><img src=\"paste-eb387293d7ead88a49ba336272543f23fbd66fdf.jpg\"><br>"
            ],
            "guid": "sgCsYr39k]",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ViViT:&nbsp;Factorised dot-product attention model",
                "<ul><li>For half of the heads, we compute dot-product attention over only the spatial axes, and for\nthe other half, over only the temporal axis.<br></li></ul><img src=\"paste-e74c11735dbae0770c4044c7f43496255f3d8a76.jpg\"><br>"
            ],
            "guid": "j:piJ1.._(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Learning with Less - Problems of annotation",
                "<ul><li>Deeap Learning thrives on large amounts of labeled data</li><li>However: Annotation is a costly process</li><li>Affected by several factors: Difficulty of domain, difficulty of instances, detail of annotation</li><li>Tradeoff between afforded annotation time and quality of trained model</li></ul>"
            ],
            "guid": "=6<K2h-F@",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Learning with less - Possible solutions",
                "<b>Minimize the annotation cost while closing the gap to supervised performance:</b><br><ul style=\"\"><li style=\"\">Object Localization</li><li style=\"\">Semi-supervised learning</li><li style=\"\">Representation Learning</li><li style=\"\">Domain Adaptation</li><li style=\"\">Active Learning</li></ul>"
            ],
            "guid": "yX!c~>.ezm",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Learning: Motivation",
                "<ul><li>Problem: Dense annotations are expensive</li><li>Idea: Can we train Computer Vision models using simpler annotations?</li><li>What are simpler annotations?&nbsp;\\(\\rightarrow\\) Depends on the task: Bounding-boxes, single points, scribbles, image-level labels</li></ul><img src=\"paste-e1e84942023ee7850fb9546fb97c0ed83f4eb63a.jpg\"><br>"
            ],
            "guid": "B/<[yPv~X$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Learning: Priors and Hints",
                "<b>Priors</b><br><ul style=\"\"><li style=\"\">Assumption of true statemetns independent of given samples</li><li style=\"\">Examples: Shape, location, contrast, ...</li><li style=\"\">Methods relating to such priors: Superpixels, region proposals, ...</li></ul><b>Hints</b><br><ul style=\"\"><li style=\"\">Given indirect supervision for a given sample</li><li style=\"\">Examples: Image labels, captions, boxes, scribbles, key points</li></ul>"
            ],
            "guid": "lw9HIxCuq^",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weak Supervision: Image-level Labels",
                "<ul><li>Instead of strong ground-truth\nsupervision we only get access to\nimage-level labels (as in classification)<br></li><li>So, we want to infer all class instances\nfrom the image label\n→ An image-label only occurs iff. there\nexists at least one instance/region of\nthe class in the bag/image<br></li><li>Commonly seen as Multiple Instance\nLearning (MIL) assumption<br></li><li>Reality is not always so nice:&nbsp;Sometimes no info which classes are not\npresent in the image, Some labels might be incorrect</li></ul>"
            ],
            "guid": "g^0L5K$!>s",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weak Supervision - Contrast",
                "<ul><li>We do not have complete information about the location of the instances<br></li><li>Models learn from images where classes are present and where they are not\npresent<br></li><li>The presence and absence of classes in different images allows to reason\nabout their position<br></li></ul>"
            ],
            "guid": "f>FppMfG=:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weak Supervision - Co-occurence",
                "<ul><li>Weakly supervised learning with image\nlabel supervision<br></li><li>Some labels co-occur with features not\ndirectly related to the object<br></li><li>If the dataset only contains boat in the\nwater, it will be hard to distinguish which\nregions belong to the boat<br></li><li>Common examples: Trains+rails,\nAirplains+Sky, Sheeps+field, …<br></li></ul>"
            ],
            "guid": "s#2c[=e:is",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervison -&nbsp; What defines an instance?",
                "<ul><li>Intend to find all instances present in an\nimage, however image-level labels have\nno indication of the amount<br></li><li>E.g. an image with one sheep and an\nimage with several sheep will have the\nsame label<br></li><li>How can we get a network to identify all\ninstances correctly?<br></li></ul>"
            ],
            "guid": "DBC|4#w=?c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervision - Network Activation",
                "<ul><li>Neural Networks tend to activate\nprimarily on the most discriminative\nfeatures of an object<br></li><li>For tasks like detection and\nsegmentation we intend to capture the\nentire instance<br></li></ul><img src=\"paste-fc5f7b4444633219fa2cdff16ea593bfa1776146.jpg\"><br>"
            ],
            "guid": "g(J_0PoNl$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Deep Detection Network: Overview",
                "<ul><li>First end-to-end weakly supervised object detection method<br></li><li>Utilizing pre-trained networks &amp; region proposals:&nbsp;Extract image wide features and pool for regions using SPP or RoI-Pool/Align<br></li><li>Classification and detection branch</li></ul><img src=\"paste-f665f348f741ef923753f0564e3d627b1f297b7c.jpg\"><br>"
            ],
            "guid": "wvknnq#BlM",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Deep Detection Network: Activations",
                "<ul><li>Classification branch performs for each box a softmax operation over classes:&nbsp;\\([\\sigma_{class}(x^c)]_{ij} = \\frac{e^{x^c_{ij}}}{\\sum_{k=1}^C e^{x^c_{kj}}}\\)<br></li><li>Detection branch performs for each box a softmax operation over boxes: \\([\\sigma_{det}(x^d)]_{ij} = \\frac{e^{x^d_{ij}}}{\\sum_{k=1}^{|\\mathcal{R}|} e^{x^d_{ik}}}\\)<br></li><li>Hadamard product of both branches results in box-wise classification score:&nbsp;\\(x^{\\mathcal{R}} = \\sigma_{class}(x^c) \\odot \\sigma_{det}(x^d)\\)<br></li></ul>"
            ],
            "guid": "kEPc+=3a-:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Deep Detection Network: Loss",
                "<b>Loss calculation</b><br><ul style=\"\"><li style=\"\">Region-wise scores are summed up over number of regions porposals&nbsp;\\(y_c = \\sum_{r=1}^{|\\mathcal{R}|}x_{cr}^{\\mathcal{R}}\\)<br></li><li style=\"\">Optimize the energy function:&nbsp;<img src=\"paste-8acaba786d5cdddcb9e9ee4ed2a123f6081cc78d.jpg\"></li></ul><b>Additionally, assume neighbors of a high scoring class should have high scores</b><br><ul style=\"\"><li style=\"\">Soft regularisation to penalize feature map\ndiscrepancies for all regions with an\noverlap with the most likely region of a\ncertain class of at least 60% IoU in training:&nbsp;<img src=\"paste-3ce0006ac7d538fba2f25b57687c5496d2703342.jpg\"></li></ul>"
            ],
            "guid": "oJ?pNrrD#$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Deep Detection Network: Advantages, Disadvantages, Limitations",
                "<b>Advantages</b><br><ul style=\"\"><li style=\"\">Learns End-to-End (no extensive pre/post processing)</li><li style=\"\">No custom layers</li></ul><b>Disadvantages</b><br><ul style=\"\"><li style=\"\">Tends to detect discriminative parts</li><li style=\"\">Groups instances</li></ul><b>Limitations</b><br><ul style=\"\"><li style=\"\">Does not leverage multiple positive instances</li><li style=\"\">Does not regress bounding boxes</li></ul><img src=\"paste-979791a8b9a8b5564bb0e6b95532ed100e41d6b3.jpg\"><br>"
            ],
            "guid": "H=zyp@hW0m",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Online Instance Classifier Refinement: Idea",
                "<ul><li>WSDDN tends to predict most\ndiscriminative regions<br></li><li>Proposed method:\nAdd multi-stage instance classifiers which\nutilize their previous stage as supervision<br></li></ul><img src=\"paste-b2a7e4e864546cf9fb105231642e84ab46624f69.jpg\"><br>"
            ],
            "guid": "oB{{|VL@:0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Online Instance Classifier Refinement Model",
                "<ul><li>First branch based on WSDDN: Multiple Instance detection network</li><li>Performs k-steps of online renfinement: Instance Classifier refinement; use highest scoring box of previous step as supervision</li><li>Prediction during inference as the aggreation of all heads</li></ul><img src=\"paste-ef61d61a52b570eb8e6ee42e4eb0644e81631f6b.jpg\"><br>"
            ],
            "guid": "zd([Z1K~b2",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Online Instance Classifier Refinement: Advantages and Limitations",
                "<b>Advantages</b><br><ul style=\"\"><li style=\"\">End-to-End WSOD method<br></li><li style=\"\">Creates Supervision in online steps<br></li><li style=\"\">Iterative refinement captures larger regions of\ndesired objects<br></li></ul><b>Limitations</b><br><ul style=\"\"><li style=\"\">Does not leverage multiple instances<br></li><li style=\"\">Still suffers from part-domination<br></li><li style=\"\">No box regression<br></li></ul>"
            ],
            "guid": "AbpC$=gwxl",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Instance-Aware, Context-Focused,\nand Memory-Efficient WSOD",
                "<b>Build upon working parts of previous methods:</b><br><ul><li>Main block: MIDN (Multiple Instance detection\nnetwork)</li><li>Refinement: OICR (Online Instance classifier Refinement)</li></ul><b>Proposes solutions to the limitations:</b><br><ul><li>Part Domination:&nbsp;Parametric spatial dropout to adversarially\nmaximize the detection objective\n-&gt; focus on context than on discriminative part</li><li>Instance Ambiguity:&nbsp;Pseudo-labels with label and regression (!)\ntargets by spatial diversification constraints</li></ul><img src=\"paste-b4487dbc1eee9b2f91625a29eb74d87489f989c0.jpg\"><br>"
            ],
            "guid": "IY>]v;9k*{",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Instance-Aware, Context-Focused,\nand Memory-Efficient WSOD: Multiple Instance Self-Training",
                "<b>Assumptions for instances</b><br><ul><li>Instance-associative: Overlapping boxes -&gt; similar label</li><li>Representativeness: Score of proposal indicates how fitting it is</li><li>Spatial-diversity: Useful indictive bias in self-training</li></ul><b>Self-training with regression</b><br><ul><li>Each refinement step\nhas an additional\nregression head<br></li><li>Target is inferred by prior\nrefinement step<br></li></ul>"
            ],
            "guid": "PU^!GI+J0P",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Instance-Aware, Context-Focused,\nand Memory-Efficient WSOD: Concrete DropBlock",
                "<ul><li>Idea: most methods only predict most\ndiscriminative features\n-&gt; if we 'eliminate' these features the network is\nforced to learn other relevant features</li></ul><b>Adversarial process for each ROI utilizing an\nresidual conv block with parameters θ</b><br><ul><li>Get mask of most likely areas\nvia a hard gumbel softmax and drop it<br></li><li>To avoid trivial solutions clip probability map&nbsp;\\(p_{\\theta}(r) = min(p_{\\theta}(r), \\tau)\\)<br></li><li>Maximize the loss wrt. θ while minimizing the\nloss in respect to the rest of the network w<br></li></ul><img src=\"paste-a3f1119ad86e2a543c86be09c65106691abf1939.jpg\"><br>"
            ],
            "guid": "GR#:d>-6~0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Instance-Aware, Context-Focused,\nand Memory-Efficient WSOD: Model sketch",
                "<img src=\"paste-bc765240107bfbb97d567dc7b4928698ad936626.jpg\">"
            ],
            "guid": "lA@-<@k`0K",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Weakly Supervised Region Proposal Network",
                "<ul><li>Traditional proposal mechanisms\nmight not always be the best\napproach<br></li><li>Depending on task and domain the\nperformance might be insufficient<br></li><li>Idea: Network activates on areas\nrelevant for the task: get Proposals on activations</li></ul><b>Multi-stage approach</b><br><ul><li>Fuse feature maps of different layers of\nthe network, get sliding windows and\ntheir objectness score with EB + NMS<br></li><li>Pool features and predict box\nrefinement<br></li><li>Finally perform WSD on new predicted\nboxes<br></li></ul><img src=\"paste-09f6eae51e3b2f973175f56ffcf174d8cdc27ab3.jpg\"><br>"
            ],
            "guid": "GL7fSZCho_",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Recursive training (Simple does it): Idea",
                "Sources of Information:<br><ul><li>The bounding boxes<br></li><li>Priors about the objects<br></li></ul>Cues/Assumptions that are used:<br><ol><li>Bounding boxes are expected to cover all\nrelevant objects, any pixel outside\n→ background<br></li><li>Boxes delimit the extend of objects, the box\ntherefore gives information of the expected\nobject area<br></li><li>Use contrast between background and\ncontent in the box, priors on object shape\nby using segmentation proposal techniques.<br></li></ol><img src=\"paste-bcf49851beb10d0da0d5f76da2e9ac3152d14a53.jpg\"><br>"
            ],
            "guid": "n(e9J1PTa|",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Recursive training (Simple does it): Basic algorithm",
                "<ul><li>Naive starting point:&nbsp;Train a segmentation model (e.g. FCN, DeepLab) by using boxes as segmentation\ntarget&nbsp;<img src=\"paste-fe0a14148fdb389efbbdc22df0caebdaec657a17.jpg\"></li><li>The result is better than the boxes\n→ Use the segmentation output as new ground-truth (recursive training)<br></li><li>For this examples it works quite good, but this naive approach easily\ndegenerades<br></li></ul>"
            ],
            "guid": "cP43ewZ:.*",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Recursive training (Simple does it): Algorithm improvements",
                "Add simple rules:<br><ul><li>Pixels outside bounding boxes are reset to background<br></li><li>Reset segmentations that get to small in relation to their box to the whole box<br></li><li>Use methods to filter segmentations, to better follow object boundaries (e.g.\nDenseCRF)<br></li></ul>Change the segmentation mask we start with:<br><ul><li>Do not use full boxes to train models at first, but only the center regions in the\nboxes, other regions are set to „ignore“ in the loss computation\n→ Idea lower recall but higher precision might be better<br></li><li>Continue with the same rules as before&nbsp;<img src=\"paste-ef773936b86c86533b0dc95592698ddbde70f18a.jpg\"><br></li></ul>"
            ],
            "guid": "ICdb9.?oS4",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS semantic segmentation: Recursive training (Simple does it): Advanced improvements",
                "Add segmentation proposal techniques:<br><ul><li>GrabCut algorithm is incoorperated to\napproximate segmentations by modeling the\nforeground and background using the box<br></li><li>MCG algorithm is used to integrate object\nshape priors by generating likely segmentations<br></li><li>Both algorithms refine the segmentations\nthat are used as ground-truth for training<br></li></ul>"
            ],
            "guid": "iKMENPuh`q",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Box2Seg: Overview",
                "<ul><li>Relies less on handcrafted rules and more on end-to-end learning, an attention mechanism and\nregularization<br></li><li>Encoder-decoder segmentation network with shared encoder<br></li><li>Three decoders, that decode:<br></li><li>Pixel embeddings: 𝑤𝑖𝑑𝑡ℎ × ℎ𝑒𝑖𝑔ℎ𝑡 × 𝑑 (d is a chosen embedding dimension)<br></li><li>Semantic segmentation output: 𝑤𝑖𝑑𝑡ℎ × ℎ𝑒𝑖𝑔ℎ𝑡 × #𝑐𝑙𝑎𝑠𝑠𝑒𝑠 + 1 → Softmax<br></li><li>Class-wise attention maps with: 𝑤𝑖𝑑𝑡ℎ × ℎ𝑒𝑖𝑔ℎ𝑡 × #𝑐𝑙𝑎𝑠𝑠𝑒𝑠 → Sigmoid<br></li></ul><img src=\"paste-7ee7d849d3ab6d9c289970f4f43b24dc1e0a3495.jpg\"><br><br>"
            ],
            "guid": "e]+5l1@(TK",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Box2Seg: Learning from coarse masks",
                "<ul><li>Box2Seg also refines boxes into coarse „masks“ via GrabCut<br></li><li>These coarse masks are then used to train the network, with standard crossentropy:&nbsp;\\[\\mathcal{L}_{GC} = - \\frac{1}{m} \\sum_{c=0}^L \\sum_{i=1}^m M(i,c) \\cdot log(y(i,c))\\]<br></li></ul><img src=\"paste-0428f1496b9f64cac60fbfd389cf5b4faa27b5ce.jpg\"><br>"
            ],
            "guid": "M|Pswcf-&3",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Box2Seg: Attention mechanism",
                "<ul><li>GrabCut segmentations are coarse and noisy, Box2Seg deals with this via an\nattention mechanism, produced by the decoder on the bottom<br></li><li>For each foreground-class 𝑐 and pixel 𝑖 this decoder produces an attention\nvalue 𝛼 𝑖, 𝑐 ∈ [0,1], a loss is setup by:&nbsp;\\[\\mathcal{L}_{fg} = \\frac{-1}{\\sum_i^m B(i,c)} \\sum_{c=1}^L \\sum_{i=1}^m \\alpha(i,c)B(i,c)log(y(i,c))\\]</li><li>The attention modeling only considers foreground classes, so for the\nbackground class we add a seperate loss (0 is the background class-index):&nbsp;\\[\\mathcal{L}_{bg} = - \\frac{\\sum_i^m B(i,0) log(y(i,0))}{\\sum_i^m B(i,0)} \\]<br></li></ul><img src=\"paste-0899d807362afe108ba5aaeb6d9afbd9d6533bbd.jpg\"><br>"
            ],
            "guid": "JU|<KNH(Gi",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Box2Seg: Pixel-wise affinity",
                "<ul><li>The last decoder branch enforces similarities between pixels that share the\nsame semantics and dissimilarities between pixels that don‘t<br></li><li>The decoder on top produces embeddings 𝛽 for all pixels<br></li><li>Using these embeddings, set up affinity matrix:&nbsp;\\[\\mathbb{A}(i,j) = \\beta_i \\cdot \\beta_j = \\frac{\\beta_j^T\\beta_i}{||\\beta_j||&nbsp; \\cdot ||\\beta_i||}\\]<br></li><li>To enforce that embeddings that match semantically have a high similarity and\na low similarity if they do not match semantically, the following loss is used:&nbsp;\\[\\mathcal{L}_{\\mathbb{A}} = \\sum_{i,j}(\\mathbb{A}(i,j) - y_j^Ty_i)^2\\]<br></li></ul>"
            ],
            "guid": "rCIvvP{fSd",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Class Activation Mapping (CAM)",
                "<ul><li>Basis for a lot of weakly supervised segmentation approaches<br></li><li>A classifier CNN is trained with the last feature maps having 𝑛 channels where 𝑛 is the number\nof classes<br></li><li>Afterwards a global average pooling (GAP) layer produces a feature vector with 𝑛 elements<br></li><li>This feature vector is feed through a linear classification layer, and a softmax layer<br></li><li>With a classifier that was trained this way, we can calculate CAMs<br></li><li>We forward an image through the CNN but get rid of the GAP and apply the last linear\nlayer on each 𝑛 dimensional feature at each location in the feature map<br></li><li>The resulting prediction map is resized to the input image size<br></li><li>This way, for each class, we get spatial activations → class activation maps<br></li><li>As the classifier is trained with image-level labels, we can use CAMs for segmentation: (Train GAP CNN; Calculate CAM for each image in the training set; For each image, get the channel in the CAM, that corresponds to the image-level label; Threshold the CAM in said dimension to obtain segmentation mask; Train a segmentation network on these coarse segmentation masks)</li></ul><img src=\"paste-84b4614c81c69bbf31ac52cf5ca1889e0b7577c7.jpg\"><br>"
            ],
            "guid": "E^GET3b]Gc",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Adversarial erasing: Idea",
                "<ul><li>The method „adversarial erasing“ tackles the problem of classifiers only\nidentifying the most discriminative regions<br></li><li>The basic idea is to train classifiers repeatedly and in each iteration exclude the\ndiscriminative regions of the previous iteration<br></li></ul><img src=\"paste-31d7c09f358360b7147ac144674abe17da318bcc.jpg\"><br>"
            ],
            "guid": "QQ*/-:A/gq",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Adversarial erasing: Training strategy",
                "<ul><li>Repeat, until classification scores deteriorate:</li></ul><ol><li>Train classifier on image level labels<br></li><li>Calcuate CAMs and threshold them to determine most disciminative regions<br></li><li>Mask out these thresholded regions from the images in the dataset<br></li><li>Train a new classifier on the images with masked out regions<br></li></ol><ul><li>Masked out regions of an image make up its segmentation mask<br></li><li>Train a standard segmentation model supervised with these masks<br></li></ul><img src=\"paste-1e798cc7d60541829ba3b4bf645a7c69054b9ce1.jpg\"><br>"
            ],
            "guid": "lKES$01GNp",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Self-supervised Equivariant Attention\nMechanism (SEAM): Motivation",
                "<ul><li>Self-supervised Equivariant Attention\nMechanism (SEAM) directly tries to\nimprove generated CAMs<br></li><li>When training segmentation models, an\nimplicit constraint is enforced when data\naugmentation is used.\n→ Equivariance constraint<br></li><li>E.g. if the image is flipped, the groundtruth also has to be flipped<br></li><li>Classifiers for CAMs do not necessarily\nhave to fulfill this constraint, largely\nbecause of the pooling function<br></li></ul><img src=\"paste-2c6080c9ded359b654f25ddc2785c984fbdf0d70.jpg\"><br>"
            ],
            "guid": "A?t{&IMwe9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: SEAM: Equivariant regularization",
                "<ul><li>In SEAM, for the classification network 𝐹, that produces the CAM, a\nregularization loss is introduced:&nbsp;\\[\\mathcal{R}_{ER} = || F(A(I)) - A(F(I)) ||_1\\]<br></li><li>Here,&nbsp;\\(A\\) is a spatial affine transformation (e.g. scaling) and 𝐼 is the input image</li><li>To calculate this loss, the image 𝐼 is forwarded twice:&nbsp;Once as is and a second time transformed as 𝐴(𝐼)<br></li><li>The output feature map of the former forwardpass is then transformed by 𝐴(⋅)<br></li><li>Thus, logically, the feature maps have to match, to enforce this L1-loss<br></li></ul><img src=\"paste-e3d5316e976c70817ae8ee40ff4923932f46eada.jpg\"><br>"
            ],
            "guid": "c<)=GqA>&E",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: SEAM: Self-attention",
                "<ul><li>The authors propose to add a self-attention mechanism, refining the CAMs<br></li><li>The motivation is to better model the context information\n→ Refine CAMs with similarity of pixels in the pixel-correlation module (PCM)<br></li></ul><img src=\"paste-f54f507df528ec4359d11c7e49632f0b6691442c.jpg\"><br>"
            ],
            "guid": "x7KvOI_RC5",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Pixel-correlation-module",
                "<ul><li>With only convolutional layers, achieving equivariance is hard\n→ Add self-attention-like structure to capture broader context information<br></li><li>The pixel-correlation module tries to achieve this<br></li><li>Self-attention uses dot product and embedding functions 𝜃 and 𝜙, pixel-correlation\nuses only one embedding function and cosine similarity as affinity measure<br></li><li>ReLU is used in the PCM to zero out negative affinities<br></li></ul><img src=\"paste-5431e41618b55070c6960840bd99bf6e87896b04.jpg\"><br>"
            ],
            "guid": "PAP4=~5w<7",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: SEAM: Losses",
                "<ul><li>Multi-label classification loss on top of the original pooled CAM predictions:&nbsp;Both from original and transformed input<br></li><li>Equivariance regularization<br></li><li>Equivariance cross regularization with PCM refinements and CAMs<br></li></ul><img src=\"paste-e0e6495cc4354a7b8cf462f9e093a61b0a3e8b76.jpg\"><br><img src=\"paste-0b7a33b46b99ca6180067ff563f04c745fd6bb3d.jpg\"><br><br><img src=\"paste-e801016f8dd6c6ee1b1680502f22ed303bfde8a5.jpg\"><br>"
            ],
            "guid": "KI&D-!n/PY",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SW Semantic Segmentation: Context Decoupling: Idea",
                "<ul><li>If we would have segmentation masks for the images, we could use them to cut\nout objects and paste them into other contexts&nbsp;<img src=\"paste-1ca79e3dcf178bb864b80e3070cbe4f98471e1e5.jpg\"></li><li>If we then train a CNN on these images, we have to alter the ground-truth such\nthat it contains all the classes (from image and the pasted object)<br></li><li>This way we have training data, that decouples objects from specific contexts<br></li></ul>"
            ],
            "guid": "xumH4rlo[F",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "WS Semantic Segmentation: Context Decoupling Augmentation: Training",
                "1st Stage:<br><ul><li>We do not have segmentation masks<br></li><li>First we need approximate segmentations → utilize CAM model<br></li><li>With approximate segmentations, we can paste objects between images and\nstart training with „context decoupling augmentation“ (stage II)<br></li></ul>2nd Stage:<br><ul><li>In the second stage, we continue training the CAM network<br></li><li>This time, we leverage the idea of mixing images by pasting objects from other images\n→ Here we have to also alter target image-level labels as new class will be present<br></li><li>Train with the normal image and the pasted-image<br></li></ul><img src=\"paste-17ac8d03cd57635e01c884457df2a5d5cd437855.jpg\"><br>"
            ],
            "guid": "zL9|<(.?<W",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Semi-supervised Learning (SSL)?",
                "<ul><li>Wikipedia:\n„Semi-supervised learning is an approach to machine learning that combines a\nsmall amount of labeled data with a large amount of unlabeled data during\ntraining. Semi-supervised learning falls between unsupervised learning (with no\nlabeled training data) and supervised learning (with only labeled training data).”<br></li><li>O. Chapelle; B. Scholkopf; A. Zien [10]:\n“SSL is particularly devoted to application domains in which unlabeled data are\nplentiful, such as images processing, information retrieval, and bioinformatics.\nSSL is halfway between supervised and unsupervised learning and so, the\ndata set is divided into labeled and unlabeled data sets.”<br></li></ul>"
            ],
            "guid": "crlt~0_%v&",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contunity assumption / Smoothness assumption",
                "<ul><li>Points that are close to each other are more likely to share a label<br></li><li>This yields a preference for decision boundaries in low-density regions, so few points are\nclose to each other but in different classes<br></li></ul>"
            ],
            "guid": "tfk}F#<[-0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cluster assumption (special case of the smoothness assumption)",
                "Points in the same cluster are more likely to share a label (although data that\nshares a label may spread across multiple clusters). → Makes possible feature learning with clustering algorithms."
            ],
            "guid": "b^pYSUsfA9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Manifold assumption",
                "The data lie approximately on a manifold of much lower dimension than the\ninput space. In this case learning the manifold using both the labeled and\nunlabeled data can avoid the curse of dimensionality. Then learning can\nproceed using distances and densities defined on the manifold. → We assume that instead of in input space, we can also work on lower\ndimensional feature space."
            ],
            "guid": "dB/E^hg5&M",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Proxy label methods",
                "<ul><li>Definition:\nProxy label methods are a class of semi-supervised learning algorithms that\nproduce proxy labels for the unlabeled data using the prediction function 𝝋\nitself or some variant of it without any supervision.<br></li><li>Proxy labels are used together with labeled data, providing additional training\ninformation, even if the produced labels are noisy or weak and do not reflect the\nground truth.<br></li></ul>"
            ],
            "guid": "s{@8+NGZ9`",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-training",
                "<ul><li>The most naive proxy-label method is self-training (sometimes used synonymous with\npseudo-label)<br></li><li>Self-training can be done offline: pseudo-labels are updated for all of&nbsp;\\( \\{x_i\\}_{i=l+1}^{l+u} \\) in a certain frequency<br></li><li>Self-training can be done online:&nbsp;In each iteration the unlabeled images in the current batch are newly „pseudo-labeled“<br></li><li>The number of labeled and unlabeled images is quite different (recall: 𝑙 ≪ 𝑢)<br></li><li>Therefore it is important to balance the loss of the labeled and the unlabeled\nimages:&nbsp;\\[ L = \\frac{1}{n} \\sum_{m=1}^n \\sum_{i=1}^C L(y_i^m, f_i^m) + \\alpha (t) \\frac{1}{n'} \\sum_{m=1}^{n'} \\sum_{i=1}^C L(y_i^{'m}, f_i^{'m}) \\]<br></li></ul>"
            ],
            "guid": "G$:L>2c@F{",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-training: Weight Scheduling",
                "<ul><li>How does weight scheduling function&nbsp;\\(\\alpha(t)\\) look like?</li><li>No general rule, varies between datasets, it is a hyperparameter to be tuned<br></li><li>To show an example, [2] use:&nbsp;<img src=\"paste-7a716a334e4151d55f1c9dda0d40c1fab962d1bb.jpg\"><br></li></ul>"
            ],
            "guid": "C>2kLz|Ek6",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-training: Improvements",
                "<ul><li>Network output-probabilities are poorly calibrated [16], using a fixed threshold 𝜏 for\naccepting pseudo-labels might be a bad strategy\n→ Throttling: a variant of self-training which accepts pseudo-labels of the n unlabeled samples\nwith the highest confidence to the labeled data, instead of all samples exceeding 𝜏.<br></li><li>Network outputs are, of course, sometimes incorrect:&nbsp;Naive self-training reinforces these incorrect predictions;&nbsp;Overfitting to these incorrect precitions is called confirmation bias;&nbsp;The model cannot correct its own mistakes and aplifies them<br></li><li>Tackle confirmation bias</li></ul>"
            ],
            "guid": "oJ.O|;bl7X",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Tackling confirmation bias",
                "<ul><li>Restrict the batch sampling to contain a specified minimum number of annotated samples<br></li><li>This means we oversample labeled datapoints as otherwise the unlabeled samples will\ndominate the loss (recall: 𝑙 ≪ 𝑢) in the optimization process<br></li><li>Upweighting the loss on labeled examples can have a similar effect<br></li><li>It is proposed to use MixUp [5] data augmentation:&nbsp;Sample two image-label pairs (𝑥𝑝, 𝑦𝑝) ; (𝑥𝑞, 𝑦𝑞) and interpolate a new image and soft label\nfrom them with a randomly drawn 𝛿 and train with these:&nbsp;<img src=\"paste-3c6049444fabd9bc0d9b97cfa07b259a45422f32.jpg\"><br></li><li>This requires two forwardpasses, when 𝑦𝑝 and 𝑦𝑞 are pseudo-labels themselves!<br></li></ul>"
            ],
            "guid": "x0}(sBYa:8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Co-training",
                "<ul><li>Another proxy-label method, which\noriginally was proposed in 1998 for\nweb-page classification<br></li><li>Definition:\nCo-training requires that each data point 𝑥 can be represented unsing two\nconditionally independent views 𝒗𝟏(𝒙) and 𝒗𝟐(𝒙), and each view is sufficient\nto train a good model. After training two prediction models 𝒇𝟏 and 𝒇𝟐 on a\nspecific view on the labeled set&nbsp;\\( \\{ (x_i, y_i) \\}_{i=1}^l \\). We start the proxy labeling\nprocedure. At each iteration, an unlabeled data point is added to the training\nset of the model 𝒇𝒊\nif the other model 𝒇𝒋 outputs a confident prediction with\na probability higher than a threshold 𝜏.<br></li><li>With this setup, we enhance the training set, by using two views on the data<br></li><li>We restrict the hypothesis space to solutions that produce consistent\npredictions between the views<br></li><li>Idea is that the algorithms complement each other to jointly solve the problem<br></li></ul>"
            ],
            "guid": "oYt}d;8oa@",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Co-training (RGB+D Object Recognition)",
                "<ul><li>Initial phase: Train one CNN on depth- and one CNN on RGB images on&nbsp;\\( \\{ (x_i, y_i) \\}_{i=1}^l\\)<br></li><li>Then apply each model on&nbsp;\\( \\{ x_i \\}^{l+u}_{i=l+1}\\) and label most confident samples for the other model<br></li><li>CNNs are trained in the next round<br></li><li>Repeat until no more confident examples can be chosen<br></li><li>A fusion layer to combine the two streams is added and the whole model is trained jointly<br></li><li>co-training does not lead to convincing results:&nbsp;Model overfits in the initial phase -&gt; add a pre-training strategy;&nbsp;In Co-training Phase, the model is prone to predict a biased distribution over categories -&gt; add diversity preserving co-training which balances the samples from \\( \\{ x_i \\}^{l+u}_{i=l+1}\\)</li></ul><img src=\"paste-9337b4b4ef2ef21d05ce20eaf9f728672f082869.jpg\"><br>"
            ],
            "guid": "m4#1eqdn;0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Co-training (RGB+D Object Recognition): Pretraining",
                "<ul><li>Pre-training by reconstructing either the RGB image or the depth surface normals:</li></ul><img src=\"paste-0c1424398aee9a1c0c4bf18e97b6fe02c5451491.jpg\"><br><ul><li>Initialization with this\nreconstruction task works\nbetter than imagenet pretraining for this task<br></li></ul>"
            ],
            "guid": "FV2b}zv?_V",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Co-training (RGB+D Object Recognition): Diversity preserving co-training",
                "<ul><li>Goal: Keeping selected examples for labeling such that they uniformly cover each category\nas well as intra-class attributes for each category.<br></li><li>Cluster the CNN-features to obtain multiple clusters for each category, done for RGB\nand depth<br></li><li>In calculating pseudo-labels, it is considered, that samples spread across these clusters\nuniformly → then predictions of such selected samples are added<br></li></ul>"
            ],
            "guid": "CX|{|O/vqZ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Tri-training: Motivation",
                "<ul><li>Instead of different views on the data (co-training), we can instead use multiple models<br></li><li>The models are independently trained, then their agreement is used in a selftraining kind of way\n→ This idea is e.g. called tri-training, when using three models</li></ul>"
            ],
            "guid": "H5QR&,/gN`",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Tri-training: Idea",
                "Tri-Training:<br><ul><li>Train three prediction functions&nbsp;\\(f_1, f_2\\) and&nbsp;\\(f_3\\)<br></li><li>An unlabeled data point is added to the labeled set of function \\(f_i\\) if \\(f_j\\) and&nbsp;\\(f_k\\) agree on the predicted label<br></li><li>Training stops if no data points are being added<br></li></ul>No multiple views needed"
            ],
            "guid": "@g?k9K3a}",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Tri-training (Tri-net)",
                "<ul><li>Tri-training was developed before the big neural nets, jointly training three\nnetworks might be computationally infeasible!\n→ A shared backbone with multiple classification-heads can be used ([21] from\n2018)&nbsp;<img src=\"paste-3cba2285a0d9852b5f4f96b9a2d51217fa55a29e.jpg\"><br></li><li>Tri-training is performed by enhancing the labeld training set of one model by the agreement\non an unlabeled sample of the other two<br></li><li>The average of the maximum posterior prediction of those two models is taken, if it exceeds\na threshold it is added as pseudo-labeled image to the third model‘s training set<br></li></ul>"
            ],
            "guid": "juAHs!W/4_",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Tri-training (Tri-net): Problems",
                "Problem: We have to make sure that the classifiers do not simply learn the\nsame function.<br><ul><li>This is tackled by „output smearing“ the labeled samples are altered slightly for each of the\nclassification heads differently<br></li><li>Output smearing adds noise to the ground-truth one-hot labels sampled from a standard\nnormal distribution, then the resulting smeared label is normalized to sum to 1<br></li><li>Training is done with the standard cross-entropy loss</li><li>Different classification head architectures help as well<br></li></ul>"
            ],
            "guid": "A^>WukhC*|",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Tri-training (Tri-net): Further techniques",
                "<ul><li>Pseudo-label editing is introduced<br></li><li>A mechanism to recover from incorrect pseudo-labels (recall: confirmation bias)</li></ul>Dropout is used when computing pseudo-labels to determine whether the\npredictions of the classification heads are stable:<br><ul><li>For a pseudo-labeled sample \\((x, \\bar{y})\\) compute 𝐾 forward passes with dropout active<br></li><li>If the frequency of the predictions being different to \\( \\bar{y} \\) exceeds&nbsp;\\(\\frac{K}{3}\\) , it is regarded unstable and\nis eliminated<br></li></ul><br>"
            ],
            "guid": "I+=vt$iIjg",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Consistency Regularization",
                "<ul><li>Definition:\nA primary track of recent deep semi-supervised methods can be summarized as consistency-based\nmethods. In this type of methods, two roles are commonly created, either explicitly or implicitly: a teacher\nmodel and a student model. The teacher guides the student to approximate its performance under\nperturbations. The perturbations could come from the noise of the input or the dropout layer, etc. A\nconsistency constraint is then imposed on the predictions between two roles, and forces the\nunlabeled data to meet the smoothness assumption of semi-supervised learning.</li><li>Recall the smoothness assumption:\n“Points that are close to each other are more likely to share a label.”<br></li><li>In other words, these methods introduce a loss term such that the prediction of an unlabeled image and\nthe prediction of a perturbed version of the same image is consistent.<br></li><li>This consistency regularization idea can be seen as constraining hypothesis space H to solutions\nsatisfying this consistency requirement<br></li></ul>"
            ],
            "guid": "gsaHJ6/v(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Mean Teacher: Idea",
                "<ul><li>Consistency regularization works if the input is perturbed specifically or the\nteacher model is chosen carefully (e.g. using the same model as teacher and\nstudent would not work as it produces the same predictions)<br></li><li>The mean teacher model is a consistency regularization method with an explicit\nteacher<br></li></ul><img src=\"paste-954677df3fd9afda610f4b08fc66641f224366a3.jpg\"><br>"
            ],
            "guid": "Hg6r*5;2pL",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Mean Teacher: Training procedure",
                "<img src=\"paste-f94bf7453a4dd955981ca92c5a7aa37d83d35e8e.jpg\"><br><ul><li>Both the classification loss and consistency loss are backpropagated via SGD through the\nstudent model<br></li><li>The teacher model is not directly trained<br></li><li>The teacher model 𝜃\n′\nis updated after each iteration by:&nbsp;<img src=\"paste-64adda44c5eed1044c51f24ae3cbe6565515b0e3.jpg\"><br></li><li>The updated teacher parameters&nbsp;\\(\\theta_t'\\) are the combination of parameters of the previous\nteacher&nbsp;\\(\\theta_{t-1}'\\) and the current student&nbsp;\\(\\theta_t\\)<br></li><li>\\(\\alpha\\) is a smoothing coefficient hyperparameter<br></li></ul>"
            ],
            "guid": "ox%TELqGt[",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Mean Teacher: Why is using EMA for the teacher model sensible?",
                "<ul><li>Aggregates information after each iteration<br></li><li>Can be seen as an ensemble of models of all previous iterations\n→ strong, robust predictions<br></li><li>Previously temporal ensembling was done on predictions for all images in the dataset which\nis expensive, the EMA for a teacher model is more economical<br></li><li>In inference the teacher is used as it generally yields better predictions<br></li></ul>"
            ],
            "guid": "Bp-%J`x-<:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MixMatch",
                "<img src=\"paste-cd7a6d049143b20c98515f99629a6ee5788d57f4.jpg\"><br><img src=\"paste-262ce599bd5b32c79e8d377bfe2ab1da4321810f.jpg\"><br><img src=\"paste-0227840a90cb69e7f6038e2e9f578a89d34ef987.jpg\">"
            ],
            "guid": "xwAnIf4?4/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ReMixMatch: Three alterations",
                "<ul><li>Distribution alignment<br></li><li>Augmentation anchoring<br></li><li>Rotation prediction (from self-supervised literature)<br></li></ul>"
            ],
            "guid": "evxorDWhb0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ReMixMatch: Distribution alignment",
                "<ul><li>Aggregate of predictions on unlabeled data should\nmatch the distribution of labeled data<br></li></ul><img src=\"paste-9019a144a987ffd7dab474bce66d7c70237867ea.jpg\"><br>"
            ],
            "guid": "gNh;OWnyJ$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ReMixMatch: Augmentation anchoring",
                "<ul><li>To improve consistency regularization, the basic\nidea in augmentation anchoring is to use the\nmodel’s prediction for a weakly augmented\nunlabeled image as the guessed label for many\nstrongly augmented versions of the same image.<br></li></ul><img src=\"paste-b5dd8629c74380fc1d2ff86059549c3a38e574f6.jpg\"><br>"
            ],
            "guid": "c>tg?}NhXx",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ReMixMatch: Rotation Prediction",
                "<ul><li>The images are rotated by 0, 90,180, 270 degrees,\nthe model has to classify the used rotation.</li></ul>"
            ],
            "guid": "Fu3N:++O_C",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "FixMatch",
                "Simplified methof for SSL pseudo-labeling:<br><ul><li>Augment an unlabeled example with weak augmentations to predict the pseudo-label, accept\npseudo-labels only if the highest predictions exceeds a threshold 𝜏<br></li><li>Then train the model by enforcing this pseudo-label on strongly augmented versions of this\nunlabeled image</li></ul><br><ul><li>Weak augmentations: horizontal flipping (50% probability), random translation (12,5%\nprobability)</li><li>Strong augmentations: AutoAugment, RandomAugment or CTAugment with CutOut<br></li></ul><img src=\"paste-e67cefa81955b57eb93642f7a5cd5118665426e2.jpg\"><br><img src=\"paste-386bd1c4d3623ed8a866e9b11d5773a7f5e59966.jpg\"><br><br>"
            ],
            "guid": "x]~IzV(Zxi",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-supervised SSL",
                "<ul><li>Use supervision from the data itself in the form of pre-text task<br></li><li>Here we want to learn rich and transferable features for downstream tasks from\nthe unlabeled data first<br></li><li>Pretext tasks can be: predicting rotation, predict patch position, colorization,\ncontrastive predictive coding, …</li></ul>Training procedure:<br><ul><li>Train with the self-supervised pretext task on the whole dataset, then add standard crossentropy loss using the small&nbsp;\\(\\{ (x_i, y_i) \\}_{i=1}^l \\) (multi-task training, see ReMixMatch [9])</li><li>Or: Pre-train with the self-supervised task and fine tune with the labeled data afterwards<br></li></ul>"
            ],
            "guid": "rs/l]@7.=a",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Three challenges in Deep Learning (Yann LeCun)",
                "<img src=\"paste-98c1d8ae16c01b8446ebe10e5cac97219ca5e867.jpg\">"
            ],
            "guid": "FOE]1|u_Lm",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Why Self-Supervised Learning",
                "Prediction is the essence of intelligence:<br><ul><li>Humans and animals learn through observation of the world, they build models of the world by\npredicting<br></li><li>The observation and ability to predict gives us common sense<br></li><li>This acquired common sense (model of the world) enables us to learn new things/tasks very\nquickly<br></li></ul>For machines:<br><ul><li>We aim to develop this common sense via observation of swathes of data available to us<br></li><li>Creating labeled datasets for each task is an expensive, time-consuming, tedious task<br></li><li>Self-supervised learning can potentially generalize better because we learn more about the world<br></li></ul><br>"
            ],
            "guid": "pvy+>I$xrG",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "What is Self-Supervised Learning?",
                "Observe and Predict:<br><ul><li>A form of unsupervised learning where the data itself provides the supervision signal<br></li><li>In general, withhold some part of the data and task the network with predicting it<br></li><li>Usually, define a pretext task for which the network is forced to learn what we really care about<br></li><li>For most pretext tasks, a part of the data is withheld and the network has to predict it<br></li><li>The features/representations learned on the pretext task are subsequently used for a\ndifferent downstream task, usually where some annotations are available<br></li></ul>"
            ],
            "guid": "k3.6y=eF:c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-Sup. Learning: Pretext Task Example: Rotation Prediction",
                "<img src=\"paste-35f8c6ff4117515bfcca84d95649432d5919f49c.jpg\">"
            ],
            "guid": "tn<RF<2_Bf",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-Sup. Learning Pipeline",
                "<img src=\"paste-d188d5a3d78d4604c09bc97bcb90d002b1fac379.jpg\">"
            ],
            "guid": "HZr901pEll",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-Supervised Learning Techniques",
                "<img src=\"paste-b2ae73a550d14b86d8fa47be8c3c02ef07c51efc.jpg\">"
            ],
            "guid": "G.dZ!O<^62",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Self-Supervised Learning: Pretext Tasks",
                "For Images:<br><ul><li>Relative patch position for context prediction<br></li><li>Predict patches position in a jigsaw puzzle<br></li><li>Predict missing pieces, also known as context\nencoders, or inpainting<br></li></ul>For Videos:<br><ul><li>Frame ordering also known as shuffle and learn<br></li><li>Video colorization or temporal coherence\nof color<br></li></ul>For NLP:<br><ul><li>Predict the order of words in a randomly shuffled\nsentence:<br></li><li>Predict masked sentences in a document<br></li></ul>Domain-Agnostic:<br><ul><li>Clustering</li></ul>"
            ],
            "guid": "C8L$bG#dO8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pretext tasks: Relative patch position for context prediction",
                "<ul><li>Training data: multiple patches extracted from images</li><li>Pretext task: train a model to predict the relationship between the patches. (E.g predict the relative position of the selected patch below. For the center patch, there are eight possible neighbor patches\n(eight possible classes))</li></ul><img src=\"paste-3240a86a46d54ee1452333768f16e2971744a230.jpg\"><br>"
            ],
            "guid": "Nu$%`Y%}]u",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pretext tasks: Predict patches position in a jigsaw puzzle",
                "<ul><li>Training data: nine patches extracted in images</li><li>Pretext task: predict the position of all nine patches:&nbsp;Instead of predicting the relative position of only two patches,\nthis approach uses the grid of 3×3 patches and solves a jigsaw\npuzzle</li></ul><img src=\"paste-bcdbe115d5fde9d2d7bc77071680f8b024484e81.jpg\"><br>"
            ],
            "guid": "C)Jf^67$(S",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pretext tasks: Predict missing pieces, also known as context encoders or inpainting",
                "<ul><li>Training data: remove random region in images</li><li>Pretext task: fill in a missing piece in the image:&nbsp;The model needs to understand the content of the entire image,\nand predict a plausible replacement for the missing piece</li></ul><img src=\"paste-d38c2ec87dc54f15f85671f9dfe4efe568c7c858.jpg\"><br>"
            ],
            "guid": "l[Fk-5Zl}m",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pretext tasks: Frame ordering also known as shuffle and learn",
                "<ul><li>Training data: videos of objects in motion with shuffled\norder of the frames<br></li><li>Pretext task: predict if the frames are in the correct\ntemporal order<br></li><ul><li>The frames are shuffled, and pairs of videos with correct and\nshuffled order are used for training the model<br></li><li>The model needs to learn the object classes, as well as it needs\nto learn the temporal ordering of the objects’ positions across\nthe frames<br></li></ul></ul><img src=\"paste-c420c9adfdde8768070020961c0fd9c67089b60a.jpg\"><br><br>"
            ],
            "guid": "z2nYYd!t6G",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pretext tasks:&nbsp;Video colorization or temporal coherence\nof color",
                "<ul><li>Training data: pairs of color and\ngrayscale videos of moving objects<br></li><li>Pretext task: predict the color of moving\nobjects in other frames<br></li><ul><li>The learned representations are useful for\ndownstream segmentation, object tracking, and\nhuman pose estimation tasks<br></li></ul></ul><img src=\"paste-a520ea6c22c847734e00ca73e518fafb311acc64.jpg\"><br>"
            ],
            "guid": "ffs-JJ5!4U",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Clustering based Self-Supervised Learning, Pretext Task: Clustering",
                "<ul><li>Even without any training, similar data\nproduces similar representations<br></li><ul><li>weak learning signal</li></ul><li>Iteratively until convergence</li><ul><li>Cluster representations<br></li><li>Use cluster assignments as labels<br></li><li>Train a few epochs<br></li></ul><li>Also works with cross-modality data, e.g.\npaired audio and video<br></li></ul><img src=\"paste-635a33aa070cf98fcaaf97b6d410628b65635b7f.jpg\"><br>"
            ],
            "guid": "jJ&}tTX*t",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Clustering based Self-Supervised Learning, Deep clustering of images",
                "<ul><li>The model treats each cluster as a\nseparate class<br></li><li>The output is the number of the cluster\n(i.e. cluster label) for an input image<br></li><li>The authors used k-means for clustering\nthe extracted feature maps<br></li></ul><img src=\"paste-7e6efca05feb10ecdd6a9adac2267ed56d1f5dad.jpg\"><br>"
            ],
            "guid": "*`2~D!mTL",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Clustering based Self-Supervised Learning: Cross-modality data",
                "<ul><li>Cross modality data</li><li>e.g audio and video</li><li><img src=\"paste-8d99fa0608fcae8162a4714848d5fc17259ca277.jpg\"></li></ul>"
            ],
            "guid": "pYSCO9X[Xp",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: Principle",
                "<ul><li>Contrast between similar and dissimilar inputs<br></li><li>How to know what is same and not same in SelfSupervised Learning ?</li><ul><li>Use data augmentation to generate same pairs\n(positives)<br></li><li>Different augmented views of the same image&nbsp;<img src=\"paste-57bb1211adb25d28f65fdaa71067faa83d5d546b.jpg\"></li><li>Assume that all other data is dissimilar (negatives)<br></li><ul><li>Not always the case, but if the data has a high variation of\nobjects / concepts, it will be correct most of the time<br></li></ul></ul></ul><img src=\"paste-09a88dda1b66233143cc626063c9dfd0ae3b31d3.jpg\"><br>"
            ],
            "guid": "jC/q6tQ<}A",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: representation encoder",
                "<ul><li>Learn representation encoder&nbsp;\\(f_{\\Theta}\\)<br></li><ul><li>Representations&nbsp;\\(y_+\\)</li><li>Train by instance discrimination</li><li>Matching / Non-matching data</li></ul></ul>Types of Contrastive Learning<br><ul><li>How to sample matching / non-matching representations?</li><li>Representations invariant to specific input changes?</li><li>One or multiple encoders?</li><li>Many differnt ways to do this</li></ul><img src=\"paste-5f0c755cbcf73d3dac8051bcc55724873ff9e96f.jpg\"><br><img src=\"paste-28d8a3941bed64a21dd3447d7c52af83a326f679.jpg\"><br>"
            ],
            "guid": "A)gNcLSugy",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: Face Verification",
                "<ul><li>Contrastive Learning is an Old Technique</li><li>Face verification in 2005</li><li>Train CNN weight&nbsp;\\(W\\) to enable discrimination task</li><li>Euclidean distance based contrastive loss<img src=\"paste-482f73353bc216525e83dcb1f129d67e546ac6e1.jpg\"></li></ul>"
            ],
            "guid": "Nv`#Q&sS1Q",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: Batch Contrast",
                "<ul><li>Representations in the Stimulus set are\nusually generated with&nbsp;\\(f\\)<br></li><ul><li>But&nbsp;\\(f\\) changes during training!</li><li>If dictionary representations (especially positive) have been\ngenerated with an outdated encoder, loss does not converge<br></li><ul><li>Dictionary set looses semantic relation to query<br></li></ul></ul><li>Sample stimulus set from current batch<br></li><li>Make sure that each sample has a positive pair in the batch<br></li><li>Assume other batch samples to be negatives<br></li><li>Often used with\nNoise Contrastive\nEstimation:&nbsp;\\[ \\mathcal{L}_N = - \\mathbb{E}_X \\left[log \\frac{f_k(x_{t+k}, c_t)}{\\sum_{x_j \\in X}f_k(x_j, c_t)}\\right] \\]<br></li></ul><img src=\"paste-7edc11a8786b67c8e08ca5f2b8318b32cd1fccfd.jpg\"><br>"
            ],
            "guid": "g/QO?^Bu:F",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: SimCLR",
                "<ul><li>Specifically train for invariance to image\naugmentations<br></li><li>Contrast against other images<br></li><li>InfoNCE based loss with temperature scaling<br></li><li>Paired images are augmented to increase\ndifficulty of the task<br></li><li>Near supervised performance<br></li><li>Normalized Temperature-scaled Cross\nEntropy Loss (NT-Xent)&nbsp;\\[ \\mathcal{l}_{i,j} = -log \\frac{exp(sum(z_i,z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{[k \\ne i]} exp(sim(z_i, z_k) /\\tau)} \\]<br></li></ul><img src=\"paste-1c0305f73387e5bfbff1afad99f05619bd9e93a3.jpg\"><br>"
            ],
            "guid": "D4,G(^Y-`{",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: Memory Contrast",
                "<ul><li>Batch contrast is limited by batch size<br></li><li>Use queue of representations<br></li><ul><li>But encoder changes too fast\n→ Representations do not provide\na valid learning signal anymore<br></li></ul><li>Enforce stable representations<br></li><li>Save representations\nfrom previous epochs</li><ul><li>Proximal regularization<br></li><li>Update with momentum term<br></li></ul></ul><img src=\"paste-af4955100c456877c9c291607e408227cd25a7d5.jpg\"><br>"
            ],
            "guid": "q-DN*nI}By",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning:&nbsp;Non-Parametric Instance Discrimination",
                "<ul><li>Randomly initialize memory bank entries&nbsp;\\(v_i\\)</li><li>Minimize&nbsp;\\(J(\\theta) = - \\sum_{i=1}^n log\\, P(i | f_{\\theta}(x_i))\\) with&nbsp;\\( P(i|v) = \\frac{exp(v^T_iv / \\tau)}{\\sum_{j=1}^n exp(v_j^Tv/\\tau)} \\)<br></li><li>More contrastive sample&nbsp;\\(m\\) increase accuracy</li><li>128D feature vectors are normalized</li></ul><img src=\"paste-77bfdd016fc6b9a8e5b2ce2fce4dcef2dd192359.jpg\"><br><ul><li>Memory entries are replaced each epoch,\nno smooth update<br></li><li>Leads to high oscillation, especially at\nbeginning of training<br></li><li>Proximal Regularization: Minimize euclidean distance of&nbsp; positive pairs<br></li><li>Extend loss with&nbsp;\\( \\lambda || v_i^{(t)} - v_i^{(t-1)} ||^2_2 \\)<br></li><li>This allows to perform memory contrastive\nlearning without momentum updates<br></li><li>As training goes on, representations stabilize\nand proximal loss looses influence<br></li></ul>"
            ],
            "guid": "snj@y9gE,j",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning:&nbsp;Pretext-Invariant Representation Learning (PIRL)",
                "<ul><li>Differences to Non-Parametric Instance Discrimination<br></li><ul><li>Augmented image as positive pair\ninstead of memory representation<br></li><li>Update with momentum term instead of replacement<br></li></ul><li>Jigsaw puzzling (Noroozi et al.) as image\ntransformations<br></li><li>Memory Bank entries from untransformed images<br></li><li>Loss targets memory representation instead of paired\nrepresentation:&nbsp;\\[L(I, I^t) = \\lambda L_{NCE}(m_I, g(v_{It})) + (1- \\lambda)L_{NCE}(m_I, f(v_I))\\]<br></li></ul><img src=\"paste-8a3162453d26c55dbb1a1ef4caec05285d08319a.jpg\"><br>"
            ],
            "guid": "Egs6Jk<MOB",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: Momentum Contrast",
                "<ul><li>Different solution to stable\nrepresentations<br></li><li>Slowly moving second encoder instead\nof memory entries<br></li><li>Enables training on infinite datasets</li><ul><li>Memory Banks have to complete epochs to\nslowly update the stored representations<br></li></ul><li>Keeping a second network not as costly<br></li><ul><li>Only inference is needed, most resources\nare used by backpropagation<br></li></ul><li>Gradients for queue samples are\nignored<br></li></ul><img src=\"paste-3f50de25449fc22a33661edaf5281b2b7a397f54.jpg\"><br><ul><li>Slowly progressing momentum encoder (m=0.999)</li><ul><li>Second network which is slowly updated:&nbsp;\\(\\theta_k \\leftarrow m\\theta_k + (1-m)\\theta_q\\)<br></li></ul><li>Allows to continuously sample new data<br></li><ul><li>Memory banks expect to see the same input in each epoch<br></li><li>Momentum contrast does not rely on the representations of\nthe previous epoch and can randomly select unseen data<br></li></ul></ul><img src=\"paste-8426fba602999ee54c1003512201b9865330080c.jpg\"><br>"
            ],
            "guid": "u6An$1},uK",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning: Contrastive Clustering (SwAV): Principle",
                "<ul><li>Prototype vectors in latent space<br></li><li>Create soft cluster assignments<br></li><li>Contrast on assignment vector instead\nof representations<br></li></ul>Latent space representations might\ncollapse to a single cluster<br><ul><li>Use Sinkhorn-Knopp algorithm to\nprevent trivial solution<br></li></ul><img src=\"paste-7aa0eb2b481dd0ee7f4b0390474657ad792f59bf.jpg\"><br>"
            ],
            "guid": "CYJ`Ryy0b+",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning:&nbsp;Contrastive Clustering (SwAV): Technique",
                "Technique<br><ul><li>Based on Asano et al. [4] and Caron et al. [10]<br></li><li>Combines clustering and contrastive learning<br></li><li>Increases similarity of cluster assignments&nbsp;\\(y\\)<br></li><li>No contrastive term in the loss function<br></li></ul>Sinkhorn-Knopp Algorithm<br><ul><li>Intends equal usage of all cluster centers<br></li><li>Generates target assignments&nbsp;\\(q\\)<br></li></ul><img src=\"paste-06fc603005d37146b5d13ee8d9f100bae6a1bca6.jpg\"><br>"
            ],
            "guid": "f0L^eO+dyN",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Bootstrapping Existing Learning Signals",
                "<ul><li>Describes a self-started and self-growing process<br></li><li>\"Bootstrapping\" is used very ambiguously<br></li></ul><b>Observation</b><br><ul style=\"\"><li style=\"\">Randomly initialized network\n→ Representations which are better than random<br></li><li style=\"\">Similar input samples are likely to have (slightly similar)\nrepresentations<br></li></ul><b>Basic Idea</b><br><ul style=\"\"><li style=\"\">Train a network by its own generated representations<br></li></ul>"
            ],
            "guid": "Fel1?`mL#{",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Bootstrap Your Own Latent",
                "<ul><li>Similar to Momentum Contrast<br></li><li>But Contrastive learning requires positive and negative\npairs<br></li><ul><li>For example multiple augmented data samples<br></li><li>Sufficiently close to make it challenging<br></li><li>→ Large number of negative samples<br></li><li>Avoids collapsing representations<br></li></ul></ul><b>Can it be done without negative pairs?</b><br><ul style=\"\"><li style=\"\">Less dependent on batch size<br></li></ul><br><ul><li>Grill et al. show that momentum encoders can be used\nwithout a contrastive term<br></li><li>Target network not optimized (“stop gradient”)<br></li><ul><li>Its weights are momentum updates of the online network<br></li><li>Necessary to prevent representation collapse<br></li></ul></ul><img src=\"paste-6f72e024a6386afbfabcbfbd6aa9d1c4007bde1b.jpg\"><br>"
            ],
            "guid": "wLpapj1]YG",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Image-based Self-Supervised Learning",
                "<ul><li>Contrastive learning methods have\ndominated self-supervised learning in\nrecent years</li><li>Pre-text tasks are still important but\npart of contrastive learning frameworks<br></li><li>Recent self-supervised representation\nlearning approaches already beat the\nsupervised state-of-the-art on some\ntransfer learning tasks<br></li></ul>"
            ],
            "guid": "rB;WY2~Aw!",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Video-based Self-supervised Learning",
                "<ul><li>Most image-based techniques can easily be extended\nto video-based learning<br></li><ul><li>Pre-text tasks have to be changed<br></li><li>contrastive learning almost without any changes<br></li></ul><li>But video contains more information<br></li><li>Changes of perspective and lighting during video<br></li><li>Dynamics of time and motion<br></li><ul><li>Sense of speed<br></li><li>Time direction<br></li><li>Body / Object movements<br></li></ul><li>Paired audio data<br></li><ul><li>Subtitles / Speech<br></li><li>Object sounds (e.g. glass)</li></ul></ul><img src=\"paste-0b517da9f364eb17461b38452ee6a051c504aa59.jpg\"><br>"
            ],
            "guid": "G3T>R?Y)$s",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Contrastive Learning from Video Data",
                "<ul><li>Study comparing multiple known contrastive learning paradigms on video data<br></li><ul><li>SimCLR<br></li><li>Momentum Contrast (MoCo)<br></li><li>Bootstrap Your Own Latent (BYOL)<br></li><li>Swapped Augmented Views (SwAV)<br></li></ul><li>Contrastive learning Task<br></li><ul><li>Match multiple augmented and temporally shifted clips from the same video</li><li>Contrast against clips from other videos in the same batch<br></li><li>Batch size 𝜌 * B with 𝜌 being the number of clips per video and B being the number of different videos<br></li></ul></ul><img src=\"paste-5530b2182de1f9d815796383e61ffd642687eed4.jpg\"><br>"
            ],
            "guid": "e]348EhO<N",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Constrastive Learning for Videos: Importance of Augmentations",
                "<ul><li>Having multiple positive clips per video\nsignificantly improves performance for all\napproaches<br></li></ul><b>Agumentations are important</b><br><ul style=\"\"><li style=\"\">MoCo and BYOL less dependant on\naugmentations (still a big difference)<br></li><li style=\"\">SimCLR and SwAV performance varies\nstrongly with augmentations<br></li><li style=\"\">Learning temporal persistency and colour\naugmentation invariance is most important<br></li><li style=\"\">Cropping as augmentation is not as powerful<br></li></ul>"
            ],
            "guid": "=)B.XA]Ek",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transfer Learning: Motivation",
                "<ul><li>Transfer Learning (TL) is often used when there is a lot of training data available in principle but\nnot for the actually desired task<br></li><li>The main idea is to reuse existing, learned \"knowledge\"<br></li><li>One of the simplest forms of TL is pretraining a model on e.g. ImageNeT<br></li></ul>"
            ],
            "guid": "D1c*`1Iu^9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transfer Learning: Domain and Task",
                "<ul><li>Key terminology for transfer learning is the concept of a <b>domain</b> and a <b>task</b><br></li><li>A <b>task</b> \\(T\\) consists of a label space \\(y\\) and an objective predictive function&nbsp;\\(f(\\cdot)\\)<br></li><ul><li>Given training pairs&nbsp;\\(\\{ x_i, y_i \\}\\) function \\(f\\) can be used to predict the corresponding label<br></li></ul><li>A <b>domain</b>&nbsp;\\(D\\)&nbsp;consists of a feature space χ and a marginal probability distribution\nP(X) where X={x1\n, x2\n, ...}∈χ</li></ul>"
            ],
            "guid": "CXDi:T*dD*",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transfer Learning vs \"Normal Learning\"",
                "<img src=\"paste-9d61e3a44cf9bcd79b05dfff151f6bfeff9671fa.jpg\">"
            ],
            "guid": "i1%4L<#=;+",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Adaptation",
                "<ul><li>One of the most popular subareas of transfer learning<br></li><li>Deals with the problem of adapting a model (e.g. a CNN) to a previously unseen domain with a\ndifferent marginal distribution<br></li><li>Main idea: Labeled training pairs \\(\\{x_i\n,y_i \\}\\) are only available for a source domain \\(D_S\\) while the\ntarget domain \\(D_T\\) contains little or no labeled data<br></li><li>Objective: Still classify/segment/detect instances correctly in the target domain</li></ul><img src=\"paste-a334fe72b91e9d783fcdc1682531bf69a38d71a8.jpg\"><br>"
            ],
            "guid": "ypuK-$>wQ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Adaptation: Domain shift, Domain Gap",
                "<ul><li>Common DA setup: \\(D_S\\)≠\\(D_T\\) and marginal distributions \\(P(D_S)\\)≠\\(P(D_T)\\), however task \\(T_S=T_T\\)</li><li>Main challenge: domain shift or domain gap between the different distributions and adapt the model in a way that performs reasonable on the target domain</li><li>Domain shift: the data the model is trained on is different than the data the model is evaluated on, e.g. synthetic-to-real data, winter to summer landscapes …</li></ul><img src=\"paste-9b1f701b517808b1ea2d090ab6cb0e16ada506b5.jpg\"><br>"
            ],
            "guid": "qdJ13(BGpT",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Unaddressed Domain Shift",
                "<ul><li>Deep CNNs are very sensitive to changes in data distribution<br></li><li>Unaddressed domain shift: significant decline in performance!<br></li><li>Example: driver observation, sensor placement changes<br></li></ul><img src=\"paste-bfd0ea4cbde38946359e2289e7e60f716d4457e6.jpg\"><br>"
            ],
            "guid": "c@w(!&Lbhj",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Supervision Levels of Domain Adaptation",
                "<ul><li>Semi-supervised Domain Adaptation: target domain has some (usually &lt;10) labeled examples<br></li><li>Unsupervised Domain Adaptation: target domain only contains images but no labels<br></li><li>Domain generalization: no training data (labeled or unlabeled) available in the\ntarget domain!<br></li></ul><img src=\"paste-e142435093cb26d454eefb7e01485dc4128fec2b.jpg\"><br>"
            ],
            "guid": "o6^83GZ1o9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Adaptation: Partial DA, Open Set DA",
                "<ul><li>There are also modifications where TS≈TT\n, most commonly due to differences in the class sets\nC of the source and target domain</li><ul><li>Partial Domain Adaptation<br></li><li>Open Set Domain Adaptation<br></li></ul></ul><img src=\"paste-cbc2749090a368760bffa394b91f90f670253622.jpg\"><br>"
            ],
            "guid": "t|^>YXVw@+",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Open Set DA: Source-private Classes",
                "<ul><li>Literature also defines the Open Set DA task with a number of source-private classes<br></li><li>These classes are exclusive to the source domain and do not appear in the target domain data<br></li><li>Based on this, Universal Domain Adaptation [21] proposed a variable ξ, which defines the size\nof the shared class set<br></li><li>The difficulty is now to design an DA algorithm that can also distinguish if a class even appears\nin the target domain data and do so across a wide spectrum of ξ</li><li>ξ is defined as the Jaccard distance between the two sets, i.e:&nbsp;\\(\\xi = \\frac{|\\mathcal{C}_s \\cap \\mathcal{C}_t|}{|\\mathcal{C}_s \\cup \\mathcal{C}_t|}\\)<br></li></ul><img src=\"paste-2e44669237e1b53f47acc75d321dd8dfce87e5a3.jpg\"><br>"
            ],
            "guid": "OT9z&.om3P",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Adversarial Neural Network (DANN): Architecture",
                "<ul><li>One of the most famous architectures for unsupervised DA<br></li><li>Four main components:<br></li><ul><li>Base CNN for feature extraction<br></li><li>Label predictor&nbsp;\\(G_y\\)<br></li><li>Domain classifier&nbsp;\\(G_d\\)<br></li><li>Gradient Reversal Layer (GRL)<br></li></ul></ul><img src=\"paste-eca264ba365e4229620babb7e71804f473a72866.jpg\"><br>"
            ],
            "guid": "eAu]44o%9F",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Adversarial Neural Network (DANN): Training",
                "<ul><li>Use source data to finetune the CNN and label predictor by minimizing the classification loss&nbsp;\\(L_y\\)<br></li><li>Use source+target data to train the domain classifier<br></li><li>Reverse the gradient in the GRL by multiplying the gradient with -λ<br></li><li>Variable λ is smoothly interpolated as training progresses (usually from 0 to 1)<br></li><li>This forces the features f to become domain-invariant i.e. the domain classifier cannot\ndistinguish the domains of the samples anymore<br></li><li>If features f are actually domain-invariant, then the label predictor \\(G_y\\)\n(trained with source\ninstances) can just be reused to classify target domain instances<br></li></ul>"
            ],
            "guid": "CB{vFGwH%S",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Batch Normalization",
                "<ul><li>Goal: zero-mean unit-variance activations<br></li><li>Training: compute the empirical mean&nbsp;\\(\\mu_k\\) and variance&nbsp;\\(\\sigma_k^2\\) of\nthe batch independently for each dimension 𝑘,\nthen normalize using batch statistics<br></li><li>At test time: \\(\\mu_k\\) and \\(\\sigma_k^2\\) are not computed based on the batch!</li><ul><li>Use fixed empirical mean and standard deviation of the\ndata computed during training.<br></li><li>The mean and standard deviation are estimated during\ntraining with running averages.<br></li></ul></ul><img src=\"paste-d01a81b70dd54b2610e03afe02f9e13931bc06e9.jpg\"><br>"
            ],
            "guid": "d06>DrTq.3",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Specific Batch Normalization (DSBN): Motivation",
                "<ul><li>Synthetic to real adaptation tasks commonly have a very large domain gap<br></li><li>Reflected in the color distributions of the images: synthetic images are\ncommonly grayscale without textures, while real life images use the full\ncolor space<br></li><li>Batch-Normalization learns these color (or feature) distributions in the form\nof a mean μ and standard deviation σ per channel<br></li><li>During inference, the accumulated population statistics are used<br></li><li>Obviously, the statistics of the source domain are no good match for the\ntarget domain and vice versa!<br></li></ul><img src=\"paste-9a74699cd3ce3c6fab9632bbc358240ed378e93a.jpg\"><br>"
            ],
            "guid": "J0l1]z7=Kx",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Specific Batch Normalization (DSBN): Principle",
                "<ul><li>Domain-Specific Batch-Normalization addresses the discrepancy of source domain statistics and target domain statistics by using different BN layers per domain<br></li><li>Setting: Unsupervised domain adaptation for standard image classification<br></li><li>The whole network is trained with both source and target domain images<br></li><li>Source domain images are normalized by \\(BN_S\\) while target domain images take the \\(BN_T\\) branch<br></li><li>How to train with unlabeled target domain images?<br></li><ul><li>Finetune model first on source domain<br></li><li>Use the trained model as pseudo labeler to generate a label estimate for the target domain<br></li><li>Use the pseudo label like a normal label in e.g. a cross-entropy loss and further train the model<br></li><li>Use the refined model as a pseudo labeler in the following training cycle<br></li></ul></ul><img src=\"paste-5530fc973b7776f0374a6fc53026383b06d20e3a.jpg\"><br>"
            ],
            "guid": "mH7.f%kA:|",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle-Consistent Adversarial Domain Adaptation: CyCADA: Overview",
                "<ul><li>Cycle-Consistent Adversarial Domain Adaptation (CyCADA) is a very famous Cycle-GANbased approach for unsupervised DA<br></li><li>Adaptation at multiple different levels<br></li><li>Pixel-level adaptation based on Cycle-GAN: Map source image into target domain and also\nback to the source domain. Try to fool discriminator \\(D_T\\) and satisfy the cycle consistency.<br></li><li>CyCADA can be used for different tasks such as semantic segmentation and classification, both\nin an unsupervised DA setup<br></li><li>Model needs to be adjusted to the task by a custom task loss<br></li><li>For classification this can e.g. be a simple cross-entropy loss, for segmentation this can be a\npixel-wise cross-entropy loss, Dice loss etc.<br></li></ul><img src=\"paste-c7b19ac120aa61f064c4025b98264b9f92f9c8ee.jpg\"><br>"
            ],
            "guid": "x2gBaVTT53",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle-Consistent Adversarial Domain Adaptation: CyCADA: Semantic-consistency",
                "<ul><li>Encourage consistency before and after image translation using\nlabeled source data<br></li><li>Main gist: Only the style of the image is translated into the target domain, the objects/classes\nshould be consistent between the domains<br></li><li>The translated image is still a \"source\" instance, so the ground truth can be used<br></li></ul><img src=\"paste-055808a11993c8446283e2e6fa9a727dab5e2cae.jpg\"><br>"
            ],
            "guid": "peD<tqR$f!",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle-Consistent Adversarial Domain Adaptation: CyCADA: Feature-level adaptation",
                "<ul><li>Try to learn domain-invariant features by fooling&nbsp;\\(D_{feat}\\)<br></li><li>Concept is similar to DANN, but implemented differently<br></li><li>As only features are extracted, no labels are needed for the target domain<br></li></ul><img src=\"paste-a26606c7ecd88062e73f77d55869cf5e64b04954.jpg\"><br>"
            ],
            "guid": "O,~m2-%cpf",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle-Consistent Adversarial Domain Adaptation: CyCADA: Inference",
                "<ul><li>As \\(f_T\\)\nis trained with target-like data, it can later be used to predict target domain\nclasses, masks etc.<br></li><li>Remaining generators/discriminators not needed after training<br></li></ul><img src=\"paste-20c2d02008c9a6ab3d962349928e36c3bf6fd2dc.jpg\"><br>"
            ],
            "guid": "j(Mc>VzDIQ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Alignment",
                "<ul><li>Another common approach is to tackle the domain shift by aligning the\ndistribution of the source and target domain features<br></li><li>Deep CORAL uses the distance between second-order statistics\n(covariances) for this alignment<br></li><li>There are many other approaches:<br></li><ul><li>Kullback-Leibler divergence on logits\naligns classifier output between domains<br></li><li>Maximum Mean Discrepancy (MMD)<br></li><li>Moment Matching<br></li><li>Subspaces described by eigenvectors<br></li><li>Uncertainty of predictions<br></li><li>Contrastive loss and other metric\nlearning approaches<br></li><li>And many more!<br></li></ul></ul><img src=\"paste-009634588cccaf44a062b5e2814d638b50bef7c1.jpg\"><br>"
            ],
            "guid": "N:%tdB,%fx",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Domain Generalization",
                "<ul><li>Goal: a model that can generalize to unseen\ndomains without any available training data\n(labelled or unlabelled) in these domains<br></li><li>Oftentimes (not always): multiple source domains</li><li>Different ways to address this problem<br></li><li>Data manipulation:<br></li><ul><li>Data augmentation<br></li><li>Data generation<br></li></ul><li>Representation Learning (e.g. feature\ndisentanglement through gradient reversal)<br></li><li>Adapting the learning strategy (e.g. ensembles,\nmeta-learning)<br></li></ul><img src=\"paste-5c64c72b14d0a52825a7f7cccd84d9e18c4008a4.jpg\"><br><img src=\"paste-83ec79f662e6f8863adb8658d34d4cbdec27a2e0.jpg\"><br>"
            ],
            "guid": "u__,Jnv4:%",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generating New Pseudo-Domains: Motivation",
                "<ul><li>Data generator: map source domains to\nnew synthesized pseudo-domains<br></li><li>Why? Additionally use pseudo domains\nusing the classifier training to enhance\ndata diversity<br></li></ul><img src=\"paste-410dc42d9d19c13b14581f703ac9b1bedb7fa203.jpg\"><br>"
            ],
            "guid": "t|aZqb%Q^}",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generating New Pseudo-Domains: Generator Training",
                "Generator training (four losses in total):<br><ul><li>Main loss: maximize the distance between\nsource domains and the new pseudodomains (optimal transport)</li><li>Diversity of generated domains: maximize\nthe difference between the generated novel\ndistributions (diversity loss)</li><li>Ensure semantic consisteny:</li><ul><li>Cross-entropy classification loss to\nmaintain the category label<br></li><li>Cycle-consistency loss: reconstruct the\noriginal representation from thr\ngenerated data<br></li></ul></ul><img src=\"paste-cfa3e79b235a66f25cf89926fb8b99399dd72549.jpg\"><br>"
            ],
            "guid": "L~>14WeqqL",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generating New Pseudo-Domains: Classifier training",
                "Classifier training: additionally use\npseudo domains using the classifier\ntraining to enhance data diversity<br><ul><li>→ Consistent improvement for image classification\nin novel domains<br></li></ul>"
            ],
            "guid": "pbLy/s];m",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Efficient Building Blocks: Complexity of full convolution",
                "<ul><li>Standard convolution: Most commonly a \\(3 \\times 3 \\times D_{in}\\) filter kernel (\\(h \\times w \\times D_{in}\\))<br></li><li>Single spatial position: multiply &amp; add \\(3 \\times 3 \\times D_{in}\\)&nbsp;values of the input with those of the filter kernel<br></li><li>Example below: input volume with \\(H_{in}=W_{in}=7\\) and \\(D_{in}\\) channels and a filter with \\(h=w=3\\) and \\(D_{in}\\)\nchannels and no padding<br></li><li>Outcome: \\(h \\times w \\times D_{in}&nbsp;\\times H_{out} \\times W_{out} \\times D_{out}\\) Multiply-Add operations and \\(h \\times w \\times D_{in} \\times D_{out}\\) weights<br></li></ul><img src=\"paste-b9ef0c4d03ad173145ec072062a37da50a2b7c5c.jpg\"><br>"
            ],
            "guid": "F&Cu.H&>G:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Efficient Building Blocks: 3x3 vs 1x1 convolutions",
                "<ul><li>Often \\(h=w\\) for a filter kernel, complexity is therefore quadratic w.r.t. \\(h\\) (or \\(w\\))<br></li><li>In terms of computations, \\(h=w=3\\) is therefore 9 times as expensive as \\(h=w=1\\)!<br></li><li>Takeaway: 1x1 convolutions are cheap!<br></li><li>Problem: 1x1 filters lack spatial awareness, a CNN with only 1x1 filters would not perform well<br></li><li>But: we can use 1x1 convolution to reduce the input dimension Din and apply 3x3 filters\nafterwards → the total number of 3x3 convolutions is reduced!<br></li></ul><img src=\"paste-b9566cba1f9de1ee88e9ae1774fc064f38e6d9d6.jpg\"><br>"
            ],
            "guid": "imhe%X`-Go",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SqueezeNet v1",
                "<ul><li>1x1 convolutions extensively used in SqueezeNet v1<br></li><li>Basic building block is the \"Fire module\"<br></li><ul><li>First \"squeeze\" input: Reduce number of channels with cheap\n1x1 convolutions<br></li><li>Then \"expand\" with a combination of 1x1 (cheap) and 3x3 (spatial\ninformation) filters<br></li><li>Concatenate output of 1x1 and 3x3 convolution<br></li></ul><li>Lowers both computation time and parameter count<br></li></ul><img src=\"paste-13430ce4d8b567121fba6f330f9e56054a4a1832.jpg\"><br><img src=\"paste-ac5f895650d6622dd83703b61c4a8853c8fecea7.jpg\"><br>"
            ],
            "guid": "b1kny%f189",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Grouped Convolution",
                "<ul><li>Grouped convolution (sometimes called group convolution)<br></li><li>First introduced in AlexNet in 2012, at that time more an implementation\ndetail, nowadays used for speeding up networks<br></li><li>Main gist: divide input volume into groups. Filters only \"work\" on their group, in\nthe example below number of groups g=2.<br></li><li>Each filter only has 1/g amount of work and parameters<br></li><li>But each filter also only sees 1/g channels and cannot work on all information<br></li></ul><img src=\"paste-80eb2fc6d014c340bfb9be96e461085c1617f539.jpg\"><br>"
            ],
            "guid": "Be&dE)`;<e",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Depthwise Separable convolution",
                "<ul><li>Depthwise convolution is a special case of grouped convolution with \\(g=D_{in}\\)<br></li><li>Every filter group only filters 1 channel of the input volume. This is very cheap\ncomputationally and has very few parameters.<br></li><li>Depthwise separable convolution: depthwise convolution followed by a 1x1\nconvolution (1x1 convolution is also referred to as pointwise convolution)<br></li></ul><img src=\"paste-8f06883b5a3063ca1a1a840a19d9140254d2074b.jpg\"><br>"
            ],
            "guid": "Ad5mHgt<g3",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Depthwise Separable Convolution: Complexity",
                "<ul><li>(Reminder: standard convolution: \\(h \\times w \\times D_{in}&nbsp;\\times H_{out} \\times W_{out} \\times D_{out}\\) Multiply-Add operations and \\(h \\times w \\times D_{in} \\times D_{out}\\) weights)</li><li>Depthwise part has \\(h \\times w \\times D_{in} \\times H_{out} \\times W_{out}\\) Multiply-Add operations and \\(h \\times w \\times D_{in}\\) weights</li><li>Pointwise part has \\(D_{in}&nbsp;\\times H_{out} \\times W_{out} \\times D_{out}\\) Multiply-Add operations and \\(D_{in} \\times D_{out}\\) weights</li><li>For most inputs/outputs, even the combination of depthwise and pointwise part\nis more computationally efficient than a standard convolution<br></li></ul><img src=\"paste-51faa6aa8f3a29082b875683d2fd03e5ddc7356b.jpg\"><br>"
            ],
            "guid": "by|(=bLif5",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "MobileNets",
                "<ul><li>MobileNet v1 is mostly based on depthwise separable convolution<br></li><li>Basic building block is indeed very basic, but has been shown to work decently\nfor many different tasks<br></li><li>MobileNet v2 expands on this basic unit and adds skip connections and\ninverted residual structures<br></li></ul><img src=\"paste-aabfef2e5afac78682425e4ed0201dbc3590d55b.jpg\"><br>"
            ],
            "guid": "e9=0P&KmY:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ShuffleNet",
                "<ul><li>ShuffleNet extensively uses grouped convolution<br></li><li>Problem: When only using grouped convolution, information\nof the groups is never mixed (left). A <font color=\"#ff0000\">red</font>&nbsp;group filter would only\nwork on information from previous <font color=\"#ff0000\">red</font> filters.<br></li><li>Solution: channel shuffle layer (right). Channels\nare now mixed so that the next <font color=\"#ff0000\">red</font> filter can also consider\ninformation from the <font color=\"#aaff00\">green</font> and <font color=\"#0000ff\">blue</font> group<br></li></ul><img src=\"paste-e5b9154ba5b5795c8377ec54dccc497e01b61205.jpg\"><br><img src=\"paste-72f7c1b9217c4be3a9eeadc3238f41ae3e2a6629.jpg\"><br>"
            ],
            "guid": "A=t8bt>>l7",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Efficient Building Blocks – Downsampling",
                "<ul><li>For CNNs, computational demand also depends on the size \\(h \\times w\\) of the input<br></li><li>Filters have to be evaluated at every spatial position, which is expensive for\nlarge input sizes<br></li><li>As often \\(h=w\\), there is an obvious quadratic relationship between number of\ncomputations and the input size</li><li>Thus, a common strategy of efficient neural networks is downsampling fast<br></li><ul><li>Mostly handled by the top 2 layers (\"stem cells\")<br></li><li>Often a normal convolution with stride 2 (MobileNet v1) or a convolution with stride 2\nfollowed by max pooling with stride 2 (SqueezeNet, ShuffleNet)<br></li><li>The latter reduces the common input size of 224x224 to 56x56 in only 2 layers!<br></li><li>This results in only 1/16th of spatial positions w.r.t. the input image<br></li></ul></ul>"
            ],
            "guid": "o.VTW,m.%A",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Mixed Precision: Motivation",
                "<ul><li>Commonly, neural networks are trained with 32-bit floating point (FP32) inputs\nand weight parameters<br></li><li>This ensures a large range of representable numbers at the cost of storage\nspace and computational powe</li><li>Using a smaller data type such as FP16 (half precision) would ensure more\nlightweight and more performant models and also faster training!<br></li><li>Problem: Representable range of FP16 is small, due to 5-bit exponent and 10-\nbit mantissa</li><ul><li>Gradients below 2-24 are rounded towards 0!\nThis actually happens quite a lot during training<br></li></ul></ul>"
            ],
            "guid": "DG<{A~u;-$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Mixed Precision: Principle",
                "<ul><li>Problem:&nbsp;Training diverges with FP16 although it would have converged with a\nFP32 data type</li><li>Solution: Using a mixed precision approach with both FP16 and FP32 while\nalso scaling the loss to an appropriate range</li></ul><img src=\"paste-91bfc8cb36dc8d4ad58ba110fb0187e13138bdab.jpg\"><br>"
            ],
            "guid": "hNY`Wx>=KV",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Mixed Precision: Benefits",
                "<ul><li>Half precision math throughput can be 2x-8x faster than single precision on modern GPUs<br></li><li>Weights stored on GPU take less space. Batch size can be increased!<br></li><li>Data transfers from/to the GPU are faster<br></li><li>Results mostly stay the same and can even increase in same cases<br></li><li>Easy to use in most deep learning frameworks such as PyTorch<br></li></ul>"
            ],
            "guid": "lc=Rus;FCF",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pruning: Idea",
                "<ul><li>Pruning: removing redundancy/low value information from the network<br></li><li>Pruning starts with a \"bigger/heavier\" network and tries to reduce the size<br></li><li>Objective: Eliminate neurons or whole filters (in a CNN) while maintaining the\nmetric (e.g. accuracy)<br></li><li>Can help to remove e.g. multiple filters that learned (almost) the same feature\nlike edge detection or color features<br></li><li>Redundancy is actually quite common in NNs: Think about training with\ndropout, where often 50% of the values are randomly zeroed<br></li></ul>"
            ],
            "guid": "P~*${l03;h",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pruning: Common Setup",
                "<ul><li>There is not a singular pruning strategy that always works. Many different approaches can\nachieve a good pruning ratio<br></li><li>However, a common setup is<br></li><ul><li>Find unimportant filters according to some metric<br></li><li>Remove filters and adjust the filters of the subsequent layer<br></li><li>Finetune to \"repair\" the damage<br></li><li>Repeat until the target pruning percentage is achieved<br></li></ul></ul><img src=\"paste-4b77301b82c08ba58e65d31ca330449ca16bbc83.jpg\"><br>"
            ],
            "guid": "j#@}Q{;#~t",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pruning: Common strategies and metrics",
                "<ul><li>How to determine which filter to remove?<br></li><li>Common strategies and metrics:<br></li><ul><li>Sum of absolute weight values in a filter. Small weights tend to produce weak activations and do not contribute\nmuch. ℓ1 or ℓ2 norms are commonly used.<br></li><li>Average Percentage of Zeros in a filter. Considers the sparsity of a filter, many zeros = information loss<br></li><li>Phrasing it as an optimization problem. [17] tries to find a filter that affects the output of the following layer the\nleast, removes it and finetunes the network.<br></li><li>[18] uses an iterativ pruning approach, temporarily removing filters while monitoring the sensitivity metric of a\ndetection task. Filters leading to the smallest drop are removed. No finetuning needed after every step<br></li></ul><li>Differences in pruning setups:<br></li><ul><li>Iterative vs. one-shot methods: Iterative setups only remove a small amount of filters per step<br></li><li>Finetuning: Iterative methods often retrain after every pruning step, others only at the end.<br></li><li>Structured vs. Non-structured pruning: Structured pruning removes whole filters, non-structured removes single\nweights to induce sparsity. This often requires special hard- or software to handle.<br></li><li>Global vs. Local pruning: Global pruning considers all filters, local e.g. only a single layer.<br></li></ul></ul>"
            ],
            "guid": "l|V&Tm1#FQ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Zero-Shot Learning: Motivation",
                "<ul><li>Datasets get larger and larger, labeling is expensive and time-consuming<br></li><li>Long-tail distributions of categories<br></li><li>We will never be able to annotate all possible categories<br></li></ul><img src=\"paste-602296c3def84de27dd774984b5e463811e56952.jpg\"><br><ul><li>Humans have an impressive ability to address new tasks by transferring solutions from\nfamiliar problems.<br></li><li>Knowing the concepts lion and tiger, we can coarsely “imagine” a mixture of both concepts\nwithout ever seeing an image of a liger.<br></li></ul><img src=\"paste-6a9ff2fcf7279fc313697be13af78688586f6ea0.jpg\"><br>"
            ],
            "guid": "s_eTu~@r@z",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Zero-Shot Learning: Idea",
                "<ul><li>Goal:&nbsp;Assign target categories without any (in our case visual) training data</li><li>How? Use some form of auxiliary semantic information to link the known and unknown\ncategories<br></li><li>Auxillary semantic information (also “side-information”, “semantic embedding”) represents\nobservable distinguishing properties of objects&nbsp;<img src=\"paste-4f49e383b77539f8b3ef56d0282b901d5b6d2514.jpg\"><br></li></ul>"
            ],
            "guid": "kn.LH#OAB1",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Zero-Shot Learning: Definition",
                "<ul><li>Assign target categories without any (in our case\nvisual) training data by leveraging a semantic\nembedding.<br></li><li>Given:</li><ul><li>Visual data&nbsp;\\(x^{seen} \\in X\\) corresponding to the\n<b>seen (= source)</b> category labels&nbsp;\\(y \\in Y\\)<br></li><li>Embedding of the auxillary semantic\ninformation&nbsp;\\(s(y): Y \\cup Z \\rightarrow S\\) each class has an associated semantic\nembedding<br></li></ul><li>Goal:</li><ul><li>Recognize visual samples&nbsp;\\(x^{unseen} \\in X\\) corresponding to the <b>unseen(=target)</b> classes&nbsp;\\(z \\in Z\\) under the <b>Zero-Shot condition:</b>&nbsp;\\(Y \\cap Z = \\varnothing \\)</li></ul></ul><img src=\"paste-8b8731f3b04f6d35ac50bd29bd1f720cdecf236c.jpg\"><br>"
            ],
            "guid": "AUauPPIX&X",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Zero-Shot Learning: Different Formulations",
                "Problem Statement depends on:<br><ul><li>Semantic information: what kind of auxiliary semantic information?<br></li><li>Access to unlabeled test-data at inference-time: inductive (standard) vs. transductive<br></li><li>Are known categories also present at test-time?  Standard vs generalized ZSL<br></li></ul>"
            ],
            "guid": "C>7tlejuk/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Types of auxiliary semantic information",
                "<img src=\"paste-9904ea36a5b2da19627b2dfd2f8078cce251a1c6.jpg\">"
            ],
            "guid": "w,O;OlupU4",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Visual and semantic models",
                "Common Setting<br><ul><li>A visual model \\(f(x)\\) optimized on training examples of known (source) classes<br></li><li>Embedding of the auxillary semantic information&nbsp;\\(s(y): Y \\rightarrow S\\)<br></li></ul><img src=\"paste-98e83f69744f849082e7914438261e9507d383cd.jpg\"><br>"
            ],
            "guid": "rDjx1C]F_H",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Semantic embedding of category labels as\nside-information",
                "<ul><li>Category labels are one or few words -&gt; leverage the intersection of\ncomputer vision and natural language processing<br></li><li>Use a word embedding model \\(s(y)\\), e.g., a Skip-gram model trained\non a large corpus of text documents (e.g. word2vec trained on\nWikipedia or news articles)<br></li><li>Skip-gram: words appearing in similar context will get similar vectors<br></li><li>Skip-gram trained embedding of category label provides semantic information \\(s(y)\\)<br></li><li>Assumption: visual and semantic embeddings correlate.<br></li></ul><img src=\"paste-24d08b85d34548d9357836a90d51019c89a511a3.jpg\"><br><img src=\"paste-b7a8790124f2a2ad920c591b184339ea6aea9637.jpg\"><br>"
            ],
            "guid": "AW4uBW|ui/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Attributes as side-information",
                "<ul><li>Classify new categories given their semantic attributes ( such as color or shape)<br></li><li>Attributes can be:<br></li><ul><li>Real valued<br></li><li>Binary (mostly encoded as a one hot vector)<br></li></ul><li>Famous dataset: Animals with Attributes (AWA)<br></li></ul><img src=\"paste-123293f031c62041583261f79397ec4a7370bf07.jpg\"><br><img src=\"paste-2c685eaa5d23739613bd955f34d3b2b39a5b3df0.jpg\"><br>"
            ],
            "guid": "C[LX_H&58R",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Transductive vs. Inductive ZSL",
                "<ul><li>Inductive ZSL (standard): no access to visual data of unknown categories at test-time<br></li><li>Transductive ZSL ≈ ZSL + semi-supervised learning</li><ul><li>Unlabelled target images of unknown classes are available.<br></li><li>Leads to a significant gain in accuracy<br></li></ul></ul><img src=\"paste-c2623a8d325fcd3e1daf69bc7f6898ceaa830a1b.jpg\"><br>"
            ],
            "guid": "zgD_-7M9E.",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generalized ZSL: Definition",
                "Recap ZSL:&nbsp;<br><ul><li>Training: seen (=source) categories \\(y \\in Y\\), with available labelled visual training data<br></li><li>Goal ZSL: recognize unseen (= target) classes&nbsp;\\(z \\in Z\\) under the<br></li><li>Zero-Shot condition: \\(Y \\cap Z = \\varnothing\\)<br></li></ul>Generalized ZSL:<br><ul><li>Training: same as ZSL<br></li><li>Both seen and unseen categories are present at test-time<br></li><li>Goal: recognize unseen (= target) classes&nbsp;\\(z \\in Z\\) and seen (=source) classes&nbsp;\\(y \\in Y\\)<br></li><li>(Of course, the Zero-Shot condition still holds)<br></li></ul>"
            ],
            "guid": "H9N0{luO42",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generalized ZSL: What should Generalized ZSL models be capable of?",
                "Genralized ZSL models should be capable of:<br><ol><li>standard classification of previously seen categories<br></li><li>knowledge transfer to new unseen classes (-&gt; e.g. through Zero-Shot learning )<br></li><li>discriminating between those two cases (-&gt; novelty detection )<br></li></ol><img src=\"paste-d08ce305789c960eff3b2184e35a137213c3c9d2.jpg\"><br>"
            ],
            "guid": "K39>KCg]4:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating Zero-Shot Learning: Standard ZSL",
                "<ul><li>The only difference in evaluation: we evaluate on new categories<br></li><ul><li>-&gt; Usually the same metric as in supervised recognition of the corresponding task (e.g.\naccuracy)<br></li></ul><li>Problem: the performance is strongly affected depending on which categories are\nknown and unknown<br></li><li>Metric: usually the same as in supervised recognition of the corresponding task (e.g.\naccuracy in case of classification)<br></li></ul>"
            ],
            "guid": "c2M::7kCWf",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating Zero-Shot Learning: Generalized ZSL",
                "<ul><li>Both known and unknown categories are present\nat test-time<br></li><li>Problem: the results are much better on the\nknown categories -&gt; optimization towards\nstandard metrics would mainly focus on\noptimizing standard supervised recognition. We\ndo not want that!<br></li><li>Common generalized ZSL metric: harmonic mean\nof known and unknown accuracies:&nbsp;\\[H = \\frac{2 \\cdot acc_{known} \\cdot acc_{novel}}{acc_{known} + acc_{novel}}\\]<br></li></ul><img src=\"paste-c5bc8e29237dc4043a22233deac42b1753083ccb.jpg\"><br>"
            ],
            "guid": "D=F)wd@m=C",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CONSE",
                "<ul><li>CONSE: Convex Combination of Semantic Embeddings<br></li><li>Reminder: visual model f (x) optimized on known (source) training classes<br></li><li>Skip-gram trained embedding of category label provides semantic information s(𝑦)<br></li><li>New input:</li><ul><li>Forward pass with the visual model: gives us probability estimates for the known classes<br></li><li>“Synthesize” a new word vector embedding s* for the given input by taking a linear\ncombination of the predicted probabilities and the semantic representation<br></li><li>A parameter&nbsp;\\(T\\) to only select top&nbsp;\\(T\\) predictions:&nbsp;\\(top(T) \\equiv \\{ i\\; |\\; p(y_i | x) \\textrm{ is among top } T&nbsp;&nbsp;\\textrm{&nbsp;probabilities} \\}\\)&nbsp;<img src=\"paste-6f374904ba1bd76b03462eb0211e22dad38fab5f.jpg\"><br></li><li>Final prediction: k−Nearest−Neighbours search in the semantic space (also containing\nembeddings of the target classes) with the synthesised embedding s∗&nbsp;<img src=\"paste-e92a44df68c856075e0afbf8802462356f518dd2.jpg\"><br></li></ul></ul>"
            ],
            "guid": "Ir&7l#wYe1",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CONSE: Overview",
                "<ul><li>Main idea: synthesize a new word vector embedding s* for the given input by taking a linear\ncombination of the predicted probabilities and the semantic representation<br></li><li>With a given visual and semantic embedding model, no extra training is needed<br></li></ul><img src=\"paste-354f80d340c009b648b38b980752f5dd953f84ab.jpg\"><br><img src=\"paste-9f3de22e16497eac34ee3b6cc7d3f53c98af72ad.jpg\"><br>"
            ],
            "guid": "p3cf^sG~!m",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeViSE: Overview",
                "<ul><li>DeViSE: A Deep Visual-Semantic Embedding Mode<br></li><li>Instead of predicting the known categories (classification), train the visual model to directly\noutput the semantic embedding (regression) through a linear transformation layer<br></li><li>Of course, we only train with semantic embeddings of the known categories<br></li><li>Idea: the model will capture the correlation of visual and semantic information and be able to\nproduce new numeric vector for new visual inputs at test-time<br></li><li>Embedding vector lookup table: precompute semantic embeddings of all categories<br></li></ul><img src=\"paste-a241e65638208ee89283e3a3be5273ae80f7fcba.jpg\"><br>"
            ],
            "guid": "B<OMr}xj#c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeViSE: Deep Visual Semantic Embedding Model",
                "<ul><li>Instead of predicting the known categories,\ndirectly predict the semantic embedding<br></li><li>Replace the classification layer of the visual\nmodel: use a linear embedding layer to map the\nfeatures to match the size of the word2vec\nmodel (often 500 or 1000d)<br></li></ul><img src=\"paste-c914660266baacc38d6d191c4f9fbbc571c09f7b.jpg\"><br>"
            ],
            "guid": "u-r|Cyyd-*",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeViSE: Training",
                "<ul><li>Since we regress numeric values (e.g. word2vec vectors) we could use L2 loss, but it was\nineffective in practice<br></li><li>Instead, hinge rank loss was used:&nbsp;<img src=\"paste-02bdf548805b6bdc42de8ed04ecd8c2c55770f77.jpg\"><br></li><li>\\(v\\) : output of the visual model bevor the new projection layer<br></li><li>\\(M\\):&nbsp;learned weight matrix of the last projection layer (regressing the semantic embedding)<br></li><li>\\(t_i\\): semantic representation of the known label i<br></li></ul>"
            ],
            "guid": "dnh{Qb%b7r",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DeViSE: Final prediction",
                "<ul><li>Final prediction: kNN over the\nlabel space (similar to ConSE)<br></li></ul><img src=\"paste-26655b33f2eac1a97be619c255bea78ba0f686ed.jpg\"><br>"
            ],
            "guid": "i6-hRxcrI2",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ZSL: Attributes",
                "<ul><li>Classify new categories given their semantic attributes ( such as color or shape)<br></li><li>Attributes can be:<br></li><ul><li>Real valued<br></li><li>Binary (mostly encoded as a one hot vector)<br></li></ul><li>Famous dataset: Animals with Attributes (AWA)<br></li></ul><img src=\"paste-37759bafedb54078fe502cd0f532b4356b1e232b.jpg\"><br>"
            ],
            "guid": "DM,m{#z;%Y",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Direct Attribute Prediction (DAP)",
                "<ul><li>Two step process</li><ul><li>Learn a binary classifier for each attribute&nbsp;\\(a_m\\)<br></li><li>Combine scores of learned attribute classifiers: this gives us&nbsp;\\(p(a_m\\; | \\; x)\\) (for input x)<br></li><li>Assumption: deterministic known dependence of attributes and classes \\(p(a_m\\; | \\; y) = [[a_m = a^y_m]]\\)<br></li><li>Final class is assigned as:&nbsp;\\[f(x) = argmax_c \\prod_{m=1}^M \\frac{p(a_m^c | x)}{p(a^c_m)}\\]</li></ul></ul><img src=\"paste-e806b1ee80547c79c882a16979134553217facef.jpg\"><br>"
            ],
            "guid": "IF$d5,$`qd",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Indirect Attribute Prediction (IAP)",
                "<ul><li>First learn a multiclass classifier on the known classes<br></li><li>Assumption: deterministic known dependence between attributes and classes:&nbsp;\\(p(a_m | y) = [[a_m = a_m^y]]\\)<br></li><li>Probability of an attribute&nbsp;\\(a_m\\)&nbsp;given a new image x is weighted sum of class probabilities and\nattributes of the corresponding classes&nbsp;\\(p(a_m | x) = \\sum^K p(a_m | y_k)p(y_k | x)\\)</li><li>Predicting the unknown classes: the same way as in DAP</li></ul><img src=\"paste-e18b118a25b2874fbc31a28c774a9a343c31ab18.jpg\"><br><br>"
            ],
            "guid": "HE/2U2@,HG",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ZSL:&nbsp;Text-to-image Synthesis with GANs: Idea",
                "<ul><li>Generative Adversarial Networks (GANs) can generate new samples conditioned on the input!<br></li><li>Main idea: train a GAN to generate images/features based on the auxillary semantic\ninformation (class labels, attribute vectors, descriptions, etc.)<br></li><li>GANs are able to not only generate diverse samples within the known classes\nbut also generate samples with significant differences between classes<br></li></ul><img src=\"paste-67c4c5e28772f5596a2dd73134459003f66c7872.jpg\"><br><br>"
            ],
            "guid": "D9I/yaF=nv",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ZSL:&nbsp;Text-to-image Synthesis with GANs: Generator",
                "<ul><li>Text embedded with a pretrained model&nbsp;\\(\\phi\\)<br></li><li>Input to Generator: 1) Description embedding 2) Noise prior vector&nbsp;\\(z \\in \\mathbb{R}^Z \\sim \\mathcal{N}(0,1)\\)<br></li><li>Generator synthesises an image conditioned on the input<br></li></ul><img src=\"paste-c3352a7538b6b6dd1cab77a364769e27afe9c634.jpg\"><br>"
            ],
            "guid": "p?H<nWorvb",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ZSL:&nbsp;Text-to-image Synthesis with GANs: Discriminator",
                "<ul><li>Matching-aware Discriminator D learns to:<br></li><ul><li>Distinguish between real and fake images (as in normal GANs)<br></li><li>Additionaly: learn text-image matching: description is also used as input to D<br></li><ul><li>Sometimes correct, sometimes incorrect descriptions<br></li><li>The discriminator should additionaly identify if the text and the synthesised image match<br></li></ul></ul></ul><img src=\"paste-da37213a494a2e1a6c5bcd42c89f779e69bdf9de.jpg\"><br>"
            ],
            "guid": "EdoX:,Z?%o",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Feature Generating Networks (f-CLSWGAN): Idea",
                "CNN features can be extracted from:<br><ol><li>real images, however in ZSL we do not\nhave access to any real images of unseen\nclasses,<br></li><li>synthetic images (as in previous method),\nhowever they are not realistic enough<br></li></ol>Idea: take a shortcut over features – generate\nfeatures instead of images<br>"
            ],
            "guid": "z&ngx,>G?`",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Feature Generating Networks (f-CLSWGAN): Key Ingredients",
                "<ul><li>Generate features instead of images<br></li><li>Underlying architecture: Wasserstein GAN\n(WGAN, Arjovsky et al., 2017)<br></li><li>Standard WGAN Loss&nbsp;\\(L_{WGAN}\\) (more details in\nthe GAN lecture) + classification driven loss<br></li><li>Classification driven loss&nbsp;\\(L_{CLS}\\):&nbsp;guarantee that\nthe generated CNN features are well suited\nfor training a discrimi native classifier\n(Negative Log Likelihood Loss)</li><li>Final objective (𝛽 is a hyperparameter):&nbsp;\\(min_G max_D \\;&nbsp; \\mathcal{L}_{WGAN} + \\beta\\mathcal{L}_{CLS}\\)<br></li></ul><img src=\"paste-770661740410ceb972841af16d008930fab701c8.jpg\"><br>"
            ],
            "guid": "J|.Ai&N{N>",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Feature Generating Networks (f-CLSWGAN): Classification driven loss",
                "<ul><li>This concept is similar to the\nsemantic consistency loss in\nCYCADA (see domain adaptation\nlecture)<br></li></ul><img src=\"paste-89058b5eeff01b221cf9c44e108176713811baf5.jpg\"><br>"
            ],
            "guid": "e=Wl={&YT$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Feature Generating Networks (f-CLSWGAN):&nbsp;Zero-shot Classification with f-CLSWGAN",
                "<ul><li>Train f-CLSWGAN for synthetic feature\ngeneration, we can now generate training\nsamples for new semantic descriptions!<br></li><li>Generate a synthetic dataset for the unknown\nclasses<br></li><li>Standard ZSL: train a classifier on data from the\nsynthetic unseen classes<br></li><li>Generalized ZSL: train a classifier on data from\nthe real seen and synthetic unseen classes<br></li></ul>&nbsp;\\(\\rightarrow\\)&nbsp;ZSL becomes simple classification"
            ],
            "guid": "F{{.fNF>r<",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generalized Zero-Shot Learning: Motivation",
                "<ul><li>Both known and unknow categories can occur at test-time<br></li><li>Most Zero-shot learning approaches can deal with it by design (e.g. we simply include the\nknown categories in our final nearest neighbour search)<br></li><li>In practice: strong bias towards known categories<br></li><ul><li>ConSE: will almost always assign a known category (since classifier predictions tend to be biased\ntowards towards very high values, there will probably be a known category with predicted probability\nof almost 1)<br></li><li>DEVISE: will almost always predict one of the seen semantic embedding<br></li><li>Generative Networks: less affected by this, but there is still a significant bias towards the known\ncategories<br></li></ul></ul><img src=\"paste-fbc0fb2dc06869c79057ef7117e0fd31c2e20fb5.jpg\"><br>"
            ],
            "guid": "Nb^fi8v>tq",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generalized Zero-Shot Learning: Principle",
                "<ul><li>Problem: strong bias towards known categories<br></li><li>Most common solution: gating mechanism for novelty detection<br></li><li>Gating mechanisms detects if the sample is known or unknown<br></li><ul><li>If known: standard supervised classification<br></li><li>If unknown: ZSL approach<br></li></ul><li>Different Gating mechanisms:<br></li><ul><li>One Class-SVM<br></li><li>Gaussian Mixture Model<br></li><li>Softmax Confidence<br></li><li>Model Uncertainty<br></li><li>many more</li></ul></ul><img src=\"paste-a51992b91e0982975114da85208f3bbb9d5599c6.jpg\"><br>"
            ],
            "guid": "PRmr[a3p!>",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Few-Shot Learning: Problems Formulation",
                "<ul><li>In short: given abundant training examples for the base classes, few-shot learning algorithms\naim to learn to recognize novel classes with a limited amount of labeled examples.<br></li><li>Given:</li><ul><li>Data rich base categories \\(C_{base}\\) with a large amount of labelled training data \\(D_{base}\\)</li><li>N data scarce categories \\(C_{novel}, |C_{novel}| = N\\)&nbsp;with only K reference training examples\navailable (where K is small and can be as low as 1)</li><li>The dataset containing these training examples of data scarce categories \\(C_{novel}\\) is referred\nto as the support set \\(D_{supp}\\)</li><li>Condition same as in ZSL (just different terminology):&nbsp;\\(C_{base} \\cap C_{novel} =&nbsp; \\varnothing \\)</li><li>The test set which is referred to as the query set Q, with test examples belonging to \\(C_{novel}\\)</li></ul><li>Goal: Assign the correct category&nbsp;\\(y \\in C_{novel}\\) to each test sample from&nbsp;\\(D_q\\)<br><br></li></ul><img src=\"paste-fc68ef92c6c95986f7c9a92ed7ae44fe0fc2e05b.jpg\"><br>"
            ],
            "guid": "K2)W74MHPA",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "N-way K-shot few shot learning",
                "N-way K-shot few shot learning task:<br><ul><li>A <b>support set</b> composed of <b>N categories</b> and,\nfor each label, <b>K labeled</b> images<br></li><li>a query set&nbsp;<b>\\(D_q\\)</b> with query images;<br></li></ul><img src=\"paste-032b62452e420e7d85d820aaaa11007827962ba9.jpg\"><br><img src=\"paste-ef6f8f10f634403d8238fd3bfa3fe68cccc82b37.jpg\"><br>"
            ],
            "guid": "p+3^hG:?)c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Few Shot Learning Approaches",
                "<ul><li>Metric Learning</li><ul><li>Learn to project the image to an embedding space using a distance loss function that aims\nto establish similarity or dissimilarity between images.<br></li></ul><li>Meta Learning</li><ul><li>Learn a learning strategy to adjust well to a new few-shot learning task.</li></ul><li>Augmentation-based</li><ul><li>Synthesize more data from the novel classes to facilitate the regular learning.<br></li></ul></ul>"
            ],
            "guid": "KzxceB-g@Z",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Standrad Learning vs. Meta Learning: Overview",
                "<img src=\"paste-43153e25abba4adca963b8fe6b45379c86a41ee2.jpg\">"
            ],
            "guid": "I9H`zA2#;",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Meta-Learning: Episode",
                "<ul><li>Episode: a batch of tasks (as opposed to a batch of labelled examples)<br></li><li>Few-shot learning: create different episodes by constructing different splits of\nsupport set and query set classes<br></li></ul><img src=\"paste-9f2ecf6b91420d4004a528c53136eab995d307e4.jpg\"><br><img src=\"paste-a3dd219b307fd09e128041c31b6485e86f2a17e1.jpg\"><br><img src=\"paste-9054bb976290a44e6fb533e7789ff00418d7eff8.jpg\"><br>"
            ],
            "guid": "h|eT:g=Z-a",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Meta Learning: Formal Definition",
                "<ul><li>Learning Algorithm A</li><ul><li>input: training set&nbsp;\\(D_{train} = \\{ (x_i, y_i) \\}\\)<br></li><li>output: parameters&nbsp;\\(\\theta\\) model M (the learner)</li><li>objective: good performance on test set&nbsp;\\(D_{test} = \\{ (x_i', y_i') \\}\\)</li></ul><li>Meta-learning algorithm</li><ul><li>input: meta-training set&nbsp;\\(\\mathcal{D}_{meta-test} = \\{ ({D'}_{train}^{(n)},{D'}_{test}^{(n)}) \\}^{N'}_{n=1}\\)</li><li>output: parameters&nbsp;\\(\\Theta\\) algorithm A (the meta-learner)</li><li>objective good performance on meta-test set \\(\\mathcal{D}_{meta-train} = \\{ ({D}_{train}^{(n)},{D}_{test}^{(n)}) \\}^{N}_{n=1}\\)</li></ul></ul>"
            ],
            "guid": "LGAHwJJcFi",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Relational Networks: Meta Learning with Metric Learning Objective: Architecture",
                "<ul><li>Embedding module: generates\nrepresentations of the query and\ntraining images<br></li><li>Relation module:<br></li><ul><li>Concatenate embeddings of query\nand support samples<br></li><li>Trained to produces score 1 for\ncorrect class and 0 for others\nusing Mean Squared Error<br></li><li>If multiple shots are available:\ncompute the mean value of the\ncategory<br></li></ul></ul><img src=\"paste-746ff01510f75e6d1324db02d1f767e58b7759ae.jpg\"><br>"
            ],
            "guid": "DN>G0MFZCa",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Relational Networks: Meta Learning with Metric Learning Objective: Training",
                "<ul><li>Training: meta-learning</li><li>Generate a new support and query\nsets in each episode: Randomly\nsample N classes and rearrange base\nclass data into meta-training tasks that\nsimulate test<br></li><li>Trained using Mean Squared Error:&nbsp;<img src=\"paste-bfab39bde04cf38fe2aa112e6ad0bab1b70cadf4.jpg\"></li></ul><img src=\"paste-dcc9cb54296caf0ffaf147777b54f90dd1b0bfe8.jpg\"><br>"
            ],
            "guid": "m+8HW|Db&Z",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Language-Vision Pre-Training Methods",
                "<ul><li>UNITER:</li><ul><li>UNiversal Image-TExt\nRepresentation Learning<br></li></ul><li>CLIP:</li><ul><li>Learning Transferable\nVisual Models From Natural\nLanguage Supervision<br></li></ul><li>ALIGN:</li><ul><li>Scaling Up Visual and\nVision-Language\nRepresentation Learning\nWith Noisy Text\nSupervision<br></li></ul></ul>"
            ],
            "guid": "c1Gnwxy1nJ",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Applications for LanguageVision Models",
                "<ul><li>ClipCap:<br></li><ul><li>CLIP Prefix for Image\nCaptioning<br></li></ul><li>DenseClip:<br></li><ul><li>Extract Free Dense Labels\nfrom CLIP<br></li></ul><li>Text-driven Image\nManipulation/Synthesis:<br></li><ul><li>StyleCLIP<br></li></ul></ul>"
            ],
            "guid": "j:?{Ar0EGK",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Extensions to Language-Vision Pre-Training",
                "<ul><li>BASIC:</li><ul><li>Scaling Up Visual and\nVision-Language\nRepresentation Learning\nWith Noisy Text Supervision<br></li></ul><li>CLIP-Lite:<br></li><ul><li>Information\nEfficient Visual\nRepresentation Learning\nfrom Textual Annotations<br></li></ul><li>SLIP:<br></li><ul><li>Self-supervision meets\nLanguage-Image Pretraining<br></li></ul></ul>"
            ],
            "guid": "i6Al%FD{2J",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNiversal Image-Text Representation Learning: UNITER: Overview",
                "<ul><li>Encodes Image Regions and\ntextual words into shared\nembedding space<br></li><ul><li>Image Embedder\n(Faster RCNN [Visual Genome\nobject+attribute pre-trained]\n+ box features)<br></li><li>Text Embedder\n(Tokenizer+Position)<br></li><li>Modality embedding (omitted\nin the figure)<br></li><li>All fed to FC into shared\nspace<br></li></ul><li>Transformer model then tries\nto learn generalizable\nembeddings for each region\nand word<br></li></ul><img src=\"paste-6cfeeefa6d0fc16fc2b7767773a468e2d0513457.jpg\"><br>"
            ],
            "guid": "seBs_kt0o}",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Masked Language Modeling",
                "<ul><li>Randomly mask words\n(15%) and recover as\ntransformer output<br></li><ul><li>Replaced with a random\nword, unchanged, or\n[MASK]-token\n([10,10,80]% following\nBERT)<br></li><li>Only one modality masked\nat a time<br></li></ul><li>Minimize NLL&nbsp;<img src=\"paste-7ef8bbe2146f8e96683f8f3ba0b3a252a5543765.jpg\"><br></li></ul><img src=\"paste-87557fc48b83b6f9c964036afb122e99df455fea.jpg\"><br>"
            ],
            "guid": "m8d78R^;c8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Masked Region Modeling",
                "<ul><li>Randomly mask regions\n(15%) and recover as output<br></li><ul><li>Features of Masked Region\nset to zeros<br></li><li>Only one modality masked\nat a time<br></li></ul><li>Can’t directly apply NLL<br></li><li>3 Approaches:<br></li><ul><li>Feat. Regression (L2-loss)<br></li><li>Region Classification (hard kclass classification of Faster\nRCNN)<br></li><li>Region Classification with KL-div\n(soft k-class classification of\nFaster RCNN)<br></li></ul></ul><img src=\"paste-a1a4671e6e5463108cf356621ab8611d1857ff28.jpg\"><br>"
            ],
            "guid": "y1^>$DMTCG",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Word Region Alignment",
                "<ul><li>Optimal Transport to\noptimize alignment\nbetween tokens and\nregions such that cosine\ndistance is minimized<br></li><li>Transport plan&nbsp;\\(T \\in \\mathbb{R}^{T \\times K}\\) that sums to 1:&nbsp;&nbsp;<img src=\"paste-0d8638095088f2c6756edc0f510b6b35aa2dffa6.jpg\"><img src=\"paste-02733f858062cad2ab0bbcae007f70634ee29222.jpg\"></li></ul>"
            ],
            "guid": "o]=Hz4<gFD",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Image-Text Matching",
                "<ul><li>Sample positive or\nnegative pair at each step<br></li><li>Input:\nSentence and set of image\nregions<br></li><li>Output:\nBinary label if sampled\npair is a match<br></li><li>Additional [CLS] token fed\ninto FC and perform BCE<br></li></ul><img src=\"paste-ebb6343490bdf52e226640862d4d8bf80555d33e.jpg\"><br>"
            ],
            "guid": "fya`X5!E[8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Design for Downstream Tasks - VQA",
                "<ul><li>Take ~3k most frequent answers\nas candidates<br></li><li>Pass [cls]-embedding into linear\nclassification layer<br></li><li>BCE for multi-label classification<br></li><li>At inference return most\nprobable answer as predicted\nanswer<br></li><li>Additional question-answer pairs\nfrom Visual Genome are used as\ndata augmentation<br></li></ul><img src=\"paste-a497fee30b3b5e117c1ef0271c0ad89aae3f91a2.jpg\"><br>"
            ],
            "guid": "G/q1#3GRYI",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Design for Downstream Tasks – Visual Reasoning",
                "<ul><li>Visual Reasoning:</li><ul><li>Question-answering<br></li><li>Answer-justification<br></li><li>Question-answering + answerjustification if answer is correct<br></li></ul><li>Concatenation of questions and\nanswers (+ rationales)<br></li><li>[cls]-token passed to linear layer<br></li><li>CE loss for each question answer pair or question-answer-rationale\ntriplet (True/False)<br></li><li>Fine-tuning with previous pretraining tasks (-ITM as text doesnt\nneed to be a description)<br></li></ul><img src=\"paste-cacd223ef1c514ef1b7b1c016f604036be127ed4.jpg\"><br>"
            ],
            "guid": "M70vsqB5q(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER: Design for Downstream Tasks – Retrieval",
                "<ul><li>Retrieval:<br></li><ul><li>Image -&gt; Text<br></li><li>Text -&gt; Image<br></li></ul><li>Concatenate query with element\nin DB individually<br></li><li>[cls]-token passed to linear layer</li><li>Trained via triplet loss and hardnegative-sampling (top-20 for\neach image/sentence)<br></li><li>At inference return pairs with\nhighest scores<br></li></ul><img src=\"paste-c15498ef28acd6b0df42da94c384ff8f57ad583b.jpg\"><br>"
            ],
            "guid": "Q4=&`iaD%%",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Learning Transferable Visual Models From Natural\nLanguage Supervision - CLIP: Idea",
                "<ul><li>Assumption: Learning from predetermined categories restricts network generality<br></li><li>Learning from raw text about images leverages broader source of supervision to generate\nmore robust models<br></li><ul><li>Creating a fitting dataset<br></li><li>Contrastive pre-training method<br></li><li>Scaling model sizes<br></li></ul><li>The model is pre-trained to predict if an image&amp;text snippet fit<br></li><ul><li>Contrastive Dual-Encoder architecture<br></li><li>No intermediate transformer to generate generalizable embedding required<br></li></ul></ul>"
            ],
            "guid": "Ap!d5V~hVp",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP: Dataset Creation",
                "<ul><li>0.5M queries based on uni-/bi-grams occurring at least 100 times in Wikipedia-en<br></li><li>20K pairs per query<br></li><li>Resulting 400M (image, text) pairs<br></li><li>Similar word count to NLP datasets used to train GPT-2<br></li></ul>"
            ],
            "guid": "pj}qJAD+as",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP: Model Design",
                "<ul><li>Joint training of an Image CNN and text transformer to predict whether a caption\nmatches an Image<br></li><li>Linear projection as mapping<br></li></ul><img src=\"paste-8b2064791f7602a151a60eec60594703069e91c7.jpg\"><br>"
            ],
            "guid": "i`QkF;Q{Af",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP: Pretraining Objective",
                "<ul><li>Contrastive objective: minimize similarity of incorrect pairings in NxN matrix with symmetric\nCE over similarity scores (InfoNCE)<br></li><ul><li>Temperature scaling in softmax optimized during training<br></li></ul></ul><img src=\"paste-bc518497dd664c5c4ec1e85091f40dfbe6f1771d.jpg\"><br>"
            ],
            "guid": "k...}JwV7(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP: Model Selection",
                "<ul><li>ResNet-50 with some tricks<br></li><ul><li>‘Smarter’ residual block – less feature loss during downsampling (He et al. CVPR 19)<br></li><li>Antialiased rect-2 blur pooling<br></li><li>GAP replaced with attention pooling with a single multi-head QKV attention\nlayer<br></li></ul><li>Vision Transformer (ViT)<br></li><ul><li>Different initialization<br></li><li>Additional layer norm to the combined patch/position embedding\nbefore the transformer</li></ul><li>Text Encoder is a transformer after Redford et al. OpenAI blog2019<br></li></ul>"
            ],
            "guid": "n)2AEc|v<c",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP: Details",
                "<ul><li>Adam with decoupled weight decay<br></li><li>Batchsize of 32,768 (Pre-training batchsize not specified in UNITER [4k-10k])<br></li><li>To save memory<br></li><ul><li>Mixed precision training<br></li><li>Gradient checkpointing<br></li><li>half-precision Adam statistics<br></li><li>Half-precision text encoder weights</li></ul></ul>"
            ],
            "guid": "Q6eEH3AT1X",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ALIGN: A Large-scale ImaGe and Noisy-text Dataset",
                "<ul><li>Collection of 1.8B Image-text\npairs<br></li><li>Collection follows Conceptual\nCaptions (3M)<br></li><li>Keep images with shorter dim &gt;\n200px, aspect ratio &lt;3 images\nwith more than 1000 alt-texts are\ndiscarded<br></li><li>Exclude alt-texts shared by more\nthan 10 images or any rare/too\nshort/too long tokens<br></li><li>Remove near-duplicates of eval\ndatasets<br></li></ul><img src=\"paste-630a4330cfe62e2dcc6ab8fedc2f12b629f4d995.jpg\"><br>"
            ],
            "guid": "pWL@aJ2[1o",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ALIGN: Model (nearly identical to CLIP)",
                "<ul><li>Image Encoder: EfficientNet\nwith global pooling<br></li><li>Text Encoder: Bert with [CLS]\ntoken + FC layer as text\nembedding<br></li><li>Optimized via normalized\nsoftmax loss (InfoNCE)<br></li><li>Matched Image-text pair\npositive, rest of potential pairs\nacross all cores as negative&nbsp;<img src=\"paste-78a75427da936036c7d91999dbede25825d5ff3a.jpg\"><br></li></ul><img src=\"paste-4d8b8b70426958020c45d77b101989e2c47b89c2.jpg\"><br>"
            ],
            "guid": "e/t=%dD?r?",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ALIGN: Details",
                "<ul><li>Encoders: EfficientNet-L2 + BERT-Large<br></li><li>Resolution 346x346 -&gt; random crop to 289x289 + horizontal flip<br></li><li>LAMB optimizer, warm up learning rate to 1e-3 for 10k steps then decay to 0 over\n1.2M steps<br></li><li>Batchsize of 16384<br></li><ul><li>1024 TPUv3 cores<br></li><li>16 positive pairs on each core</li></ul></ul>"
            ],
            "guid": "jVtRod!P.m",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "UNITER vs. Contrastive Dual Encoders",
                "<ul><li>UNITER:</li><ul><li>Separate extraction of regions+features and text features<br></li><li>Fusion-like transformer mapping features into shared space<br></li></ul><li>Contrastive Dual-Encoders:</li><ul><li>Separate extraction of image and text features<br></li><li>Features brought into same space via training objective w/o architecture adaptation<br></li></ul></ul><img src=\"paste-1097f7a9db9ae3f4d080902bf1025f06495316b6.jpg\"><br>"
            ],
            "guid": "lPeSB!W+Uu",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ClipCap: Clip Prefix for Image Captioning: Idea",
                "<ul><li>Combine CLIP’s rich semantic features with a strong pre-trained text generation\nmodel<br></li><li>Works also when just learning the mapping network – keeping main models frozen<br></li><li>No explicit need for object detection models<br></li></ul>"
            ],
            "guid": "iHrf>3fy;7",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "ClipCap: Method",
                "<ul><li>Mapping network generates\nfixed-length prefix embeddings\nfor GPT-2 to generate fitting\ncaptions<br></li><li>Mapping network is a\nlightweight transformer<br></li><li>Concatenating Mappings to\nthe caption<br></li><li>Related to Prefix-Tuning<br></li><li>Authors argue that models\nshould stay frozen as training\nthem only adds to training time\nbut does not add to\nperformance<br></li><li>Auto-regressive model\npredicting next token without\nconsidering future tokens<br></li><li>Simple loss CE loss on tokens:&nbsp;\\[ \\mathcal{L}_X = -\\sum_{i=1}^N \\sum_{j=1}^{\\mathcal{l}} log\\, p_{\\theta}(p_1^i, \\dots , p_k^i,c_1^i, \\dots , c_{j-1}^i) \\]<br></li><li>Inference starts on visual prefix\nand predicts next tokens one\nby one<br></li><ul><li>Can apply Greedy approach\nor beam search<br></li></ul><li>No reinforcement learning is\napplied<br></li><li>If finetuning, even a FC layer is\nsufficient as mapping<br></li><li>When frozen, a transformer is\nused (8 multi-head (8 heads)\nattention layer + 2-layer MLP)<br></li><li>Input into transformer is a\nlearned constant input and the\nCLIP embedding<br></li></ul><img src=\"paste-e0d2ddd0bef5f69a9595dfbe351e2bc42a26eaea.jpg\"><br>"
            ],
            "guid": "M/o[y3%q&9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "StyleCLIP: Text-Driven Manipulation of StyleGAN Imagery: Idea",
                "<ul><li>Utilize strong generation ability of Style-GAN (further covered next week) with\nlanguage understanding of CLIP models<br></li><li>Find point in GAN latent space that maximally corresponds to given CLIP\nword/sentence encoding<br></li><li>Similar approaches using BigGAN or similar exist (VQGAN, dVAE, StyleGAN,\n…)<br></li></ul>"
            ],
            "guid": "fG9}3?%kP!",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "StyleCLIP: The three methods of image manipulation",
                "<ul><li>Three methods of image manipulation investigated<br></li><ul><li>Utilizes Pre-trained : Image Encoder for StyleGAN, CLIP, StyleGAN<br></li></ul><li>Latent Optimization<br></li><ul><li>Minimize global loss computed in CLIP space<br></li><li>Online gradient descent for each image-text pair individually<br></li></ul><li>Latent Mapping<br></li><ul><li>Train mapping network to infer manipulation in single step in latent space<br></li><li>Manipulation may differ based on starting position in latent space<br></li></ul><li>Global Directions<br></li><ul><li>Transform text into mapping directions<br></li><li>Use different StyleGan latent space<br></li></ul></ul>"
            ],
            "guid": "hOY84z(;qB",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "StyleCLIP: Latent Optimization",
                "<ul><li>Latent Optimization<br></li><ul><li>Optimization problem solved by gradient\ndescent/Back-Prop<br></li><li>We want to find the location in latent space that\nminimizes:&nbsp;<img src=\"paste-b92c8c83e0629539254251d2da62f11529507f5e.jpg\"><br></li><li>\\(\\mathcal{w}+\\)&nbsp;is an extended version of the learned latent space<br></li><ul><li>Concatenation of multiple different&nbsp;\\(w \\in \\mathcal{W} \\)<br></li></ul></ul><li>Control similarity to input image via identity loss\nof a face recognition model&nbsp;\\(\\mathcal{L}_{ID}(w) = 1 - \\langle R(G(w_s)), R(G(w)) \\rangle\\)<br></li><li>Couple of hundred Iterations</li></ul><img src=\"paste-0ed58c407428832dc8c33892c4b93c250b27ffd0.jpg\"><br>"
            ],
            "guid": "Q{|m;]]er$",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "StyleCLIP: Latent Mapper",
                "<ul><li>Latent Mapper</li><ul><li>Different StyleGAN-layers correspond to different levels of detail</li><li>Learn to manipulate layers individually with a different part of the latent vector</li></ul><li>Mappers consist of 4 linear layers with design as in StyleGAN<br></li><li>Losses:&nbsp;\\(\\mathcal{L}(w) = \\mathcal{L}_{CLIP} + \\lambda_{L2} \\cdot ||M_t(w)||_2 + \\lambda_{ID} \\mathcal{L}_{ID}(w)\\)</li></ul><img src=\"paste-6a7692ec8ec97076a36f9eaaf6dcbe77de3f3998.jpg\"><br>"
            ],
            "guid": "iudT]*T3r",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "StyleCLIP: Global Directions",
                "<ul><li>Global Directions</li><ul><li>Instead of changing point in latent space&nbsp;\\(\\mathcal{W}+\\) find point in style space&nbsp;\\(s \\in S\\)<br></li><li>Directions of change tend to be similar for different manipulation steps<br></li><ul><li>Learn direct mapping from text prompt to direction in style space<br></li></ul></ul><li>Let&nbsp;\\(G(s)\\) bet the generated image</li><li>We seek a direction such that&nbsp;\\(G(s + \\alpha \\Delta s)\\)&nbsp;produces an amplified attributes<br></li><li>Use CLIP text encoder to find desired change in language space&nbsp;\\(\\Delta t\\)<br></li><ul><li>Use prompt engineering to get stable&nbsp;\\(\\Delta t\\) - target attribute + neutral class<br></li></ul><li>Find corresponding direction&nbsp;\\(\\Delta s\\) by assessing the relevance of each style\nchannel for the attribute&nbsp;<img src=\"paste-2656c36139971804ef03ee0cfcc7cadf368f9997.jpg\"><br></li></ul>"
            ],
            "guid": "qVpwVKoI$R",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DenseCLIP:&nbsp;Extract Free\nDense Labels from CLIP: Idea",
                "<ul><li>Pre-trained CLIP works well for ZS\nclassification or image manipulation<br></li><li>To correctly identify images in\npretraining CLIP has to fit different\nparts of sentences with image\nsections<br></li><li>Can Visualization methods like CAMs\nbe used to generate ZS\nsegmentations?<br></li></ul><img src=\"paste-8f3d6023662f006a078768533431b13d06404a4b.jpg\"><br>"
            ],
            "guid": "xAf#3`XM|y",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DenseCLIP: Finetuning fails",
                "<ul><li>Replace IN-pretrained\nbackbone with CLIP-pretrained\nnetwork<br></li><li>1x1 Conv Layer as mapping\nfunction&nbsp;<img src=\"paste-67e06902a69d07acf64038dbb58aac7e5dda049e.jpg\"><br></li><li>Fine-tuning CLIP lead to model\ndegradation<br></li><li>Manipulating text unnecessarily\nalso led to failure<br></li></ul><img src=\"paste-98fc340766848e064234ea8748d33d65628f1537.jpg\"><br>"
            ],
            "guid": "h,0eet2XlK",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "DenseCLIP and DenseCLIP+",
                "<ul><li>DenseCLIP: Remove\nGAP/attention pooling and\nbasically use classification\ndirectly from CLIP text encoder<br></li><li>DenseCLIP+: Use DenseCLIP\nas pseudo-labels to train\nsegmentation network like\nDeepLab or PSPNet<br></li><li>After 1/10th of training\nschedule DenseCLIP produces\ninferior pseudolabels than\nDenseCLIP+\n-&gt; SelfSupervision<br></li><li>CLIP initialization not evaluated<br></li></ul><img src=\"paste-dd1889c23e4725354216d2d24f4c26c7c65b1d6e.jpg\"><br>"
            ],
            "guid": "MpKySb4PW9",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Combined Scaling for Zero-shot Transfer\nLearning – BASIC: Idea",
                "<ul><li>Scaling contrastive learning in terms of Data,\nModel and Batch Size<br></li><li>More resources -&gt; better?</li></ul>"
            ],
            "guid": "j{)poL5IN^",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BASIC: Batch Size Scaling",
                "<ul><li>Contrastive Learning is memory hungry<br></li><li>How can batch size be efficiently increased to improve performance?<br></li><li>Authors propose improvement to GradAccum and re-materialization\n(not covered here)<br></li></ul>"
            ],
            "guid": "I=1sG<bGQk",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BASIC: Data Scaling",
                "<ul><li>Start with ALIGN dataset and add 5B image-text pairs from JFT<br></li><li>JFT is a classification dataset<br></li><li>Convert classes to text sequence&nbsp;<img src=\"paste-5f2763c6ffe8802e154076eeed5322424ff0861e.jpg\"><br></li><li>Tokenizer:\nsample 200M sentences from ALIGN+JFT to train sentence piece model<br></li><li>Filter and discard text sequences longer than 64 tokens<br></li></ul>"
            ],
            "guid": "t_Ob~|e*M0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BASIC: Model Scaling",
                "<ul><li>Visual Encoder: CoAttNet (Neurips21) in S,M,L<br></li><li>Language Encoder: standard Transformer but GAP instead of taking [CLS] token<br></li><li>Pretraining on JFT using softmax classification\n-&gt; fix image encoder and train text encoder using contrastive learning<br></li></ul><br>"
            ],
            "guid": "dg9sB_Sia?",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SLIP: Self-supervision meets Language-Image Pretraining: Idea",
                "<ul><li>Contrastive language supervision works well,\ncontrastive self-supervision also works well<br></li><ul><li>How well do they work together?<br></li></ul><li>SLIP – Multi-task framework combining selfsupervision with CLIP pre-training<br></li></ul>"
            ],
            "guid": "mXk+cOQb<7",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SLIP: Details",
                "<ul><li>Dataset:<br></li><ul><li>15M Subset of YFCC100M -&gt; YFCC15M<br></li><li>filtered as in CLIP<br></li><li>Clear text caption for each image<br></li></ul><li>Data Augmentation:<br></li><ul><li>Random Resize&amp;Cropping to 50-100% of original image (for everything)</li><li>For the self-supervised branch MoCo V3[16] augmentation scheme</li></ul><li>Architecture:<br></li><ul><li>ViT-{B,L,S}/16<br></li><li>Smallest text transformer as in CLIP<br></li></ul><li>Embedding Spaces<br></li><ul><li>CLIP – separate linear layers to 512d output space<br></li><li>SSL – 3-layer MLP with 4096d hiddens to 256d output space<br></li></ul></ul>"
            ],
            "guid": "L7s2,>`haS",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP-Lite: Information Efficient Visual Representation\nLearning from Textual Annotations: Idea",
                "<ul><li>Contrastive Vision&amp;Language Pretraining methods require lots of data\nto function properly<br></li><ul><li>ZS IN performance drops by ~half when\njust training on ContextualCaptions<br></li></ul><li>Can we come up with a more data\nefficient training scheme?<br></li></ul><img src=\"paste-ecf3fa4d8718e55f9e2c67d604e224045c150d72.jpg\"><br>"
            ],
            "guid": "M7-yh3w(:8",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP-Lite: Assumption",
                "<ul><li>Multiple contrastive views maximize a\nlower bound on MI between two or\nmore views of same element<br></li><li>InfoNCE theoretically loose in cases\nwhen true MI is larger than log #negS<br></li><li>Adopt lower bound based on Shannon\nDivergence to maximize MI<br></li></ul><img src=\"paste-92505791355b5a9c37b2fc13f6dd4d0ce61bfa09.jpg\"><br>"
            ],
            "guid": "z85K;bV)m2",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "CLIP-Lite: Method",
                "<ul><li>Estimate JS-Divergence\ninstead of InfoNCE<br></li><li>Uses one positive and one\nnegative sample<br></li><ul><li>Less memory consumption and\narguably less required data<br></li></ul><li>Discriminator network&nbsp;\\(T_{\\omega}: \\mathcal{Y} \\times \\mathcal{Z} \\rightarrow \\mathbb{R} \\) judges matching of embeddings<br></li></ul><img src=\"paste-77199e91ce106a0e14c5834f8228329e264244ac.jpg\"><br>"
            ],
            "guid": "mYTf+,sQPH",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Generative Models",
                "<ul><li>Goal: Given training data, generate new samples from the same\ndistribution<br></li><li>Density estimation: a core problem in unsupervised learning<br></li><ul><li>Explicit density estimation: explicitly define and solve for&nbsp;\\(p_{model}(x)\\)<br></li><li>Implicit density estimation: implicitly learn model that can sample\nfrom&nbsp;\\(p_{model}(x)\\) without explicitly defining it<br></li></ul></ul><img src=\"paste-0743e7782acb48cb25386fa0f2d67de70388db86.jpg\"><br>"
            ],
            "guid": "w*PLi@`(X:",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implicit Generative Models",
                "<ul><li>We train a network 𝐺 that models the distribution&nbsp;\\(p_{model}(x)\\)<br></li><li>The distribution should be close to the target distribution&nbsp;\\(p_{data}(x)\\)<br></li><ul><li>i.e. a distribution over images<br></li></ul><li>Sampling from 𝐺 enables us to judge whether distributions are close:<br></li></ul>-&gt; Rapid progress in recent years: modeling very diverse images<br>-&gt; Rapid progress in recent years: generating high resolution images<br>"
            ],
            "guid": "u@a<,Ud{Sa",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implicit Generative Models: Modeling probability distribution",
                "<ul><li>These networks implicitly model a probability distribution:<br></li><ul><li>Start by sampling the code vector 𝑧 from a fixed, simple distribution\n(e.g. spherical Gaussian or uniform distribution)<br></li><li>The generator network 𝐺 computes a differentiable function, mapping 𝑧\nto an instance 𝑥 in data space (i.e. \\(x \\in \\mathbb{R}^{3 \\times h \\times w}\\) for color images)<br></li></ul></ul><img src=\"paste-9813930bd5adc2385c079b35dd6b9abfb584954f.jpg\"><br>"
            ],
            "guid": "IEwc#vl6y?",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implicit Generative Models: Training&nbsp;",
                "<ul><li>To train a generative model 𝐺, we are given samples (indicated as\ndots) that originate from the true data distribution<br></li><li>For tuning the model parameters, to better grasp the true data\ndistribution, a loss function has to penalize the generator when\nproducing samples that are unlikely under&nbsp;\\(p_{data}(x)\\)<br></li></ul><img src=\"paste-11c71e73fb8b752c8488285f3274b3c7e020d0ad.jpg\"><br>"
            ],
            "guid": "DC0Iess[5Z",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Implicit Generative Models: Inference",
                "<img src=\"paste-8b0a31587577362d44611f92300f3c04df9f4364.jpg\">"
            ],
            "guid": "QThRyO)6mp",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "GANs: Motivation",
                "<ul><li>Advantage of implicit generative models:<br></li><ul><li>With a criterion for evaluating the quality of generated samples, the\ngradient with respect to the network parameters can be computed.<br></li><li>With this, the parameters can be updated to generate samples with a\nlittle better quality.<br></li></ul><li>The idea behind Generative Adversarial Networks (GANs) is to train\ntwo different neural networks:<br></li><ul><li>The generator network 𝐺 tries to produce realistic-looking samples<br></li><li>The discriminator network 𝐷 tries to figure out whether an image came\nfrom the training set or the generator network<br></li></ul><li>In short: The generator network tries to fool the discriminator network.<br></li></ul>"
            ],
            "guid": "Oy:r9nI@ib",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "GANs: Sketch",
                "<img src=\"paste-eea6ae36c35883371506d888dc627c60e0da1712.jpg\">"
            ],
            "guid": "zKh8ap%+/u",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Training GANs: Objective",
                "<ul><li>Generator network: try to fool the discriminator by generating reallooking images<br></li><li>Discriminator network: try to distinguish between real and fake images<br></li><li>Train jointly in minimax game&nbsp;<img src=\"paste-ae3302e31cfdd09b61334d72ab047f0dea547841.jpg\"><br></li><li>Discriminator&nbsp;\\(D_{\\theta d}\\) wants to maximize objective such that&nbsp;\\(D(x)\\) is close to 1 (real) and&nbsp;\\(D(G(z))\\) is close to 0 (fake)</li><li>Generator&nbsp;\\(G_{\\theta g}\\)&nbsp;wants to minimize objective such that&nbsp;\\(D(G(z))\\) is close to 1&nbsp;(discriminator is fooled into thinking generated&nbsp;\\(G(z)\\) is real)</li><li>Alternate between:&nbsp;<img src=\"paste-526795975da2eb01ce2d4e67a9dce0a8e4ff6403.jpg\"></li></ul>"
            ],
            "guid": "Ix@/@Kf^GO",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Training GANs: Updating the discriminator",
                "<img src=\"paste-17baf89eea2b506120d6cc37bd8810f0e1542bbb.jpg\"><br><img src=\"paste-9526b7a4c0b9b11797dd655b1ba565090e78fa5b.jpg\">"
            ],
            "guid": "Jjb.8f&&5",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Training GANs: Saturation problem",
                "<img src=\"paste-620c45e5d9ec098e852496f6b1da4cad6754cf2b.jpg\"><br><ul><li>In more realistic settings, the fake samples may be so poor initially that\nthe response of 𝐷 saturates<br></li><li>Thus, the loss for 𝑮 may be far in the exponential tail of 𝐷's sigmoid, and\nsince log(1 + 𝜖) = 𝜖 , 𝐺 will receive zero gradients.<br></li></ul>Solution:&nbsp;<br><img src=\"paste-07cff660823667950fb0c63c0323a16c5271cca1.jpg\"><br>"
            ],
            "guid": "4tQb[qz,6",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Training GANs: Full algorithm",
                "<img src=\"paste-bb09f7cf3217d1843a92818a7fcc6cb5332bac22.jpg\">"
            ],
            "guid": "Kd*jpo]FeI",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating GANs",
                "<ul><li>How do we evaluate GANs?<br></li><ul><li>If we use GANs for some downstream task, we can look at the\nperformance metric for that task (e.g. classification accuracy)<br></li></ul><li>But how to evaluate a GAN’s ability to synthesize images?<br></li><li>The properties we want the model to have are:<br></li><ul><li>Sample quality: Does the image look real?<br></li><li>Diverse samples: Does the model only generate one image or many\nplausible images?<br></li><li>Generalization: Does the model generate new images or simply memorize\ntraining samples?<br></li></ul></ul>"
            ],
            "guid": "rHV^%=oBUh",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating GANs – Inception Score",
                "<ul><li>Use a pre-trained classifier model to classify samples<br></li><li>Compare the label distribution in the dataset and the predicted label\ndistribution from generated samples -&gt; Compare using the KL-Divergence<br></li><li>The inception score gets worse:<br></li><ul><li>… when the GAN does not generate samples of a certain class -&gt; captures class diversity<br></li><li>… when the GAN produces bad samples, as the classification model will\nnot output useful predictions -&gt; captures sample quality<br></li></ul><li>Problem:<br></li><ul><li>Inception score considers class-wide statistics, producing only one unique\nimage per class does not decrease the score -&gt; Does not capture sample diversity<br></li></ul></ul>"
            ],
            "guid": "vmc{0Y+(Es",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating GANs – Fréchet Inception Distance",
                "<ul><li>How to capture whether a model produces diverse samples?<br></li><li>Compare the distribution of features as obtained from the generated\nsamples vs. obtained from the data<br></li><li>Features are computed using a pre-trained model and compared via\nthe Fréchet Distance<br></li><li>Finding good evaluation metrics that correlate with human judgment is\nreasearch itself!<br></li></ul>"
            ],
            "guid": "t<$SkQLh>(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating GANs – Nearest Neighbor",
                "<ul><li>How do we know whether our model memorizes training samples? -&gt; We look at the nearest neighbors in the training data<br></li><li>Use a pre-trained classifier to compute image features for:<br></li><ul><li>Training images<br></li><li>Generated images<br></li></ul><li>Search the nearest neighbors of generated images among the\ntraining images in feature space<br></li></ul>"
            ],
            "guid": "N@szEV%~dN",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Evaluating GANs – Nearest Neighbor: Examples",
                "<img src=\"paste-85c4fc5f63c9172690b3bdbb075e06b920b64d8e.jpg\">"
            ],
            "guid": "F(a&[JI3tg",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Deep Convolutional GAN (DCGAN): Motivation",
                "<ul><li>Training a standard GAN often results in two pathological behaviors<br></li><ul><li>Oscillations without convergence: Contrary to standard loss minimization,\nwe have no guarantee that it will actually decrease.<br></li><li>The infamous “mode collapse\", when G models very well a small subpopulation, concentrating on a few modes.<br></li></ul><li>Performance is in practice hard to assess and often boils down to a\nbeauty contest<br></li></ul>"
            ],
            "guid": "D=cnHWURe4",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Deep Convolutional GAN (DCGAN): Architecture",
                "Radford et al. converged to the following rules:<br><ul><li>Replace pooling layers with strided convolutions in 𝐷 and strided\ntransposed convolutions in 𝐺<br></li><li>Batchnorm in both 𝐷 and 𝐺<br></li><li>Remove fully connected hidden layers<br></li><li>ReLU in 𝐺 except for the output, which uses Tanh<br></li><li>LeakyReLU activation in 𝐷 for all layers<br></li></ul><img src=\"paste-b5470d87c051406c3f342ce9809738d597774612.jpg\"><br>"
            ],
            "guid": "f?;Jqi2j=|",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Conditional GAN: Motivation",
                "<ul><li>All the networks so far model a density in high dimension and provide\nmeans to sample from it, which is useful for synthesis only<br></li><li>However, most of the practical applications require the ability to sample\nfrom a conditional distribution, e.g.:<br></li><ul><li>Next frame prediction<br></li><li>Image in-painting<br></li><li>Semantic segmentation<br></li><li>Style transfer<br></li><li>Image-to-image translation<br></li></ul></ul>"
            ],
            "guid": "mm(R|{GY9-",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Conditional GAN: Details",
                "<ul><li>The Conditional GAN proposed by Mirza and Osindero consists of\nparameterizing both 𝐺 and 𝐷 by a conditioning quantity 𝑌.&nbsp;<img src=\"paste-305356f004b14461f24d96709e4aa4da5821832c.jpg\"><br></li></ul><img src=\"paste-715038db4ed468b22ba4a8bdb0fc0cbd7985c841.jpg\"><br><br>"
            ],
            "guid": "z)azBTQFW#",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pix2Pix: Overview",
                "<img src=\"paste-e8544894618a6cfe951ea6601a9ecfe8eb0509f5.jpg\">"
            ],
            "guid": "hb_q[_=mXM",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pix2Pix: Objective",
                "They define:<br><img src=\"paste-5d801e8a6c7acba97dea7afc070bc5c8a664c117.jpg\"><br><ul><li>The term 𝐿1 pushes toward proper pixel-wise prediction, and 𝑉 makes\nthe generator prefer realistic images to better fitting pixel-wise.<br></li><li><br></li></ul>"
            ],
            "guid": "xfzt!;^yb",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pix2Pix: Generator",
                "<ul><li>For the generator 𝐺, they start with the DCGAN architecture and add\nskip connections from layer i to layer D - i that concatenate channels<br></li></ul><img src=\"paste-3299bb615c2fcc62f38cb875c57fc530d61ea8f4.jpg\"><br>"
            ],
            "guid": "pcWo,g5PzY",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Pix2Pix: Discriminator",
                "<ul><li>The discriminator 𝐷 is a regular convnet which scores overlapping\npatches of size 𝑁 × 𝑁 and averages the scores for the final one.<br></li><li>This controls the network's complexity, while allowing to detect any\nhigh-frequency inconsistency of the generated image (e.g. blurriness)<br></li></ul>"
            ],
            "guid": "K7u<t=w}C_",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle GAN: Motivation",
                "<ul><li>The main drawback of Pix2Pix technique is that it requires pairs of\nsamples with pixel-to-pixel correspondence<br></li><li>In many cases, one has at its disposal examples from two densities and\nwants to translate a sample from the first (“Image of apples\") into a\nsample likely under the second (“Image of oranges\").<br></li></ul>"
            ],
            "guid": "w<}v<G^!te",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle GAN: Training with unpaired data",
                "<ul><li>If we have paired data (same content in both styles), this is a\nsupervised learning problem. But the paired data is often hard to find.<br></li><li>The Cycle GAN architecture learns to do image-to-image translation\nfrom unpaired data:<br></li><ul><li>Train two different generator nets:&nbsp;One to go from style A to style B, and one vice versa</li><li>Make sure the generated samples…<br></li><ul><li>…of style A are indistinguishable from real images of style A<br></li><li>…of style B are indistinguishable from real images of style B<br></li><li>-&gt;&nbsp;Use two discriminator networks</li></ul><li>Make sure the generators are cycle-consistent:<br></li><ul><li>Mapping from style A to style B and back, should result in the original\nimage (also enforced in the mirrored direction).<br></li></ul></ul></ul>"
            ],
            "guid": "lh1i4s!@*e",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Cycle GAN: Architecture",
                "<img src=\"paste-1b782acfdfa08e8b7caf6c9b32c94c779a84bfaa.jpg\"><br><img src=\"paste-6ddd4206804af88eaf0a5d88b3ec5137785b15ef.jpg\">"
            ],
            "guid": "DN_A4,R||B",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BigGAN",
                "<ul><li>Brock et al. explore how to scale up GANs in order to get a\nperformance boost from larger models and larger batch sizes:<br></li><ul><li>Batch sizes of 2048 images (-&gt; performance increase, but model becomes\nunstable in later iterations)<br></li><li>Models with ~160M parameters<br></li><li>Inject noise z in multiple layers via skip-connections (use latent space to\ninfluence features at different resolutions and levels of hierarchies)<br></li><li>In inference using “truncation trick”: Resample noise, when it falls beyond\na fix threshold (-&gt; trade-off individual sample quality and sample variety)<br></li></ul><li>Able to synthesize diverse 512x512 images!<br></li></ul><img src=\"paste-45869e3be7d16c01d15740efdc36303a6ef151db.jpg\"><br>"
            ],
            "guid": "mW`*Ar2AB1",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "BigGAN: Class leakage",
                "<ul><li>In training: “class leakage”, the generative model mixes properties of\ndifferent classes, e.g. dog and tennis ball -&gt; “dogball”<br></li></ul><img src=\"paste-d0d53232e866a6a83f1bc12fc08ce786c554f519.jpg\"><br>"
            ],
            "guid": "kNUe//M8Z]",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "SPADE",
                "<ul><li>Goal: Semantic image synthesis, generate a image that follows a\nunderlying given semantic structure&nbsp;<img src=\"paste-557fa472dbcedf0d20d9250ec658a7d9c2c813b8.jpg\"></li><li>Approaches like Pix2Pix “wash away” information in the semantic input\nmasks<br></li><li>SPADE omits the encoder, and injects the semantic mask at different\nscales into the generator:&nbsp;<img src=\"paste-187daafccf372e03a9ea275ee6e532ef2a41f590.jpg\"></li></ul><ul><li>SPADE employs “spatially-adaptive (de)normalization”, in contrast to\nbatch norm, it uses the semantic mask to modulate the activations.<br></li></ul>"
            ],
            "guid": "Lp++L;]d={",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Continual Learning (CL)",
                "<ul><li>Ability to continually acquire, fine-tune, and transfer\nnew knowledge and skills<br></li><li>Higher and realistic time-scale where data (and\ntasks) become available only during time<br></li><li>No access to previously encountered data.<br></li><li>Constrained computational and memory resources<br></li></ul>"
            ],
            "guid": "N,?B/9Ekh_",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Biological Factors of Continual Learning",
                "<ul><li>Structural Plasticity<br></li><ul><li>Neurosynaptic adaptation to changes in the\nenvironment<br></li><li>Change of physical structure as the result of learning<br></li><li>Stability-plasticity balance<br></li></ul><li>Complementary Learning Systems<br></li><ul><li>Retaining episodic memories (memorization)<br></li><li>Extracting statistical structure (generalization)<br></li><li>Memory replay<br></li></ul></ul>"
            ],
            "guid": "PpffO7Rd6-",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "The Stability-Plasticity Dilemma",
                "<ul><li>Stability-Plasticity Dilemma:</li><ul><li>Remember past concepts<br></li><li>Learn new concepts<br></li><li>Generalize<br></li></ul><li>Main Problem in Deep Learning:<br></li><ul><li>Catastrophic Forgetting<br></li></ul></ul>"
            ],
            "guid": "I*c?4=RL1f",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Catastrophic Forgetting",
                "<ul><li>Training a model with new information interferes with\npreviously learned knowledge<br></li><li>Abrupt performance decrease or old knowledge\ncompletely overwritten by the new one<br></li><li>Different factors varying across time cause\ncatastrophic forgetting<br></li></ul><img src=\"paste-8389d8705f08af5eeb0b18c2d3525e11e1e910fb.jpg\"><br>"
            ],
            "guid": "beNJ(K#U,p",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Three major approaches to prevent catastrophic forgetting",
                "<ul><li>Regularization<br></li><li>Architecture Growing<br></li><li>Memory Replay<br></li></ul><img src=\"paste-f914499c70c2ad1b1704e9a9369e97cbc2e6946b.jpg\"><br>"
            ],
            "guid": "k98xs>4?)/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Architecture growing vs Regularization",
                "<img src=\"paste-bc8c5e9494b35e1b35bc3ca70d1e7c96968afc0b.jpg\">"
            ],
            "guid": "yF+yhXGL3",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How lifelong learning is evaluated: Overview",
                "<ul><li>Task-incremental learning (Task-IL)<br></li><li>Domain-incremental learning (Domain-IL)<br></li><li>Class-incremental learning (Class-IL)<br></li></ul><img src=\"paste-2dc464f688cd4c9e3860418e2f38b5e6663e4b50.jpg\"><br><br><img src=\"paste-feb7dba0835bb3e0089bf94fa7dab63c796dca8d.jpg\"><br>"
            ],
            "guid": "km8*KH,tCX",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "How lifelong learning is evaluated: Benchmark",
                "<ul><li>Commonly Used Public Benchmarks<br></li><ul><li>Split MNIST, Permuted MNIST, FashionMNIST<br></li><li>CIFAR10-100 datasets (typically in Task-IL setting)<br></li><li>Core50 – (videos, temporally coherent tasks – 10 tasks (categories)\nrepresenting 5 objects (classes) each)<br></li><li>– ImageNet (Task-IL setting)<br></li></ul><li>Typical metric Global Accuracy<br></li><ul><li>Train on current task/class(s) test on all previous seen tasks/classes<br></li></ul></ul>"
            ],
            "guid": "QYUg}^u20S",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Regularization: Elastic weights consolidation (EWC): Idea",
                "<ul><li>Training on new task\nshould only change\nweights that are less\nimportant for the\nprevious task&nbsp;<img src=\"paste-10014cc44ae5e7dc665464b40773131329d2b8e5.jpg\"></li><li>EWC: weighted Regularization of the network\nparameters&nbsp;<img src=\"paste-76e99d220acfe215bc647dedd788fdbfe81cd19f.jpg\"></li></ul>"
            ],
            "guid": "yqcG8N^c2[",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "EWC: Distributions",
                "<img src=\"paste-8522d32fb03e12d71ccc378b1c1751b8f4e8d8dd.jpg\"><br><ul><li>After training on task A we obtain log p(θ*|A) – i.e. optimized theta\nrepresents the mode of the log p(θ|A)<br></li><li>To compute the above on task B, we need to estimate the full\nposterior density function log p(θ|A)<br></li><li>The posterior log p(θ|A) is intractable – needs to be approximated<br></li></ul><img src=\"paste-aa90eb1fbd746390fc66eaf05fa98a891f95472e.jpg\"><br>"
            ],
            "guid": "g?fBq?X1w,",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Elastic weights consolidation (EWC): Loss",
                "<img src=\"paste-c43bdb1b79e0bffde6cce66ca2afe5394aafb08f.jpg\"><br><ul><li>In the paper they use a simple notation to represent this&nbsp;<img src=\"paste-a1ba950d36137c763de685e5f260c9cc7a1a69b4.jpg\"></li><li>The fisher information matrix (FIM) ‘F’ has some desirable properties<br></li><ul><li>Equivalent to the second derivative of the loss<br></li><li>Can be computed from first-order derivatives alone – easy for large models<br></li></ul></ul>Practically a diagonal FIM is used.\nWhere the diagonal elements of FIM on\ntask k+1 can be easily computed by sum of\nsquared gradients of the loss on dataset.\nS is the train data of task k&nbsp;<br><br><img src=\"paste-1cfb931a806d4941364ac8f6b45aad5075c1e9ec.jpg\"><br>"
            ],
            "guid": "JWdReI-U1=",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Memory Replay: Idea",
                "<ul><li>Maintain an extra memory to store samples from the\ncurrent and previous tasks<br></li><li>Replay these when training on new tasks, therefor\nmitigate forgetting<br></li><li>Most successful and widely used method(s)<br></li><li>Considerations:</li><ul><li>How to populate memory – which samples to store<br></li><li>Memory size constraints – how to update<br></li><li>How to replay – random replay?<br></li></ul></ul>"
            ],
            "guid": "G(d2zEz[e/",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Memory Replay: How to populate memory",
                "<ul><li>Reservoir sampling</li><ul><li>put all encountered data in the\nmemory<br></li><li>When full, replace uniformly with a\nprobability m/n<br></li></ul><li>Class-Balancing Reservoir\nSampling (CBRS)<br></li><ul><li>Class balanced sample replacement<br></li><li>Weighted replay<br></li></ul><li>Gradient Episodic Memory\n(GEM)<br></li><ul><li>Greedily maximize the variance of\ngradients directions of the samples in the\nmemory<br></li><li>Require knowledge of task labels<br></li></ul><li>Gradient based Sample\nSelection (GSS)<br></li><ul><li>Incremental method to improve GEM\nwithout requiring to have task labelsthus suitable for online CL<br></li></ul></ul>"
            ],
            "guid": "gE0n9ydv?;",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Memory Replay: What samples should be replayed",
                "<ul><li>Retrieve samples that suffer from an increase in loss given\nthe estimated parameters update of the model<br></li></ul>"
            ],
            "guid": "Kfk*1yAFP0",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Memory Replay: REMIND",
                "<ul><li>REMIND REplay using Memory Indexing</li><ul><li>Instead of storing images stores latent vectors<br></li><li>Online (Streaming) learning model<br></li><li>Efficient storage of latent vectors using Product\nQuantization<br></li><li>Among the first to use latent vectors for replay instead\nof images<br></li><li>Nice results on largescale datasets ImageNet and Core50</li></ul></ul><img src=\"paste-53685d6d29359eb1fbfcb6fc65b6be3d714ab19f.jpg\"><br>"
            ],
            "guid": "jaeBQ+(x.(",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        },
        {
            "__type__": "Note",
            "fields": [
                "Anatomy of Catastrophic Forgetting",
                "<ul><li>Measure degree of forgetting across different layers using<br></li><ul><li>Representational similarity techniques<br></li><li>Layer freezing &amp; layer reset experiments<br></li></ul><li>Main conclusions</li></ul><ol><ol><li>lower layers representations remain stable through training on\nthe new task<br></li><li>Higher layer representations change significantly<br></li><li>Empirical evidence that forgetting follows semantically\nconsistent patterns (i.e. degree of forgetting related to task\nsimilarity)<br></li></ol></ol><img src=\"paste-a48b150879944286a3d072f4a36df8d593b1d155.jpg\"><br><img src=\"paste-c20fef3441d4999151437bf26074db68df2c8c74.jpg\"><br>"
            ],
            "guid": "K11NO6f>^s",
            "note_model_uuid": "a0b8c2ca-7d23-11ec-98a7-182649c72b2b",
            "tags": []
        }
    ]
}